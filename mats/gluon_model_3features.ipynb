{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from pipeline import DataPipeline, BuilingIdsEnum\n",
    "pipe = DataPipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "referance_buildings = pipe.get_reference_buildings_import_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "referance_buildings\n",
    "# select these columns from the data\n",
    "# value\ttimestamp temperature area wind_speed\twind_direction\tcloud_fraction\tprecipitation\n",
    "\n",
    "data_for_model = referance_buildings[['value', 'timestamp', 'temperature', 'area']]# 'wind_speed', 'wind_direction', 'cloud_fraction', 'precipitation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124404\n",
      "123480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42775</th>\n",
       "      <td>39.20</td>\n",
       "      <td>2024-05-30 11:00:00</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>16.32</td>\n",
       "      <td>2024-05-17 06:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33877</th>\n",
       "      <td>28.80</td>\n",
       "      <td>2024-03-17 07:00:00</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>28.80</td>\n",
       "      <td>2023-10-17 01:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25549</th>\n",
       "      <td>31.50</td>\n",
       "      <td>2024-01-07 21:00:00</td>\n",
       "      <td>-12.7</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value           timestamp  temperature  area\n",
       "42775  39.20 2024-05-30 11:00:00         16.5  1167\n",
       "41187  16.32 2024-05-17 06:00:00         10.0  1095\n",
       "33877  28.80 2024-03-17 07:00:00         -1.8  1384\n",
       "15563  28.80 2023-10-17 01:00:00          7.0  1384\n",
       "25549  31.50 2024-01-07 21:00:00        -12.7  1384"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming you have a DataFrame called 'referance_buildings'\n",
    "print(data_for_model.size)\n",
    "data_for_model = data_for_model.dropna()  # Drop missing values if any\n",
    "print(data_for_model.size)\n",
    "\n",
    "# Convert timestamp column to datetime and use numerical features\n",
    "data_for_model['timestamp'] = pd.to_datetime(data_for_model['timestamp'])\n",
    "\n",
    "\n",
    "target = 'value'\n",
    "\n",
    "# Split into train and test\n",
    "train_data = data_for_model.sample(frac=0.8, random_state=42)\n",
    "test_data = data_for_model.drop(train_data.index)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241022_161331\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       14.18 GB / 32.00 GB (44.3%)\n",
      "Disk Space Avail:   651.30 GB / 926.35 GB (70.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241022_161331/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241022_161331/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    21952\n",
      "Train Data Columns: 3\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14508.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.2s = Fit runtime\n",
      "\t3 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.17 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.55s of the 899.54s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` \n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.46984\n",
      "[2000]\tvalid_set's l1: 6.25179\n",
      "[3000]\tvalid_set's l1: 6.1261\n",
      "[4000]\tvalid_set's l1: 6.04842\n",
      "[5000]\tvalid_set's l1: 5.98916\n",
      "[6000]\tvalid_set's l1: 5.94268\n",
      "[7000]\tvalid_set's l1: 5.90909\n",
      "[8000]\tvalid_set's l1: 5.88409\n",
      "[9000]\tvalid_set's l1: 5.85885\n",
      "[10000]\tvalid_set's l1: 5.83787\n",
      "[1000]\tvalid_set's l1: 6.52465\n",
      "[2000]\tvalid_set's l1: 6.30463\n",
      "[3000]\tvalid_set's l1: 6.20355\n",
      "[4000]\tvalid_set's l1: 6.13161\n",
      "[5000]\tvalid_set's l1: 6.08886\n",
      "[6000]\tvalid_set's l1: 6.05919\n",
      "[7000]\tvalid_set's l1: 6.04067\n",
      "[8000]\tvalid_set's l1: 6.02277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8551. Best iteration is:\n",
      "\t[8551]\tvalid_set's l1: 6.01078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.45555\n",
      "[2000]\tvalid_set's l1: 6.19411\n",
      "[3000]\tvalid_set's l1: 6.06615\n",
      "[4000]\tvalid_set's l1: 5.97791\n",
      "[5000]\tvalid_set's l1: 5.91951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5935. Best iteration is:\n",
      "\t[5934]\tvalid_set's l1: 5.87858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.47838\n",
      "[2000]\tvalid_set's l1: 6.30236\n",
      "[3000]\tvalid_set's l1: 6.21034\n",
      "[4000]\tvalid_set's l1: 6.14842\n",
      "[5000]\tvalid_set's l1: 6.11519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5285. Best iteration is:\n",
      "\t[5278]\tvalid_set's l1: 6.10069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.42623\n",
      "[2000]\tvalid_set's l1: 6.20893\n",
      "[3000]\tvalid_set's l1: 6.1045\n",
      "[4000]\tvalid_set's l1: 6.0416\n",
      "[5000]\tvalid_set's l1: 5.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5489. Best iteration is:\n",
      "\t[5478]\tvalid_set's l1: 5.96364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.56373\n",
      "[2000]\tvalid_set's l1: 6.29642\n",
      "[3000]\tvalid_set's l1: 6.13752\n",
      "[4000]\tvalid_set's l1: 6.0337\n",
      "[5000]\tvalid_set's l1: 5.97322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5905. Best iteration is:\n",
      "\t[5905]\tvalid_set's l1: 5.93127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.4523\n",
      "[2000]\tvalid_set's l1: 6.21886\n",
      "[3000]\tvalid_set's l1: 6.10275\n",
      "[4000]\tvalid_set's l1: 6.02226\n",
      "[5000]\tvalid_set's l1: 5.96461\n",
      "[6000]\tvalid_set's l1: 5.92221\n",
      "[7000]\tvalid_set's l1: 5.89322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 7604. Best iteration is:\n",
      "\t[7604]\tvalid_set's l1: 5.8764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.45606\n",
      "[2000]\tvalid_set's l1: 6.24441\n",
      "[3000]\tvalid_set's l1: 6.12707\n",
      "[4000]\tvalid_set's l1: 6.05111\n",
      "[5000]\tvalid_set's l1: 5.99408\n",
      "[6000]\tvalid_set's l1: 5.94821\n",
      "[7000]\tvalid_set's l1: 5.91824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 7445. Best iteration is:\n",
      "\t[7444]\tvalid_set's l1: 5.907\n",
      "\t-5.9382\t = Validation score   (-mean_absolute_error)\n",
      "\t572.33s\t = Training   runtime\n",
      "\t2.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 23.65s of the 323.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 194. Best iteration is:\n",
      "\t[194]\tvalid_set's l1: 5.00855\n",
      "\tRan out of time, early stopping on iteration 165. Best iteration is:\n",
      "\t[165]\tvalid_set's l1: 5.16707\n",
      "\tRan out of time, early stopping on iteration 188. Best iteration is:\n",
      "\t[188]\tvalid_set's l1: 5.0601\n",
      "\tRan out of time, early stopping on iteration 239. Best iteration is:\n",
      "\t[239]\tvalid_set's l1: 5.05407\n",
      "\tRan out of time, early stopping on iteration 212. Best iteration is:\n",
      "\t[211]\tvalid_set's l1: 4.98378\n",
      "\tRan out of time, early stopping on iteration 222. Best iteration is:\n",
      "\t[222]\tvalid_set's l1: 5.09344\n",
      "\tRan out of time, early stopping on iteration 239. Best iteration is:\n",
      "\t[239]\tvalid_set's l1: 4.95834\n",
      "\tRan out of time, early stopping on iteration 288. Best iteration is:\n",
      "\t[288]\tvalid_set's l1: 4.87715\n",
      "\t-5.0253\t = Validation score   (-mean_absolute_error)\n",
      "\t22.63s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.91s of the 300.9s of remaining time.\n",
      "\t-3.374\t = Validation score   (-mean_absolute_error)\n",
      "\t2.51s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 297.21s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.374\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 297.18s of the 297.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.411\t = Validation score   (-mean_absolute_error)\n",
      "\t50.3s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 246.65s of the 246.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3763\t = Validation score   (-mean_absolute_error)\n",
      "\t23.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 222.99s of the 222.97s of remaining time.\n",
      "\t-3.4567\t = Validation score   (-mean_absolute_error)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 215.75s of the 215.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3623\t = Validation score   (-mean_absolute_error)\n",
      "\t8.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 206.74s of the 206.72s of remaining time.\n",
      "\t-3.4424\t = Validation score   (-mean_absolute_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 204.12s of the 204.1s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 25)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.3385\t = Validation score   (-mean_absolute_error)\n",
      "\t170.72s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 32.81s of the 32.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3861\t = Validation score   (-mean_absolute_error)\n",
      "\t5.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 27.58s of the 27.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.4333\t = Validation score   (-mean_absolute_error)\n",
      "\t26.0s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1.0s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.609, 'NeuralNetTorch_BAG_L2': 0.174, 'RandomForestMSE_BAG_L2': 0.087, 'LightGBM_BAG_L2': 0.043, 'CatBoost_BAG_L2': 0.043, 'ExtraTreesMSE_BAG_L2': 0.043}\n",
      "\t-3.327\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 898.79s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 814.0 rows/s (2744 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241022_161331/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -3.398853  -3.326961  mean_absolute_error        4.860036       5.376656  834.295779                 0.001670                0.000433           0.063615            3       True         13\n",
      "1   NeuralNetFastAI_BAG_L2      -3.406138  -3.338526  mean_absolute_error        3.419999       3.362483  768.194449                 0.361214                0.451266         170.719025            2       True         10\n",
      "2          CatBoost_BAG_L2      -3.409576  -3.362329  mean_absolute_error        3.081040       2.919901  606.435415                 0.022255                0.008684           8.959991            2       True          8\n",
      "3          LightGBM_BAG_L2      -3.424437  -3.376310  mean_absolute_error        3.094398       2.934817  621.067124                 0.035612                0.023600          23.591700            2       True          6\n",
      "4           XGBoost_BAG_L2      -3.431094  -3.386129  mean_absolute_error        3.116323       2.931873  602.630137                 0.057538                0.020656           5.154713            2       True         11\n",
      "5        LightGBMXT_BAG_L2      -3.441531  -3.410968  mean_absolute_error        3.188334       3.017600  647.779554                 0.129549                0.106383          50.304130            2       True          5\n",
      "6    NeuralNetTorch_BAG_L2      -3.466225  -3.433338  mean_absolute_error        3.835083       3.338156  623.473163                 0.776298                0.426940          25.997740            2       True         12\n",
      "7   RandomForestMSE_BAG_L1      -3.470164  -3.374036  mean_absolute_error        0.291380       0.737413    2.508304                 0.291380                0.737413           2.508304            1       True          3\n",
      "8      WeightedEnsemble_L2      -3.470164  -3.374036  mean_absolute_error        0.293577       0.737790    2.529254                 0.002197                0.000377           0.020950            2       True          4\n",
      "9   RandomForestMSE_BAG_L2      -3.496239  -3.456719  mean_absolute_error        3.349583       3.731647  603.464087                 0.290798                0.820430           5.988663            2       True          7\n",
      "10    ExtraTreesMSE_BAG_L2      -3.505946  -3.442379  mean_absolute_error        3.372189       3.645303  598.975045                 0.313404                0.734086           1.499621            2       True          9\n",
      "11         LightGBM_BAG_L1      -5.137490  -5.025312  mean_absolute_error        0.064660       0.042973   22.633742                 0.064660                0.042973          22.633742            1       True          2\n",
      "12       LightGBMXT_BAG_L1      -5.928763  -5.938237  mean_absolute_error        2.702745       2.130830  572.333378                 2.702745                2.130830         572.333378            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t904s\t = DyStack   runtime |\t2696s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2696s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241022_161331\"\n",
      "Train Data Rows:    24696\n",
      "Train Data Columns: 3\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13876.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.57 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1796.69s of the 2695.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.42747\n",
      "[2000]\tvalid_set's l1: 6.21804\n",
      "[3000]\tvalid_set's l1: 6.11772\n",
      "[4000]\tvalid_set's l1: 6.04738\n",
      "[5000]\tvalid_set's l1: 6.00651\n",
      "[6000]\tvalid_set's l1: 5.96747\n",
      "[7000]\tvalid_set's l1: 5.94198\n",
      "[8000]\tvalid_set's l1: 5.92521\n",
      "[9000]\tvalid_set's l1: 5.90678\n",
      "[10000]\tvalid_set's l1: 5.89161\n",
      "[1000]\tvalid_set's l1: 6.46296\n",
      "[2000]\tvalid_set's l1: 6.20973\n",
      "[3000]\tvalid_set's l1: 6.07375\n",
      "[4000]\tvalid_set's l1: 5.98978\n",
      "[5000]\tvalid_set's l1: 5.9282\n",
      "[6000]\tvalid_set's l1: 5.88637\n",
      "[7000]\tvalid_set's l1: 5.85202\n",
      "[8000]\tvalid_set's l1: 5.82267\n",
      "[9000]\tvalid_set's l1: 5.79653\n",
      "[10000]\tvalid_set's l1: 5.77433\n",
      "[1000]\tvalid_set's l1: 6.39936\n",
      "[2000]\tvalid_set's l1: 6.17475\n",
      "[3000]\tvalid_set's l1: 6.05096\n",
      "[4000]\tvalid_set's l1: 5.97211\n",
      "[5000]\tvalid_set's l1: 5.92719\n",
      "[6000]\tvalid_set's l1: 5.88829\n",
      "[7000]\tvalid_set's l1: 5.85787\n",
      "[8000]\tvalid_set's l1: 5.83605\n",
      "[9000]\tvalid_set's l1: 5.81504\n",
      "[10000]\tvalid_set's l1: 5.79394\n",
      "[1000]\tvalid_set's l1: 6.52728\n",
      "[2000]\tvalid_set's l1: 6.28655\n",
      "[3000]\tvalid_set's l1: 6.15948\n",
      "[4000]\tvalid_set's l1: 6.08007\n",
      "[5000]\tvalid_set's l1: 6.02032\n",
      "[6000]\tvalid_set's l1: 5.9725\n",
      "[7000]\tvalid_set's l1: 5.93833\n",
      "[8000]\tvalid_set's l1: 5.91013\n",
      "[9000]\tvalid_set's l1: 5.88763\n",
      "[10000]\tvalid_set's l1: 5.86665\n",
      "[1000]\tvalid_set's l1: 6.44255\n",
      "[2000]\tvalid_set's l1: 6.21069\n",
      "[3000]\tvalid_set's l1: 6.08252\n",
      "[4000]\tvalid_set's l1: 5.99782\n",
      "[5000]\tvalid_set's l1: 5.93009\n",
      "[6000]\tvalid_set's l1: 5.88029\n",
      "[7000]\tvalid_set's l1: 5.83807\n",
      "[8000]\tvalid_set's l1: 5.81073\n",
      "[9000]\tvalid_set's l1: 5.78654\n",
      "[10000]\tvalid_set's l1: 5.76867\n",
      "[1000]\tvalid_set's l1: 6.44694\n",
      "[2000]\tvalid_set's l1: 6.20548\n",
      "[3000]\tvalid_set's l1: 6.08063\n",
      "[4000]\tvalid_set's l1: 6.00469\n",
      "[5000]\tvalid_set's l1: 5.94178\n",
      "[6000]\tvalid_set's l1: 5.8957\n",
      "[7000]\tvalid_set's l1: 5.86721\n",
      "[8000]\tvalid_set's l1: 5.84461\n",
      "[9000]\tvalid_set's l1: 5.81994\n",
      "[10000]\tvalid_set's l1: 5.80114\n",
      "[1000]\tvalid_set's l1: 6.35329\n",
      "[2000]\tvalid_set's l1: 6.10703\n",
      "[3000]\tvalid_set's l1: 5.98869\n",
      "[4000]\tvalid_set's l1: 5.92027\n",
      "[5000]\tvalid_set's l1: 5.88353\n",
      "[6000]\tvalid_set's l1: 5.84824\n",
      "[7000]\tvalid_set's l1: 5.8243\n",
      "[8000]\tvalid_set's l1: 5.79922\n",
      "[9000]\tvalid_set's l1: 5.78567\n",
      "[10000]\tvalid_set's l1: 5.77365\n",
      "[1000]\tvalid_set's l1: 6.55403\n",
      "[2000]\tvalid_set's l1: 6.33972\n",
      "[3000]\tvalid_set's l1: 6.22393\n",
      "[4000]\tvalid_set's l1: 6.15212\n",
      "[5000]\tvalid_set's l1: 6.1056\n",
      "[6000]\tvalid_set's l1: 6.06569\n",
      "[7000]\tvalid_set's l1: 6.03288\n",
      "[8000]\tvalid_set's l1: 6.00809\n",
      "[9000]\tvalid_set's l1: 5.98641\n",
      "[10000]\tvalid_set's l1: 5.97005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.8298\t = Validation score   (-mean_absolute_error)\n",
      "\t551.64s\t = Training   runtime\n",
      "\t3.72s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1239.34s of the 2138.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.65888\n",
      "[2000]\tvalid_set's l1: 4.55713\n",
      "[3000]\tvalid_set's l1: 4.53075\n",
      "[4000]\tvalid_set's l1: 4.52481\n",
      "[1000]\tvalid_set's l1: 4.49192\n",
      "[2000]\tvalid_set's l1: 4.36629\n",
      "[3000]\tvalid_set's l1: 4.32777\n",
      "[4000]\tvalid_set's l1: 4.31487\n",
      "[5000]\tvalid_set's l1: 4.32965\n",
      "[1000]\tvalid_set's l1: 4.53553\n",
      "[2000]\tvalid_set's l1: 4.40172\n",
      "[3000]\tvalid_set's l1: 4.35444\n",
      "[4000]\tvalid_set's l1: 4.33778\n",
      "[5000]\tvalid_set's l1: 4.33562\n",
      "[1000]\tvalid_set's l1: 4.57951\n",
      "[2000]\tvalid_set's l1: 4.44122\n",
      "[3000]\tvalid_set's l1: 4.39571\n",
      "[4000]\tvalid_set's l1: 4.37066\n",
      "[5000]\tvalid_set's l1: 4.37206\n",
      "[1000]\tvalid_set's l1: 4.61265\n",
      "[2000]\tvalid_set's l1: 4.49053\n",
      "[3000]\tvalid_set's l1: 4.44925\n",
      "[4000]\tvalid_set's l1: 4.44189\n",
      "[5000]\tvalid_set's l1: 4.43369\n",
      "[6000]\tvalid_set's l1: 4.43709\n",
      "[1000]\tvalid_set's l1: 4.52619\n",
      "[2000]\tvalid_set's l1: 4.37083\n",
      "[3000]\tvalid_set's l1: 4.32105\n",
      "[4000]\tvalid_set's l1: 4.30479\n",
      "[5000]\tvalid_set's l1: 4.29349\n",
      "[6000]\tvalid_set's l1: 4.29437\n",
      "[7000]\tvalid_set's l1: 4.29768\n",
      "[8000]\tvalid_set's l1: 4.30397\n",
      "[1000]\tvalid_set's l1: 4.61311\n",
      "[2000]\tvalid_set's l1: 4.48236\n",
      "[3000]\tvalid_set's l1: 4.43409\n",
      "[4000]\tvalid_set's l1: 4.41296\n",
      "[5000]\tvalid_set's l1: 4.40561\n",
      "[6000]\tvalid_set's l1: 4.40466\n",
      "[1000]\tvalid_set's l1: 4.55168\n",
      "[2000]\tvalid_set's l1: 4.42572\n",
      "[3000]\tvalid_set's l1: 4.38315\n",
      "[4000]\tvalid_set's l1: 4.35788\n",
      "[5000]\tvalid_set's l1: 4.35062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.3752\t = Validation score   (-mean_absolute_error)\n",
      "\t598.38s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 638.95s of the 1537.96s of remaining time.\n",
      "\t-3.2579\t = Validation score   (-mean_absolute_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 635.52s of the 1534.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 6782.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 581.79s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.2579\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 581.76s of the 581.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.2852\t = Validation score   (-mean_absolute_error)\n",
      "\t71.17s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 510.37s of the 510.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.2515\t = Validation score   (-mean_absolute_error)\n",
      "\t26.88s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 483.39s of the 483.37s of remaining time.\n",
      "\t-3.2979\t = Validation score   (-mean_absolute_error)\n",
      "\t5.95s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 476.22s of the 476.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.2361\t = Validation score   (-mean_absolute_error)\n",
      "\t8.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 467.91s of the 467.89s of remaining time.\n",
      "\t-3.2768\t = Validation score   (-mean_absolute_error)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 465.41s of the 465.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.2113\t = Validation score   (-mean_absolute_error)\n",
      "\t161.98s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 303.07s of the 303.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.2564\t = Validation score   (-mean_absolute_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 299.17s of the 299.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 32)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 34)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 37)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 31)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 38)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 36)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.1448\t = Validation score   (-mean_absolute_error)\n",
      "\t280.8s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 18.11s of the 18.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[122]\tvalid_set's l1: 3.28628\n",
      "\tRan out of time, early stopping on iteration 127. Best iteration is:\n",
      "\t[127]\tvalid_set's l1: 3.239\n",
      "\tRan out of time, early stopping on iteration 154. Best iteration is:\n",
      "\t[148]\tvalid_set's l1: 3.30231\n",
      "\tRan out of time, early stopping on iteration 159. Best iteration is:\n",
      "\t[155]\tvalid_set's l1: 3.31042\n",
      "\tRan out of time, early stopping on iteration 171. Best iteration is:\n",
      "\t[170]\tvalid_set's l1: 3.24914\n",
      "\tRan out of time, early stopping on iteration 152. Best iteration is:\n",
      "\t[151]\tvalid_set's l1: 3.24166\n",
      "\tRan out of time, early stopping on iteration 200. Best iteration is:\n",
      "\t[200]\tvalid_set's l1: 3.26176\n",
      "\tRan out of time, early stopping on iteration 231. Best iteration is:\n",
      "\t[201]\tvalid_set's l1: 3.28975\n",
      "\t-3.2725\t = Validation score   (-mean_absolute_error)\n",
      "\t17.27s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 0.53s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L2': 0.75, 'RandomForestMSE_BAG_L2': 0.083, 'ExtraTreesMSE_BAG_L2': 0.083, 'NeuralNetFastAI_BAG_L2': 0.083}\n",
      "\t-3.1334\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2695.32s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 555.6 rows/s (3087 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241022_161331\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    # presets='medium_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 3 features using 5000 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Performance:\n",
      "{'mean_absolute_error': -3.1217798479004197, 'root_mean_squared_error': -4.916806183789267, 'mean_squared_error': -24.174983048948373, 'r2': 0.8285978268071899, 'pearsonr': 0.9103643086413618, 'median_absolute_error': -1.762607803344725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t194.55s\t= Expected runtime (38.91s per shuffle set)\n",
      "\t162.11s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             importance    stddev       p_value  n  p99_high   p99_low\n",
      "timestamp      5.904354  0.120787  2.100541e-08  5  6.153056  5.655651\n",
      "area           5.713944  0.122575  2.539637e-08  5  5.966327  5.461561\n",
      "temperature    5.251591  0.101069  1.645395e-08  5  5.459692  5.043489\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "print(performance)  # This will show various metrics such as R^2, RMSE, etc.\n",
    "\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54451</th>\n",
       "      <td>2024-09-04 19:00:00</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54456</th>\n",
       "      <td>2024-09-04 20:00:00</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54462</th>\n",
       "      <td>2024-09-04 21:00:00</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54468</th>\n",
       "      <td>2024-09-04 22:00:00</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54475</th>\n",
       "      <td>2024-09-04 23:00:00</td>\n",
       "      <td>17.4</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  temperature  area\n",
       "4     2023-07-01 00:00:00         13.6  1199\n",
       "8     2023-07-01 01:00:00         13.2  1199\n",
       "16    2023-07-01 02:00:00         12.3  1199\n",
       "20    2023-07-01 03:00:00         11.9  1199\n",
       "29    2023-07-01 04:00:00         11.9  1199\n",
       "...                   ...          ...   ...\n",
       "54451 2024-09-04 19:00:00         17.5  1199\n",
       "54456 2024-09-04 20:00:00         17.5  1199\n",
       "54462 2024-09-04 21:00:00         17.5  1199\n",
       "54468 2024-09-04 22:00:00         17.5  1199\n",
       "54475 2024-09-04 23:00:00         17.4  1199\n",
       "\n",
       "[10343 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_import_data_for_building(BuilingIdsEnum.MAIN.value)\n",
    "main_building\n",
    "\n",
    "data_predict = main_building[['timestamp', 'temperature', 'area']] #'wind_speed', 'wind_direction', 'cloud_fraction', 'precipitation'\n",
    "# data_predict['timestamp'] = pd.to_datetime(data_predict['timestamp'])\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "my_path = Path().resolve().parent / 'data' / 'prediciton_3features.csv'\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
