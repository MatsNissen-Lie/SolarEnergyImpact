{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/Desktop/SolarEnergyImpact\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().absolute().parent.parent\n",
    "print(root)\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from src.pipeline import Pipeline, BuilingIdsEnum\n",
    "pipe = Pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_b = pipe.get_data(BuilingIdsEnum.B)\n",
    "building_c = pipe.get_data(BuilingIdsEnum.C)\n",
    "building_a = pipe.get_data(BuilingIdsEnum.A)\n",
    "\n",
    "# from november 1. to 1. mars only use data from dataset b \n",
    "# else use data from dataset c and a\n",
    "\n",
    "# remove from 2024-01-15 from dataset A\n",
    "mask_b = (building_b['timestamp'] >= '2024-01-15') & (building_b['timestamp'] <= '2024-01-15')\n",
    "building_b = building_b[~mask_b]\n",
    "\n",
    "start_winter = '2023-11-01'\n",
    "end_winter = '2024-03-01'\n",
    "mask_b = (building_b['timestamp'] >= start_winter) & (building_b['timestamp'] <= end_winter)\n",
    "building_b = building_b[mask_b]\n",
    "\n",
    "mask_c = (building_c['timestamp'] < start_winter) | (building_c['timestamp'] > end_winter)\n",
    "building_c = building_c[mask_c]\n",
    "\n",
    "# building a, only before 2023-09-17.\n",
    "end_dataset_a = '2023-09-17'\n",
    "mask_a = (building_a['timestamp'] < end_dataset_a)\n",
    "building_a = building_a[mask_a]\n",
    "\n",
    "\n",
    "\n",
    "# normalize the value_import by area and call it value for main and building_b\n",
    "building_b['value'] = building_b['value_import'] / building_b['area']\n",
    "building_c['value'] = building_c['net_consumption'] / building_c['area']\n",
    "building_a['value'] = building_a['value_import'] / building_a['area']\n",
    "\n",
    "\n",
    "combined_df = pd.concat([building_a, building_b, building_c], ignore_index=True)\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# Perform the train-test split with stratification based on 'building_id'\n",
    "train_data, test_data = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['building']\n",
    ")\n",
    "\n",
    "# select features\n",
    "target = \"value\"\n",
    "features = [\"timestamp\", \"temperature\"] #, \"cloud_fraction\", \"precipitation\", \"area\", \"wind_speed\"\n",
    "\n",
    "train_data = train_data[features + [target]]\n",
    "test_data = test_data[features + [target]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241113_193908\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       13.50 GB / 32.00 GB (42.2%)\n",
      "Disk Space Avail:   607.65 GB / 926.35 GB (65.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241113_193908/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241113_193908/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    8702\n",
      "Train Data Columns: 2\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13743.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.1s = Fit runtime\n",
      "\t2 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.40 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.78s of the 899.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00495001\n",
      "[2000]\tvalid_set's l1: 0.00467688\n",
      "[3000]\tvalid_set's l1: 0.00451896\n",
      "[4000]\tvalid_set's l1: 0.00441288\n",
      "[5000]\tvalid_set's l1: 0.00433018\n",
      "[6000]\tvalid_set's l1: 0.00426813\n",
      "[7000]\tvalid_set's l1: 0.00421587\n",
      "[8000]\tvalid_set's l1: 0.00418395\n",
      "[9000]\tvalid_set's l1: 0.00415196\n",
      "[10000]\tvalid_set's l1: 0.00413354\n",
      "[1000]\tvalid_set's l1: 0.00482558\n",
      "[2000]\tvalid_set's l1: 0.00456458\n",
      "[3000]\tvalid_set's l1: 0.00442977\n",
      "[4000]\tvalid_set's l1: 0.00435085\n",
      "[5000]\tvalid_set's l1: 0.0043035\n",
      "[6000]\tvalid_set's l1: 0.00426043\n",
      "[7000]\tvalid_set's l1: 0.00423126\n",
      "[8000]\tvalid_set's l1: 0.0042121\n",
      "[9000]\tvalid_set's l1: 0.00419536\n",
      "[10000]\tvalid_set's l1: 0.00418205\n",
      "[1000]\tvalid_set's l1: 0.00495874\n",
      "[2000]\tvalid_set's l1: 0.00471132\n",
      "[3000]\tvalid_set's l1: 0.00457823\n",
      "[4000]\tvalid_set's l1: 0.00448806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4903. Best iteration is:\n",
      "\t[4903]\tvalid_set's l1: 0.0044302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00480689\n",
      "[2000]\tvalid_set's l1: 0.00456495\n",
      "[3000]\tvalid_set's l1: 0.00442552\n",
      "[4000]\tvalid_set's l1: 0.00433315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4091. Best iteration is:\n",
      "\t[4073]\tvalid_set's l1: 0.00432586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00489035\n",
      "[2000]\tvalid_set's l1: 0.00462095\n",
      "[3000]\tvalid_set's l1: 0.0044993\n",
      "[4000]\tvalid_set's l1: 0.0044304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4256. Best iteration is:\n",
      "\t[4256]\tvalid_set's l1: 0.00441864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00528375\n",
      "[2000]\tvalid_set's l1: 0.00499781\n",
      "[3000]\tvalid_set's l1: 0.00485029\n",
      "[4000]\tvalid_set's l1: 0.00474947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4362. Best iteration is:\n",
      "\t[4355]\tvalid_set's l1: 0.00471804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00502927\n",
      "[2000]\tvalid_set's l1: 0.00474243\n",
      "[3000]\tvalid_set's l1: 0.00460859\n",
      "[4000]\tvalid_set's l1: 0.00451767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4792. Best iteration is:\n",
      "\t[4792]\tvalid_set's l1: 0.00446831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00487628\n",
      "[2000]\tvalid_set's l1: 0.00463636\n",
      "[3000]\tvalid_set's l1: 0.00453188\n",
      "[4000]\tvalid_set's l1: 0.0044689\n",
      "[5000]\tvalid_set's l1: 0.00443492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5645. Best iteration is:\n",
      "\t[5631]\tvalid_set's l1: 0.00441392\n",
      "\t-0.0044\t = Validation score   (-mean_absolute_error)\n",
      "\t573.17s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 24.08s of the 324.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 141. Best iteration is:\n",
      "\t[141]\tvalid_set's l1: 0.00463086\n",
      "\tRan out of time, early stopping on iteration 145. Best iteration is:\n",
      "\t[145]\tvalid_set's l1: 0.00444555\n",
      "\tRan out of time, early stopping on iteration 151. Best iteration is:\n",
      "\t[151]\tvalid_set's l1: 0.0045299\n",
      "\tRan out of time, early stopping on iteration 155. Best iteration is:\n",
      "\t[154]\tvalid_set's l1: 0.00432945\n",
      "\tRan out of time, early stopping on iteration 166. Best iteration is:\n",
      "\t[164]\tvalid_set's l1: 0.00439823\n",
      "\tRan out of time, early stopping on iteration 157. Best iteration is:\n",
      "\t[157]\tvalid_set's l1: 0.00478295\n",
      "\tRan out of time, early stopping on iteration 182. Best iteration is:\n",
      "\t[182]\tvalid_set's l1: 0.00447709\n",
      "\tRan out of time, early stopping on iteration 220. Best iteration is:\n",
      "\t[220]\tvalid_set's l1: 0.00444562\n",
      "\t-0.0045\t = Validation score   (-mean_absolute_error)\n",
      "\t23.07s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.92s of the 301.03s of remaining time.\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 298.87s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 298.85s of the 298.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t40.78s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 257.87s of the 257.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t22.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 235.14s of the 235.1s of remaining time.\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 231.32s of the 231.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t13.27s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 217.97s of the 217.94s of remaining time.\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 216.6s of the 216.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t197.88s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 18.1s of the 18.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t9.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 7.94s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.286, 'ExtraTreesMSE_BAG_L2': 0.286, 'RandomForestMSE_BAG_L2': 0.143, 'CatBoost_BAG_L2': 0.143, 'NeuralNetFastAI_BAG_L2': 0.143}\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 892.13s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 573.7 rows/s (1088 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241113_193908/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0          LightGBM_BAG_L2      -0.003248  -0.003404  mean_absolute_error        1.795770       1.605360  620.483222                 0.093636                0.016979          22.674984            2       True          6\n",
      "1        LightGBMXT_BAG_L2      -0.003253  -0.003439  mean_absolute_error        1.773401       1.676532  638.584210                 0.071266                0.088151          40.775973            2       True          5\n",
      "2          CatBoost_BAG_L2      -0.003255  -0.003389  mean_absolute_error        1.733690       1.614220  611.082353                 0.031555                0.025839          13.274115            2       True          8\n",
      "3   NeuralNetFastAI_BAG_L2      -0.003260  -0.003418  mean_absolute_error        2.178675       2.084300  795.688762                 0.476541                0.495919         197.880525            2       True         10\n",
      "4           XGBoost_BAG_L2      -0.003263  -0.003411  mean_absolute_error        1.769889       1.607540  607.710793                 0.067754                0.019160           9.902556            2       True         11\n",
      "5      WeightedEnsemble_L3      -0.003268  -0.003355  mean_absolute_error        2.512050       2.718206  813.215291                 0.001123                0.000415           0.051029            3       True         12\n",
      "6     ExtraTreesMSE_BAG_L2      -0.003299  -0.003396  mean_absolute_error        1.857935       1.899089  598.690763                 0.155800                0.310708           0.882525            2       True          9\n",
      "7   RandomForestMSE_BAG_L2      -0.003328  -0.003417  mean_absolute_error        1.847030       1.885325  601.127097                 0.144896                0.296944           3.318859            2       True          7\n",
      "8   RandomForestMSE_BAG_L1      -0.003339  -0.003402  mean_absolute_error        0.189642       0.331034    1.574988                 0.189642                0.331034           1.574988            1       True          3\n",
      "9      WeightedEnsemble_L2      -0.003339  -0.003402  mean_absolute_error        0.191742       0.331402    1.589790                 0.002100                0.000368           0.014802            2       True          4\n",
      "10       LightGBMXT_BAG_L1      -0.004284  -0.004386  mean_absolute_error        1.437454       1.231811  573.165965                 1.437454                1.231811         573.165965            1       True          1\n",
      "11         LightGBM_BAG_L1      -0.004354  -0.004505  mean_absolute_error        0.075039       0.025535   23.067284                 0.075039                0.025535          23.067284            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t895s\t = DyStack   runtime |\t2705s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2705s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241113_193908\"\n",
      "Train Data Rows:    9790\n",
      "Train Data Columns: 2\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13580.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t2 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1802.81s of the 2704.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00484199\n",
      "[2000]\tvalid_set's l1: 0.00457062\n",
      "[3000]\tvalid_set's l1: 0.00444799\n",
      "[4000]\tvalid_set's l1: 0.0043723\n",
      "[5000]\tvalid_set's l1: 0.00432884\n",
      "[6000]\tvalid_set's l1: 0.00428791\n",
      "[7000]\tvalid_set's l1: 0.00426231\n",
      "[8000]\tvalid_set's l1: 0.00424727\n",
      "[9000]\tvalid_set's l1: 0.00423461\n",
      "[10000]\tvalid_set's l1: 0.00421675\n",
      "[1000]\tvalid_set's l1: 0.00503942\n",
      "[2000]\tvalid_set's l1: 0.00476434\n",
      "[3000]\tvalid_set's l1: 0.0046126\n",
      "[4000]\tvalid_set's l1: 0.00450822\n",
      "[5000]\tvalid_set's l1: 0.00443723\n",
      "[6000]\tvalid_set's l1: 0.00439485\n",
      "[7000]\tvalid_set's l1: 0.00436178\n",
      "[8000]\tvalid_set's l1: 0.0043363\n",
      "[9000]\tvalid_set's l1: 0.00431617\n",
      "[10000]\tvalid_set's l1: 0.00429268\n",
      "[1000]\tvalid_set's l1: 0.00487765\n",
      "[2000]\tvalid_set's l1: 0.00460555\n",
      "[3000]\tvalid_set's l1: 0.00445573\n",
      "[4000]\tvalid_set's l1: 0.00436439\n",
      "[5000]\tvalid_set's l1: 0.00430051\n",
      "[6000]\tvalid_set's l1: 0.0042445\n",
      "[7000]\tvalid_set's l1: 0.00420249\n",
      "[8000]\tvalid_set's l1: 0.0041662\n",
      "[9000]\tvalid_set's l1: 0.00413638\n",
      "[10000]\tvalid_set's l1: 0.00410622\n",
      "[1000]\tvalid_set's l1: 0.00482593\n",
      "[2000]\tvalid_set's l1: 0.00456892\n",
      "[3000]\tvalid_set's l1: 0.00441907\n",
      "[4000]\tvalid_set's l1: 0.00432796\n",
      "[5000]\tvalid_set's l1: 0.00426056\n",
      "[6000]\tvalid_set's l1: 0.00421188\n",
      "[7000]\tvalid_set's l1: 0.00417887\n",
      "[8000]\tvalid_set's l1: 0.00415193\n",
      "[9000]\tvalid_set's l1: 0.00412435\n",
      "[10000]\tvalid_set's l1: 0.00410959\n",
      "[1000]\tvalid_set's l1: 0.00486021\n",
      "[2000]\tvalid_set's l1: 0.00459207\n",
      "[3000]\tvalid_set's l1: 0.00447862\n",
      "[4000]\tvalid_set's l1: 0.0043963\n",
      "[5000]\tvalid_set's l1: 0.00433502\n",
      "[6000]\tvalid_set's l1: 0.00429139\n",
      "[7000]\tvalid_set's l1: 0.00426126\n",
      "[8000]\tvalid_set's l1: 0.0042351\n",
      "[9000]\tvalid_set's l1: 0.00421307\n",
      "[10000]\tvalid_set's l1: 0.00419365\n",
      "[1000]\tvalid_set's l1: 0.00484773\n",
      "[2000]\tvalid_set's l1: 0.00456617\n",
      "[3000]\tvalid_set's l1: 0.00440876\n",
      "[4000]\tvalid_set's l1: 0.00432779\n",
      "[5000]\tvalid_set's l1: 0.00426835\n",
      "[6000]\tvalid_set's l1: 0.00422529\n",
      "[7000]\tvalid_set's l1: 0.00419299\n",
      "[8000]\tvalid_set's l1: 0.00416423\n",
      "[9000]\tvalid_set's l1: 0.00414013\n",
      "[10000]\tvalid_set's l1: 0.00411846\n",
      "[1000]\tvalid_set's l1: 0.0049219\n",
      "[2000]\tvalid_set's l1: 0.00470274\n",
      "[3000]\tvalid_set's l1: 0.00459015\n",
      "[4000]\tvalid_set's l1: 0.00451397\n",
      "[5000]\tvalid_set's l1: 0.00446145\n",
      "[6000]\tvalid_set's l1: 0.00441645\n",
      "[7000]\tvalid_set's l1: 0.00437636\n",
      "[8000]\tvalid_set's l1: 0.00434613\n",
      "[9000]\tvalid_set's l1: 0.00431751\n",
      "[10000]\tvalid_set's l1: 0.00429152\n",
      "[1000]\tvalid_set's l1: 0.00503191\n",
      "[2000]\tvalid_set's l1: 0.0047763\n",
      "[3000]\tvalid_set's l1: 0.00463326\n",
      "[4000]\tvalid_set's l1: 0.00453916\n",
      "[5000]\tvalid_set's l1: 0.00448197\n",
      "[6000]\tvalid_set's l1: 0.00443175\n",
      "[7000]\tvalid_set's l1: 0.00439917\n",
      "[8000]\tvalid_set's l1: 0.0043703\n",
      "[9000]\tvalid_set's l1: 0.0043429\n",
      "[10000]\tvalid_set's l1: 0.00432539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t1266.33s\t = Training   runtime\n",
      "\t2.33s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 531.7s of the 1433.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00410531\n",
      "[2000]\tvalid_set's l1: 0.00407824\n",
      "[1000]\tvalid_set's l1: 0.00428664\n",
      "[2000]\tvalid_set's l1: 0.00425965\n",
      "[1000]\tvalid_set's l1: 0.00406805\n",
      "[2000]\tvalid_set's l1: 0.00400745\n",
      "[1000]\tvalid_set's l1: 0.00406964\n",
      "[2000]\tvalid_set's l1: 0.00406472\n",
      "[1000]\tvalid_set's l1: 0.00425099\n",
      "[2000]\tvalid_set's l1: 0.00419202\n",
      "[3000]\tvalid_set's l1: 0.00418371\n",
      "[4000]\tvalid_set's l1: 0.00418067\n",
      "[1000]\tvalid_set's l1: 0.00402891\n",
      "[2000]\tvalid_set's l1: 0.00401465\n",
      "[1000]\tvalid_set's l1: 0.00424272\n",
      "[2000]\tvalid_set's l1: 0.00419308\n",
      "[3000]\tvalid_set's l1: 0.00418739\n",
      "[1000]\tvalid_set's l1: 0.00424935\n",
      "[2000]\tvalid_set's l1: 0.00421274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0041\t = Validation score   (-mean_absolute_error)\n",
      "\t160.69s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 369.9s of the 1271.97s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 367.68s of the 1269.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 6225.\n",
      "\tRan out of time, early stopping on iteration 6315.\n",
      "\tRan out of time, early stopping on iteration 6300.\n",
      "\tRan out of time, early stopping on iteration 6481.\n",
      "\tRan out of time, early stopping on iteration 6813.\n",
      "\tRan out of time, early stopping on iteration 6920.\n",
      "\tRan out of time, early stopping on iteration 7302.\n",
      "\tRan out of time, early stopping on iteration 8418.\n",
      "\t-0.0041\t = Validation score   (-mean_absolute_error)\n",
      "\t352.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 14.93s of the 917.0s of remaining time.\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 13.81s of the 915.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 11.53s of the 913.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t8.11s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3.34s of the 905.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2.65s of the 904.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 10. Best iteration is:\n",
      "\t[10]\tvalid_set's l1: 0.00653724\n",
      "\tRan out of time, early stopping on iteration 11. Best iteration is:\n",
      "\t[11]\tvalid_set's l1: 0.00650942\n",
      "\tRan out of time, early stopping on iteration 8. Best iteration is:\n",
      "\t[8]\tvalid_set's l1: 0.00656327\n",
      "\tRan out of time, early stopping on iteration 11. Best iteration is:\n",
      "\t[11]\tvalid_set's l1: 0.00642958\n",
      "\tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\t[12]\tvalid_set's l1: 0.00640801\n",
      "\tRan out of time, early stopping on iteration 13. Best iteration is:\n",
      "\t[13]\tvalid_set's l1: 0.00616169\n",
      "\tRan out of time, early stopping on iteration 15. Best iteration is:\n",
      "\t[15]\tvalid_set's l1: 0.00615408\n",
      "\tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\t[12]\tvalid_set's l1: 0.0065322\n",
      "\t-0.0064\t = Validation score   (-mean_absolute_error)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 0.07s of the 902.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 1.\n",
      "\tTime limit exceeded... Skipping CatBoost_r177_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 901.96s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 901.92s of the 901.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t22.34s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 879.41s of the 879.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t13.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 865.41s of the 865.38s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t4.77s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 860.01s of the 859.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t14.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 845.59s of the 845.56s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 844.02s of the 843.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t272.07s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 571.46s of the 571.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t10.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 561.29s of the 561.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 53)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 54)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 66)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 72)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-0.0032\t = Validation score   (-mean_absolute_error)\n",
      "\t477.3s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 83.07s of the 83.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 389. Best iteration is:\n",
      "\t[146]\tvalid_set's l1: 0.00328771\n",
      "\tRan out of time, early stopping on iteration 397. Best iteration is:\n",
      "\t[172]\tvalid_set's l1: 0.00350637\n",
      "\tRan out of time, early stopping on iteration 199. Best iteration is:\n",
      "\t[169]\tvalid_set's l1: 0.00312831\n",
      "\tRan out of time, early stopping on iteration 141. Best iteration is:\n",
      "\t[141]\tvalid_set's l1: 0.00321097\n",
      "\tRan out of time, early stopping on iteration 131. Best iteration is:\n",
      "\t[131]\tvalid_set's l1: 0.00340022\n",
      "\tRan out of time, early stopping on iteration 68. Best iteration is:\n",
      "\t[68]\tvalid_set's l1: 0.00353181\n",
      "\tRan out of time, early stopping on iteration 47. Best iteration is:\n",
      "\t[47]\tvalid_set's l1: 0.00365098\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t79.54s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3.04s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L2': 0.667, 'RandomForestMSE_BAG_L2': 0.333}\n",
      "\t-0.0032\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2702.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 321.9 rows/s (1224 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241113_193908\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 2 features using 2448 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t205.8s\t= Expected runtime (41.16s per shuffle set)\n",
      "\t90.68s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             importance    stddev       p_value  n  p99_high   p99_low\n",
      "timestamp      0.005104  0.000103  2.021936e-08  5  0.005317  0.004891\n",
      "temperature    0.005102  0.000160  1.170895e-07  5  0.005433  0.004772\n"
     ]
    }
   ],
   "source": [
    "# AutogluonModels/ag-\n",
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "performance\n",
    "# reset index \n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': -0.003147551549112532,\n",
       " 'root_mean_squared_error': -0.004858080780383512,\n",
       " 'mean_squared_error': -2.3600948868731676e-05,\n",
       " 'r2': 0.7072616613183256,\n",
       " 'pearsonr': 0.8416842568799808,\n",
       " 'median_absolute_error': -0.0016578313675645294}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>12.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  temperature\n",
       "0     2023-07-01 00:00:00         13.6\n",
       "1     2023-07-01 01:00:00         13.2\n",
       "2     2023-07-01 02:00:00         12.3\n",
       "3     2023-07-01 03:00:00         11.9\n",
       "4     2023-07-01 04:00:00         11.9\n",
       "...                   ...          ...\n",
       "10338 2024-09-03 18:00:00         17.8\n",
       "10339 2024-09-03 19:00:00         17.8\n",
       "10340 2024-09-03 20:00:00         17.7\n",
       "10341 2024-09-03 21:00:00         18.0\n",
       "10342 2024-09-03 22:00:00         17.9\n",
       "\n",
       "[10343 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "data_predict = main_building[features]\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / 'prediction_winter.csv'\n",
    "if my_path.exists():\n",
    "    my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / f'prediction_winter2_{date_time}.csv'\n",
    "# create folder\n",
    "my_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
