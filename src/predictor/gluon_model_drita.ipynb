{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/Desktop/SolarEnergyImpact\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().absolute().parent.parent\n",
    "print(root)\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from src.pipeline import Pipeline, BuilingIdsEnum\n",
    "pipe = Pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_a = pipe.get_data(BuilingIdsEnum.A)\n",
    "building_b = pipe.get_data(BuilingIdsEnum.B)\n",
    "building_c = pipe.get_data(BuilingIdsEnum.C)\n",
    "combined_df = pd.concat([building_a, building_b, building_c])\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# Perform the train-test split with stratification based on 'building_id'\n",
    "train_data, test_data = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['building']\n",
    ")\n",
    "\n",
    "# select features\n",
    "features = [\"timestamp\", \"area\", \"temperature\", \"wind_speed\", \"cloud_fraction\", \"precipitation\"]\n",
    "target = \"value_import\"\n",
    "\n",
    "train_data = train_data[features + [target]]\n",
    "test_data = test_data[features + [target]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>value_import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21842</th>\n",
       "      <td>2023-08-16 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28067</th>\n",
       "      <td>2024-05-01 13:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>19.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18388</th>\n",
       "      <td>2024-05-30 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26603</th>\n",
       "      <td>2024-03-01 13:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24505</th>\n",
       "      <td>2023-12-05 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>2023-11-14 01:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>2024-02-16 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>2024-07-15 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>34.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14733</th>\n",
       "      <td>2023-12-29 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10130</th>\n",
       "      <td>2024-08-26 02:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6510</th>\n",
       "      <td>2024-03-28 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9120</th>\n",
       "      <td>2024-07-15 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11288</th>\n",
       "      <td>2023-08-08 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18653</th>\n",
       "      <td>2024-06-10 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>2023-08-08 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>31.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16835</th>\n",
       "      <td>2024-03-26 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15787</th>\n",
       "      <td>2024-02-11 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8940</th>\n",
       "      <td>2024-07-07 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834</th>\n",
       "      <td>2023-10-11 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11692</th>\n",
       "      <td>2023-08-25 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6871</th>\n",
       "      <td>2024-04-12 07:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23862</th>\n",
       "      <td>2023-11-08 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>2023-11-11 05:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>2023-08-18 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22951</th>\n",
       "      <td>2023-10-01 09:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19398</th>\n",
       "      <td>2024-07-11 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27544</th>\n",
       "      <td>2024-04-09 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>8.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>50.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25066</th>\n",
       "      <td>2023-12-28 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>2023-10-09 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29602</th>\n",
       "      <td>2024-07-04 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>13.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>49.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>2023-08-09 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>24.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5676</th>\n",
       "      <td>2024-02-22 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>2024-01-15 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>2024-07-23 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>2023-11-20 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>2024-08-20 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25423</th>\n",
       "      <td>2024-01-12 09:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2024-05-12 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>22.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>2024-08-18 19:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23847</th>\n",
       "      <td>2023-11-07 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26698</th>\n",
       "      <td>2024-03-05 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11785</th>\n",
       "      <td>2023-08-29 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>2024-03-10 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26525</th>\n",
       "      <td>2024-02-27 07:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24229</th>\n",
       "      <td>2023-11-23 15:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11196</th>\n",
       "      <td>2023-08-04 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>34.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17841</th>\n",
       "      <td>2024-05-07 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>2024-02-16 05:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>2023-08-19 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>2024-04-03 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25801</th>\n",
       "      <td>2024-01-28 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>2023-12-07 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2023-10-07 21:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29810</th>\n",
       "      <td>2024-07-13 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18470</th>\n",
       "      <td>2024-06-02 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>2023-09-29 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23658</th>\n",
       "      <td>2023-10-30 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14010</th>\n",
       "      <td>2023-11-29 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>2024-07-22 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>16.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>23.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25355</th>\n",
       "      <td>2024-01-09 13:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>2024-04-04 23:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15091</th>\n",
       "      <td>2024-01-13 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28431</th>\n",
       "      <td>2024-05-16 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>21.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>2024-02-20 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18389</th>\n",
       "      <td>2024-05-30 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6596</th>\n",
       "      <td>2024-03-31 20:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23362</th>\n",
       "      <td>2023-10-18 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26051</th>\n",
       "      <td>2024-02-07 13:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>2024-07-25 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15005</th>\n",
       "      <td>2024-01-10 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-17.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15453</th>\n",
       "      <td>2024-01-28 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>2023-12-20 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>2024-02-05 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29406</th>\n",
       "      <td>2024-06-26 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27102</th>\n",
       "      <td>2024-03-22 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28344</th>\n",
       "      <td>2024-05-13 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14280</th>\n",
       "      <td>2023-12-11 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>26.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15366</th>\n",
       "      <td>2024-01-25 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16784</th>\n",
       "      <td>2024-03-24 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>2023-11-06 17:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>35.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2023-07-10 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25315</th>\n",
       "      <td>2024-01-07 21:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-12.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28228</th>\n",
       "      <td>2024-05-08 06:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>2023-07-21 11:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9032</th>\n",
       "      <td>2024-07-11 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>17.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16963</th>\n",
       "      <td>2024-03-31 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5773</th>\n",
       "      <td>2024-02-26 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21481</th>\n",
       "      <td>2023-08-01 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16489</th>\n",
       "      <td>2024-03-12 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>2024-04-03 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27080</th>\n",
       "      <td>2024-03-21 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13778</th>\n",
       "      <td>2023-11-20 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12293</th>\n",
       "      <td>2023-09-19 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8.5</td>\n",
       "      <td>30.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27248</th>\n",
       "      <td>2024-03-28 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17550</th>\n",
       "      <td>2024-04-25 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>2024-03-03 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13373</th>\n",
       "      <td>2023-11-03 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>32.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15415</th>\n",
       "      <td>2024-01-27 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>2024-08-04 23:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13771</th>\n",
       "      <td>2023-11-19 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "21842 2023-08-16 04:00:00  1384         15.8         0.7             0.9   \n",
       "28067 2024-05-01 13:00:00  1384         19.7         7.0             0.0   \n",
       "18388 2024-05-30 05:00:00  1095         12.0         2.3             0.9   \n",
       "26603 2024-03-01 13:00:00  1384          5.2         3.2             1.0   \n",
       "24505 2023-12-05 03:00:00  1384         -7.7         4.0             0.5   \n",
       "3265  2023-11-14 01:00:00  1167         -0.6         3.3             0.9   \n",
       "5524  2024-02-16 04:00:00  1167          0.1         1.0             1.0   \n",
       "9126  2024-07-15 06:00:00  1167         14.2         1.8             1.0   \n",
       "14733 2023-12-29 22:00:00  1095        -10.4         0.5             0.9   \n",
       "10130 2024-08-26 02:00:00  1167          9.7         3.0             1.0   \n",
       "6510  2024-03-28 06:00:00  1167          3.1         1.4             1.0   \n",
       "9120  2024-07-15 00:00:00  1167         12.3         0.9             0.8   \n",
       "11288 2023-08-08 09:00:00  1095         15.4         0.3             1.0   \n",
       "18653 2024-06-10 06:00:00  1095          7.6         0.8             0.9   \n",
       "11286 2023-08-08 07:00:00  1095         15.2         1.5             0.9   \n",
       "16835 2024-03-26 12:00:00  1095          2.3         0.6             1.0   \n",
       "15787 2024-02-11 20:00:00  1095         -6.0         1.1             1.0   \n",
       "8940  2024-07-07 12:00:00  1167         16.3         4.4             1.0   \n",
       "12834 2023-10-11 19:00:00  1095          8.2         6.6             0.5   \n",
       "11692 2023-08-25 05:00:00  1095         13.5         0.5             1.0   \n",
       "6871  2024-04-12 07:00:00  1167          4.2         0.9             1.0   \n",
       "23862 2023-11-08 08:00:00  1384          1.8         3.0             0.6   \n",
       "3197  2023-11-11 05:00:00  1167         -2.4         1.0             0.9   \n",
       "21888 2023-08-18 02:00:00  1384         16.0         3.7             0.5   \n",
       "22951 2023-10-01 09:00:00  1384         11.2         1.7             0.6   \n",
       "19398 2024-07-11 07:00:00  1095         15.1         0.5             0.9   \n",
       "27544 2024-04-09 18:00:00  1384          8.3         6.3             1.0   \n",
       "25066 2023-12-28 12:00:00  1384         -2.1         5.1             0.9   \n",
       "2418  2023-10-09 18:00:00  1167          4.6         0.7             0.7   \n",
       "29602 2024-07-04 12:00:00  1384         13.2         3.9             1.0   \n",
       "11308 2023-08-09 05:00:00  1095         12.3         1.5             1.0   \n",
       "5676  2024-02-22 12:00:00  1167          5.2         1.0             1.0   \n",
       "4774  2024-01-15 22:00:00  1167        -15.4         0.8             0.9   \n",
       "19680 2024-07-23 01:00:00  1095         15.6         0.3             0.9   \n",
       "24159 2023-11-20 17:00:00  1384         -0.6         5.1             0.8   \n",
       "9988  2024-08-20 04:00:00  1167         12.9         1.9             1.0   \n",
       "25423 2024-01-12 09:00:00  1384         -4.3         2.2             0.9   \n",
       "7597  2024-05-12 13:00:00  1167         22.5         3.2             0.1   \n",
       "9955  2024-08-18 19:00:00  1167         15.0         1.0             0.1   \n",
       "23847 2023-11-07 17:00:00  1384          2.9         1.7             0.6   \n",
       "26698 2024-03-05 12:00:00  1384          4.3         6.9             0.2   \n",
       "11785 2023-08-29 02:00:00  1095         10.7         0.3             0.0   \n",
       "6085  2024-03-10 13:00:00  1167         -0.2         3.2             1.0   \n",
       "26525 2024-02-27 07:00:00  1384          1.6         4.2             0.9   \n",
       "24229 2023-11-23 15:00:00  1384          4.7         6.8             0.0   \n",
       "11196 2023-08-04 13:00:00  1095         18.1         0.9             1.0   \n",
       "17841 2024-05-07 10:00:00  1095         13.5         0.5             0.9   \n",
       "5525  2024-02-16 05:00:00  1167          0.2         0.4             1.0   \n",
       "1189  2023-08-19 13:00:00  1167         16.4         2.0             1.0   \n",
       "17024 2024-04-03 09:00:00  1095          0.9         2.3             0.9   \n",
       "25801 2024-01-28 03:00:00  1384          3.7         6.0             0.9   \n",
       "14193 2023-12-07 10:00:00  1095         -9.1         0.6             1.0   \n",
       "2373  2023-10-07 21:00:00  1167          2.1         1.2             0.0   \n",
       "29810 2024-07-13 04:00:00  1384         14.3         3.3             0.6   \n",
       "18470 2024-06-02 15:00:00  1095         26.0         1.8             0.3   \n",
       "2163  2023-09-29 03:00:00  1167         12.4         1.5             1.0   \n",
       "23658 2023-10-30 20:00:00  1384         -0.4         8.3             0.9   \n",
       "14010 2023-11-29 19:00:00  1095         -2.0         3.0             0.9   \n",
       "19660 2024-07-22 05:00:00  1095         16.3         0.8             1.0   \n",
       "25355 2024-01-09 13:00:00  1384         -8.1         0.9             0.2   \n",
       "27429 2024-04-04 23:00:00  1384         -1.3         6.1             1.0   \n",
       "15091 2024-01-13 20:00:00  1095         -4.9         1.1             0.5   \n",
       "28431 2024-05-16 17:00:00  1384         21.9         1.1             0.0   \n",
       "5620  2024-02-20 04:00:00  1167         -0.9         1.2             0.4   \n",
       "18389 2024-05-30 06:00:00  1095         12.1         1.2             1.0   \n",
       "6596  2024-03-31 20:00:00  1167          5.2         0.9             0.5   \n",
       "23362 2023-10-18 12:00:00  1384          9.2         4.7             0.9   \n",
       "26051 2024-02-07 13:00:00  1384         -2.4         2.2             0.4   \n",
       "30105 2024-07-25 11:00:00  1384         22.0         3.7             0.1   \n",
       "15005 2024-01-10 06:00:00  1095        -17.5         0.4             0.0   \n",
       "15453 2024-01-28 22:00:00  1095         -1.3         1.7             0.8   \n",
       "4141  2023-12-20 13:00:00  1167         -0.7         0.8             0.0   \n",
       "5260  2024-02-05 04:00:00  1167         -0.2         4.0             0.9   \n",
       "29406 2024-06-26 08:00:00  1384         18.2         2.8             0.6   \n",
       "27102 2024-03-22 08:00:00  1384          5.8         4.7             0.6   \n",
       "28344 2024-05-13 02:00:00  1384          8.3         0.7             0.7   \n",
       "14280 2023-12-11 01:00:00  1095         -1.4         2.4             1.0   \n",
       "15366 2024-01-25 07:00:00  1095         -5.4         0.9             0.0   \n",
       "16784 2024-03-24 09:00:00  1095          3.8         0.7             0.9   \n",
       "3089  2023-11-06 17:00:00  1167          3.6         1.3             1.0   \n",
       "222   2023-07-10 06:00:00  1167         15.8         0.7             0.9   \n",
       "25315 2024-01-07 21:00:00  1384        -12.7         2.6             0.0   \n",
       "28228 2024-05-08 06:00:00  1384         10.2         2.3             0.4   \n",
       "10858 2023-07-21 11:00:00  1095         19.5         0.8             0.9   \n",
       "9032  2024-07-11 08:00:00  1167         17.6         1.0             0.9   \n",
       "16963 2024-03-31 20:00:00  1095          1.4         0.8             0.4   \n",
       "5773  2024-02-26 13:00:00  1167          5.2         1.4             0.0   \n",
       "21481 2023-08-01 03:00:00  1384         12.3         2.8             0.6   \n",
       "16489 2024-03-12 02:00:00  1095         -8.4         0.4             0.9   \n",
       "6670  2024-04-03 22:00:00  1167          0.5         1.6             0.8   \n",
       "27080 2024-03-21 10:00:00  1384          6.6         1.2             1.0   \n",
       "13778 2023-11-20 03:00:00  1095         -4.5         0.5             0.9   \n",
       "12293 2023-09-19 06:00:00  1095         12.4         1.5             0.8   \n",
       "27248 2024-03-28 10:00:00  1384          7.2         8.5             0.9   \n",
       "17550 2024-04-25 07:00:00  1095          2.9         0.5             0.9   \n",
       "5927  2024-03-03 23:00:00  1167          2.8         1.0             1.0   \n",
       "13373 2023-11-03 06:00:00  1095          1.0         1.3             1.0   \n",
       "15415 2024-01-27 08:00:00  1095         -8.2         0.9             0.3   \n",
       "19990 2024-08-04 23:00:00  1095         13.6         0.7             0.2   \n",
       "13771 2023-11-19 20:00:00  1095         -4.5         0.7             1.0   \n",
       "\n",
       "       precipitation  value_import  \n",
       "21842            0.0         44.20  \n",
       "28067            0.0         28.30  \n",
       "18388            0.0         17.92  \n",
       "26603            0.0         47.50  \n",
       "24505            0.0         31.90  \n",
       "3265             0.0         23.20  \n",
       "5524             1.1         25.20  \n",
       "9126             0.1         34.40  \n",
       "14733            0.0         41.60  \n",
       "10130            0.4         21.40  \n",
       "6510             0.0         22.40  \n",
       "9120             0.0         22.00  \n",
       "11288            3.0         35.44  \n",
       "18653            0.0         26.40  \n",
       "11286            0.2         31.04  \n",
       "16835            0.0         27.92  \n",
       "15787            0.0         35.68  \n",
       "8940             0.0         23.00  \n",
       "12834            0.0         34.48  \n",
       "11692            1.1         23.20  \n",
       "6871             0.0         37.00  \n",
       "23862            0.0         50.70  \n",
       "3197             0.0         27.60  \n",
       "21888            0.0         47.90  \n",
       "22951            0.0         29.50  \n",
       "19398            0.0         29.52  \n",
       "27544            1.5         50.50  \n",
       "25066            0.0         47.40  \n",
       "2418             0.0         32.80  \n",
       "29602            0.4         49.90  \n",
       "11308            1.1         24.24  \n",
       "5676             0.0         37.80  \n",
       "4774             0.0         29.00  \n",
       "19680            0.0         27.04  \n",
       "24159            0.0         48.90  \n",
       "9988             0.0         22.40  \n",
       "25423            0.0         52.60  \n",
       "7597             0.0         27.40  \n",
       "9955             0.0         23.00  \n",
       "23847            0.0         48.10  \n",
       "26698            0.0         47.10  \n",
       "11785            0.0         22.96  \n",
       "6085             0.0         22.00  \n",
       "26525            0.0         48.30  \n",
       "24229            0.0         47.20  \n",
       "11196            0.4         34.24  \n",
       "17841            0.0         20.32  \n",
       "5525             1.3         30.00  \n",
       "1189             0.0         40.80  \n",
       "17024            0.0         29.04  \n",
       "25801            0.0         30.10  \n",
       "14193            0.0         38.64  \n",
       "2373             0.0         33.80  \n",
       "29810            0.0         25.00  \n",
       "18470            0.0         14.80  \n",
       "2163             0.0         18.60  \n",
       "23658            0.0         49.70  \n",
       "14010            0.0         36.00  \n",
       "19660            2.8         23.28  \n",
       "25355            0.0         50.90  \n",
       "27429            1.0         28.50  \n",
       "15091            0.0         37.92  \n",
       "28431            0.0         70.50  \n",
       "5620             0.0         25.40  \n",
       "18389            0.2         18.48  \n",
       "6596             0.0         25.00  \n",
       "23362            0.0         45.60  \n",
       "26051            0.0         45.30  \n",
       "30105            0.0         56.50  \n",
       "15005            0.0         43.68  \n",
       "15453            0.0         24.40  \n",
       "4141             0.0         37.00  \n",
       "5260             0.0         25.00  \n",
       "29406            0.0         56.10  \n",
       "27102            0.0         50.10  \n",
       "28344            0.0         27.60  \n",
       "14280            0.2         26.08  \n",
       "15366            0.0         34.48  \n",
       "16784            0.0         31.36  \n",
       "3089             0.4         35.60  \n",
       "222              0.0         55.20  \n",
       "25315            0.0         31.50  \n",
       "28228            0.0         43.10  \n",
       "10858            0.0         22.16  \n",
       "9032             0.0         40.20  \n",
       "16963            0.0         34.48  \n",
       "5773             0.0         38.00  \n",
       "21481            0.0         36.60  \n",
       "16489            0.0         25.28  \n",
       "6670             0.0         26.40  \n",
       "27080            0.0         47.90  \n",
       "13778            0.0         28.96  \n",
       "12293            8.5         30.72  \n",
       "27248            0.0         29.40  \n",
       "17550            0.2         28.56  \n",
       "5927             0.7         24.00  \n",
       "13373            1.7         32.32  \n",
       "15415            0.0         35.12  \n",
       "19990            0.0         27.36  \n",
       "13771            0.0         36.96  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>value_import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9584</th>\n",
       "      <td>2024-08-03 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11371</th>\n",
       "      <td>2023-08-11 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27936</th>\n",
       "      <td>2024-04-26 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17629</th>\n",
       "      <td>2024-04-28 14:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.4</td>\n",
       "      <td>29.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26934</th>\n",
       "      <td>2024-03-15 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.8</td>\n",
       "      <td>49.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22651</th>\n",
       "      <td>2023-09-18 21:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>2024-04-13 19:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18189</th>\n",
       "      <td>2024-05-21 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>2024-05-14 00:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>2023-08-31 21:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21322</th>\n",
       "      <td>2023-07-25 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26334</th>\n",
       "      <td>2024-02-19 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20778</th>\n",
       "      <td>2023-07-02 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21690</th>\n",
       "      <td>2023-08-09 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4212</th>\n",
       "      <td>2023-12-23 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11071</th>\n",
       "      <td>2023-07-30 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30848</th>\n",
       "      <td>2024-08-25 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29555</th>\n",
       "      <td>2024-07-02 13:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.802222</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7083</th>\n",
       "      <td>2024-04-21 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28984</th>\n",
       "      <td>2024-06-08 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>44.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19817</th>\n",
       "      <td>2024-07-28 18:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20750</th>\n",
       "      <td>2023-07-01 16:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29501</th>\n",
       "      <td>2024-06-30 07:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16065</th>\n",
       "      <td>2024-02-23 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>32.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18378</th>\n",
       "      <td>2024-05-29 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24786</th>\n",
       "      <td>2023-12-16 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>2024-01-26 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16985</th>\n",
       "      <td>2024-04-01 18:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11025</th>\n",
       "      <td>2023-07-28 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7667</th>\n",
       "      <td>2024-05-15 11:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329</th>\n",
       "      <td>2024-01-23 18:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30483</th>\n",
       "      <td>2024-08-10 05:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23572</th>\n",
       "      <td>2023-10-27 06:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19712</th>\n",
       "      <td>2024-07-24 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>2024-04-11 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17655</th>\n",
       "      <td>2024-04-29 16:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16765</th>\n",
       "      <td>2024-03-23 14:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11056</th>\n",
       "      <td>2023-07-29 17:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21654</th>\n",
       "      <td>2023-08-08 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>2023-08-26 17:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>34.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>2023-10-29 00:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30769</th>\n",
       "      <td>2024-08-22 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11216</th>\n",
       "      <td>2023-08-05 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30550</th>\n",
       "      <td>2024-08-13 00:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26568</th>\n",
       "      <td>2024-02-29 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>2023-12-23 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-16.100000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>2023-08-22 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>2023-11-04 16:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>2023-11-10 02:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>2024-04-16 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19368</th>\n",
       "      <td>2024-07-10 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>2024-06-05 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>2024-02-02 05:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19849</th>\n",
       "      <td>2024-07-30 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11518</th>\n",
       "      <td>2023-08-17 23:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21087</th>\n",
       "      <td>2023-07-15 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30651</th>\n",
       "      <td>2024-08-17 05:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2023-08-30 20:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29808</th>\n",
       "      <td>2024-07-13 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>2024-03-18 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7687</th>\n",
       "      <td>2024-05-16 07:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25297</th>\n",
       "      <td>2024-01-07 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-11.500000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>2024-01-14 09:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2023-09-24 07:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18345</th>\n",
       "      <td>2024-05-28 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>2024-06-20 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15326</th>\n",
       "      <td>2024-01-23 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26058</th>\n",
       "      <td>2024-02-07 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-6.600000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>2024-08-28 18:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>2024-04-04 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>2024-04-18 02:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16411</th>\n",
       "      <td>2024-03-08 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16287</th>\n",
       "      <td>2024-03-03 16:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>34.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>2024-08-18 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>2024-07-31 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26601</th>\n",
       "      <td>2024-03-01 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17298</th>\n",
       "      <td>2024-04-14 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24003</th>\n",
       "      <td>2023-11-14 05:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>2024-01-14 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21818</th>\n",
       "      <td>2023-08-15 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21928</th>\n",
       "      <td>2023-08-19 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.7</td>\n",
       "      <td>61.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13903</th>\n",
       "      <td>2023-11-25 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13598</th>\n",
       "      <td>2023-11-12 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>2023-12-18 00:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27911</th>\n",
       "      <td>2024-04-25 01:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29450</th>\n",
       "      <td>2024-06-28 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>2024-01-01 17:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-4.700000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>2023-09-19 19:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27550</th>\n",
       "      <td>2024-04-10 00:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13513</th>\n",
       "      <td>2023-11-09 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>2024-04-28 05:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>22.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>2024-05-13 23:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>2024-06-06 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11467</th>\n",
       "      <td>2023-08-15 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.7</td>\n",
       "      <td>35.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12650</th>\n",
       "      <td>2023-10-04 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>2024-01-27 04:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>2024-06-11 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14922</th>\n",
       "      <td>2024-01-06 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-24.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10992</th>\n",
       "      <td>2023-07-27 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>2023-09-04 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "9584  2024-08-03 08:00:00  1167    18.800000    0.400000        0.900000   \n",
       "11371 2023-08-11 20:00:00  1095    15.700000    1.400000        0.100000   \n",
       "27936 2024-04-26 02:00:00  1384     3.600000    3.500000        0.900000   \n",
       "17629 2024-04-28 14:00:00  1095     4.000000    2.800000        1.000000   \n",
       "26934 2024-03-15 08:00:00  1384     5.000000    1.700000        1.000000   \n",
       "22651 2023-09-18 21:00:00  1384    16.500000    6.300000        0.900000   \n",
       "6907  2024-04-13 19:00:00  1167     8.800000    1.600000        0.900000   \n",
       "18189 2024-05-21 22:00:00  1095    11.900000    0.800000        0.000000   \n",
       "17999 2024-05-14 00:00:00  1095     7.800000    0.500000        0.700000   \n",
       "11852 2023-08-31 21:00:00  1095    11.600000    0.600000        0.700000   \n",
       "21322 2023-07-25 12:00:00  1384    21.700000    5.200000        0.000000   \n",
       "26334 2024-02-19 08:00:00  1384    -0.500000    1.900000        0.900000   \n",
       "20778 2023-07-02 20:00:00  1384    15.000000    0.700000        0.400000   \n",
       "21690 2023-08-09 20:00:00  1384    14.200000    1.600000        0.900000   \n",
       "4212  2023-12-23 12:00:00  1167   -13.000000    0.900000        0.000000   \n",
       "11071 2023-07-30 08:00:00  1095    15.900000    0.600000        1.000000   \n",
       "30848 2024-08-25 10:00:00  1384    16.300000    6.400000        0.300000   \n",
       "29555 2024-07-02 13:00:00  1384    16.802222    2.466667        0.422222   \n",
       "7083  2024-04-21 03:00:00  1167     0.200000    1.700000        0.000000   \n",
       "28984 2024-06-08 18:00:00  1384    11.000000    4.700000        0.300000   \n",
       "19817 2024-07-28 18:00:00  1095    21.200000    2.100000        0.000000   \n",
       "20750 2023-07-01 16:00:00  1384    18.100000    3.500000        0.900000   \n",
       "29501 2024-06-30 07:00:00  1384    14.400000    1.700000        0.900000   \n",
       "16065 2024-02-23 10:00:00  1095     2.100000    2.600000        1.000000   \n",
       "18378 2024-05-29 19:00:00  1095    16.400000    1.100000        0.200000   \n",
       "24786 2023-12-16 20:00:00  1384     5.300000    3.200000        0.000000   \n",
       "5016  2024-01-26 00:00:00  1167    -4.000000    0.400000        1.000000   \n",
       "16985 2024-04-01 18:00:00  1095     7.300000    1.200000        0.200000   \n",
       "11025 2023-07-28 10:00:00  1095    16.900000    0.900000        0.600000   \n",
       "7667  2024-05-15 11:00:00  1167    22.100000    2.900000        0.900000   \n",
       "15329 2024-01-23 18:00:00  1095    -0.900000    0.800000        0.900000   \n",
       "30483 2024-08-10 05:00:00  1384    13.700000    4.600000        0.300000   \n",
       "23572 2023-10-27 06:00:00  1384     1.900000    4.700000        0.800000   \n",
       "19712 2024-07-24 09:00:00  1095    17.200000    2.600000        0.900000   \n",
       "6852  2024-04-11 12:00:00  1167    16.900000    4.700000        0.000000   \n",
       "17655 2024-04-29 16:00:00  1095    13.300000    2.900000        0.500000   \n",
       "16765 2024-03-23 14:00:00  1095     7.700000    1.500000        1.000000   \n",
       "11056 2023-07-29 17:00:00  1095    21.800000    2.600000        0.200000   \n",
       "21654 2023-08-08 08:00:00  1384    15.700000    2.400000        1.000000   \n",
       "11728 2023-08-26 17:00:00  1095    15.100000    2.500000        1.000000   \n",
       "23614 2023-10-29 00:00:00  1384     1.300000    4.700000        0.900000   \n",
       "30769 2024-08-22 03:00:00  1384    12.800000    4.200000        1.000000   \n",
       "11216 2023-08-05 09:00:00  1095    15.500000    1.900000        1.000000   \n",
       "30550 2024-08-13 00:00:00  1384    12.500000    0.300000        0.900000   \n",
       "26568 2024-02-29 02:00:00  1384     4.500000    7.200000        1.000000   \n",
       "4223  2023-12-23 23:00:00  1167   -16.100000    1.200000        0.500000   \n",
       "1266  2023-08-22 18:00:00  1167    19.800000    1.200000        0.100000   \n",
       "3040  2023-11-04 16:00:00  1167     3.600000    1.100000        1.000000   \n",
       "3170  2023-11-10 02:00:00  1167    -2.900000    0.900000        0.900000   \n",
       "6966  2024-04-16 06:00:00  1167    -0.600000    1.400000        0.500000   \n",
       "19368 2024-07-10 01:00:00  1095     9.800000    0.600000        0.900000   \n",
       "8172  2024-06-05 12:00:00  1167    16.100000    4.700000        0.400000   \n",
       "5189  2024-02-02 05:00:00  1167    -4.000000    0.800000        1.000000   \n",
       "19849 2024-07-30 02:00:00  1095    11.300000    0.200000        0.600000   \n",
       "11518 2023-08-17 23:00:00  1095    12.600000    0.700000        0.000000   \n",
       "21087 2023-07-15 17:00:00  1384    16.900000    3.700000        1.000000   \n",
       "30651 2024-08-17 05:00:00  1384    12.300000    4.100000        0.200000   \n",
       "1460  2023-08-30 20:00:00  1167    12.300000    0.600000        0.400000   \n",
       "29808 2024-07-13 02:00:00  1384    14.700000    3.400000        0.900000   \n",
       "16641 2024-03-18 10:00:00  1095     0.700000    1.100000        1.000000   \n",
       "7687  2024-05-16 07:00:00  1167    15.100000    1.200000        0.000000   \n",
       "25297 2024-01-07 03:00:00  1384   -11.500000    3.900000        0.000000   \n",
       "4737  2024-01-14 09:00:00  1167    -2.400000    2.000000        0.900000   \n",
       "2047  2023-09-24 07:00:00  1167     3.900000    0.500000        0.000000   \n",
       "18345 2024-05-28 10:00:00  1095    19.400000    1.800000        0.300000   \n",
       "18899 2024-06-20 12:00:00  1095    21.200000    4.100000        0.100000   \n",
       "15326 2024-01-23 15:00:00  1095     3.300000    1.300000        0.300000   \n",
       "26058 2024-02-07 20:00:00  1384    -6.600000    1.800000        0.900000   \n",
       "20561 2024-08-28 18:00:00  1095    19.300000    1.700000        0.800000   \n",
       "6675  2024-04-04 03:00:00  1167    -0.400000    2.700000        1.000000   \n",
       "7010  2024-04-18 02:00:00  1167    -2.900000    1.000000        0.000000   \n",
       "16411 2024-03-08 20:00:00  1095    -3.100000    0.500000        0.000000   \n",
       "16287 2024-03-03 16:00:00  1095     2.600000    0.700000        1.000000   \n",
       "9939  2024-08-18 03:00:00  1167     7.300000    0.400000        0.800000   \n",
       "9510  2024-07-31 06:00:00  1167    13.800000    2.000000        0.000000   \n",
       "26601 2024-03-01 11:00:00  1384     4.400000    4.500000        1.000000   \n",
       "17298 2024-04-14 19:00:00  1095     4.900000    0.800000        0.000000   \n",
       "24003 2023-11-14 05:00:00  1384     0.000000    7.800000        0.900000   \n",
       "4736  2024-01-14 08:00:00  1167    -1.500000    1.600000        0.900000   \n",
       "21818 2023-08-15 04:00:00  1384    15.600000    0.800000        0.900000   \n",
       "21928 2023-08-19 18:00:00  1384    17.400000    3.500000        1.000000   \n",
       "13903 2023-11-25 08:00:00  1095    -0.600000    0.900000        0.500000   \n",
       "13598 2023-11-12 15:00:00  1095     1.400000    0.700000        0.800000   \n",
       "14447 2023-12-18 00:00:00  1095    -3.400000    1.000000        0.300000   \n",
       "27911 2024-04-25 01:00:00  1384     3.200000    2.700000        0.900000   \n",
       "29450 2024-06-28 04:00:00  1384    18.200000    5.000000        1.000000   \n",
       "4433  2024-01-01 17:00:00  1167    -4.700000    2.800000        1.000000   \n",
       "1939  2023-09-19 19:00:00  1167    10.300000    1.600000        0.900000   \n",
       "27550 2024-04-10 00:00:00  1384     6.700000    4.200000        1.000000   \n",
       "13513 2023-11-09 02:00:00  1095     0.300000    1.700000        1.000000   \n",
       "7253  2024-04-28 05:00:00  1167     2.700000    1.700000        1.000000   \n",
       "17998 2024-05-13 23:00:00  1095     8.900000    0.700000        0.800000   \n",
       "8202  2024-06-06 18:00:00  1167    14.200000    2.800000        0.600000   \n",
       "11467 2023-08-15 20:00:00  1095    14.900000    0.600000        0.900000   \n",
       "12650 2023-10-04 03:00:00  1095     7.000000    1.300000        0.800000   \n",
       "15411 2024-01-27 04:00:00  1095    -6.500000    0.800000        0.700000   \n",
       "8308  2024-06-11 04:00:00  1167    10.500000    1.000000        1.000000   \n",
       "14922 2024-01-06 19:00:00  1095   -24.200000    0.600000        0.000000   \n",
       "10992 2023-07-27 01:00:00  1095     9.000000    0.400000        0.500000   \n",
       "11930 2023-09-04 03:00:00  1095     9.700000    1.400000        0.200000   \n",
       "\n",
       "       precipitation  value_import  \n",
       "9584             0.0         40.20  \n",
       "11371            0.0         37.60  \n",
       "27936            0.0         31.40  \n",
       "17629            1.4         29.36  \n",
       "26934            1.8         49.40  \n",
       "22651            0.0         65.30  \n",
       "6907             0.0         36.80  \n",
       "18189            0.0         36.24  \n",
       "17999            0.0         20.88  \n",
       "11852            0.0         35.68  \n",
       "21322            0.0         62.30  \n",
       "26334            0.0         48.70  \n",
       "20778            0.0         32.60  \n",
       "21690            0.0         52.30  \n",
       "4212             0.0         40.40  \n",
       "11071            0.0         22.00  \n",
       "30848            0.0         24.90  \n",
       "29555            0.0         53.60  \n",
       "7083             0.0         23.60  \n",
       "28984            0.1         44.70  \n",
       "19817            0.0         22.08  \n",
       "20750            0.0         63.70  \n",
       "29501            0.0         25.00  \n",
       "16065            0.9         32.16  \n",
       "18378            0.0         23.92  \n",
       "24786            0.0         50.00  \n",
       "5016             0.0         25.20  \n",
       "16985            0.0         24.24  \n",
       "11025            0.0         28.56  \n",
       "7667             0.0         43.60  \n",
       "15329            0.0         34.88  \n",
       "30483            0.0         26.90  \n",
       "23572            0.0         45.60  \n",
       "19712            0.0         27.84  \n",
       "6852             0.0         37.60  \n",
       "17655            0.0         10.24  \n",
       "16765            0.0         24.80  \n",
       "11056            0.0         17.92  \n",
       "21654            0.0         62.60  \n",
       "11728            1.2         34.64  \n",
       "23614            0.0         27.50  \n",
       "30769            0.0         27.00  \n",
       "11216            0.0         28.80  \n",
       "30550            0.0         26.80  \n",
       "26568            0.0         32.40  \n",
       "4223             0.0         27.20  \n",
       "1266             0.0         40.20  \n",
       "3040             0.0         36.80  \n",
       "3170             0.0         22.00  \n",
       "6966             0.0         35.40  \n",
       "19368            0.0         22.80  \n",
       "8172             0.0         38.00  \n",
       "5189             0.0         29.40  \n",
       "19849            0.0         24.48  \n",
       "11518            0.0         27.76  \n",
       "21087            0.0         62.80  \n",
       "30651            0.0         26.60  \n",
       "1460             0.0         53.20  \n",
       "29808            0.0         27.70  \n",
       "16641            0.0         33.68  \n",
       "7687             0.0         38.60  \n",
       "25297            0.0         27.50  \n",
       "4737             0.0         22.60  \n",
       "2047             0.0         16.20  \n",
       "18345            0.0         14.40  \n",
       "18899            0.0          4.32  \n",
       "15326            0.0         33.76  \n",
       "26058            0.0         52.00  \n",
       "20561            0.0         33.28  \n",
       "6675             0.0         23.40  \n",
       "7010             0.0         23.40  \n",
       "16411            0.0         35.76  \n",
       "16287            0.8         34.80  \n",
       "9939             0.0         22.60  \n",
       "9510             0.0         36.20  \n",
       "26601            0.0         48.00  \n",
       "17298            0.0         26.96  \n",
       "24003            0.0         30.90  \n",
       "4736             0.0         25.20  \n",
       "21818            0.0         43.50  \n",
       "21928            2.7         61.10  \n",
       "13903            0.0         35.36  \n",
       "13598            0.0         33.52  \n",
       "14447            0.0         26.48  \n",
       "27911            0.0         26.70  \n",
       "29450            0.0         28.40  \n",
       "4433             0.2         25.40  \n",
       "1939             0.0         35.00  \n",
       "27550            0.0         27.90  \n",
       "13513            0.8         25.04  \n",
       "7253             0.7         22.80  \n",
       "17998            0.0         25.92  \n",
       "8202             0.0         35.60  \n",
       "11467            1.7         35.92  \n",
       "12650            0.0         24.80  \n",
       "15411            0.0         24.48  \n",
       "8308             0.0         21.80  \n",
       "14922            0.0         48.32  \n",
       "10992            0.0         20.40  \n",
       "11930            0.0         24.96  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241109_111950\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       15.00 GB / 32.00 GB (46.9%)\n",
      "Disk Space Avail:   621.09 GB / 926.35 GB (67.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241109_111950/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241109_111950/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    22115\n",
      "Train Data Columns: 6\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15361.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.69 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.81s of the 899.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.07501\n",
      "[2000]\tvalid_set's l1: 5.78648\n",
      "[3000]\tvalid_set's l1: 5.6268\n",
      "[4000]\tvalid_set's l1: 5.52069\n",
      "[5000]\tvalid_set's l1: 5.44529\n",
      "[6000]\tvalid_set's l1: 5.38433\n",
      "[7000]\tvalid_set's l1: 5.34409\n",
      "[8000]\tvalid_set's l1: 5.30283\n",
      "[9000]\tvalid_set's l1: 5.273\n",
      "[10000]\tvalid_set's l1: 5.2553\n",
      "[1000]\tvalid_set's l1: 6.08035\n",
      "[2000]\tvalid_set's l1: 5.80636\n",
      "[3000]\tvalid_set's l1: 5.64797\n",
      "[4000]\tvalid_set's l1: 5.53999\n",
      "[5000]\tvalid_set's l1: 5.46072\n",
      "[6000]\tvalid_set's l1: 5.40086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6235. Best iteration is:\n",
      "\t[6235]\tvalid_set's l1: 5.38999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.94994\n",
      "[2000]\tvalid_set's l1: 5.68865\n",
      "[3000]\tvalid_set's l1: 5.55142\n",
      "[4000]\tvalid_set's l1: 5.45328\n",
      "[5000]\tvalid_set's l1: 5.38454\n",
      "[6000]\tvalid_set's l1: 5.32817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6368. Best iteration is:\n",
      "\t[6364]\tvalid_set's l1: 5.30857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.03209\n",
      "[2000]\tvalid_set's l1: 5.74906\n",
      "[3000]\tvalid_set's l1: 5.58594\n",
      "[4000]\tvalid_set's l1: 5.48716\n",
      "[5000]\tvalid_set's l1: 5.41051\n",
      "[6000]\tvalid_set's l1: 5.35182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6050. Best iteration is:\n",
      "\t[6031]\tvalid_set's l1: 5.35038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.13137\n",
      "[2000]\tvalid_set's l1: 5.83397\n",
      "[3000]\tvalid_set's l1: 5.67315\n",
      "[4000]\tvalid_set's l1: 5.55386\n",
      "[5000]\tvalid_set's l1: 5.46717\n",
      "[6000]\tvalid_set's l1: 5.40311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6386. Best iteration is:\n",
      "\t[6384]\tvalid_set's l1: 5.38428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.88699\n",
      "[2000]\tvalid_set's l1: 5.58856\n",
      "[3000]\tvalid_set's l1: 5.42108\n",
      "[4000]\tvalid_set's l1: 5.30274\n",
      "[5000]\tvalid_set's l1: 5.22316\n",
      "[6000]\tvalid_set's l1: 5.16418\n",
      "[7000]\tvalid_set's l1: 5.11578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 7383. Best iteration is:\n",
      "\t[7376]\tvalid_set's l1: 5.09959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.05065\n",
      "[2000]\tvalid_set's l1: 5.7566\n",
      "[3000]\tvalid_set's l1: 5.58677\n",
      "[4000]\tvalid_set's l1: 5.47299\n",
      "[5000]\tvalid_set's l1: 5.39058\n",
      "[6000]\tvalid_set's l1: 5.33536\n",
      "[7000]\tvalid_set's l1: 5.28602\n",
      "[8000]\tvalid_set's l1: 5.24346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8182. Best iteration is:\n",
      "\t[8182]\tvalid_set's l1: 5.23508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.99279\n",
      "[2000]\tvalid_set's l1: 5.71728\n",
      "[3000]\tvalid_set's l1: 5.54405\n",
      "[4000]\tvalid_set's l1: 5.43052\n",
      "[5000]\tvalid_set's l1: 5.35591\n",
      "[6000]\tvalid_set's l1: 5.28894\n",
      "[7000]\tvalid_set's l1: 5.25168\n",
      "[8000]\tvalid_set's l1: 5.21839\n",
      "[9000]\tvalid_set's l1: 5.18831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 9791. Best iteration is:\n",
      "\t[9716]\tvalid_set's l1: 5.16993\n",
      "\t-5.2741\t = Validation score   (-mean_absolute_error)\n",
      "\t571.83s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 24.47s of the 324.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 191. Best iteration is:\n",
      "\t[191]\tvalid_set's l1: 5.07383\n",
      "\tRan out of time, early stopping on iteration 203. Best iteration is:\n",
      "\t[203]\tvalid_set's l1: 5.03595\n",
      "\tRan out of time, early stopping on iteration 239. Best iteration is:\n",
      "\t[239]\tvalid_set's l1: 5.00877\n",
      "\tRan out of time, early stopping on iteration 257. Best iteration is:\n",
      "\t[257]\tvalid_set's l1: 4.9139\n",
      "\tRan out of time, early stopping on iteration 270. Best iteration is:\n",
      "\t[270]\tvalid_set's l1: 5.03878\n",
      "\tRan out of time, early stopping on iteration 285. Best iteration is:\n",
      "\t[285]\tvalid_set's l1: 4.79066\n",
      "\tRan out of time, early stopping on iteration 319. Best iteration is:\n",
      "\t[319]\tvalid_set's l1: 4.91514\n",
      "\tRan out of time, early stopping on iteration 386. Best iteration is:\n",
      "\t[386]\tvalid_set's l1: 4.87107\n",
      "\t-4.956\t = Validation score   (-mean_absolute_error)\n",
      "\t23.43s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.93s of the 301.05s of remaining time.\n",
      "\t-3.588\t = Validation score   (-mean_absolute_error)\n",
      "\t3.14s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 296.93s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.588\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 296.9s of the 296.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5447\t = Validation score   (-mean_absolute_error)\n",
      "\t51.15s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 245.58s of the 245.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5089\t = Validation score   (-mean_absolute_error)\n",
      "\t22.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 223.08s of the 223.06s of remaining time.\n",
      "\t-3.55\t = Validation score   (-mean_absolute_error)\n",
      "\t6.37s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 215.68s of the 215.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5061\t = Validation score   (-mean_absolute_error)\n",
      "\t9.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 206.44s of the 206.42s of remaining time.\n",
      "\t-3.5397\t = Validation score   (-mean_absolute_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 203.92s of the 203.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.4655\t = Validation score   (-mean_absolute_error)\n",
      "\t148.51s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 55.06s of the 55.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5119\t = Validation score   (-mean_absolute_error)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 50.73s of the 50.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 5)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 11)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 12)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.544\t = Validation score   (-mean_absolute_error)\n",
      "\t47.87s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 2.5s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.579, 'RandomForestMSE_BAG_L2': 0.211, 'NeuralNetTorch_BAG_L2': 0.211}\n",
      "\t-3.4477\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 897.56s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 949.6 rows/s (2765 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241109_111950/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -3.453095  -3.447680  mean_absolute_error        4.331330       4.054386  801.198722                 0.001041                0.000322           0.050539            3       True         13\n",
      "1   NeuralNetFastAI_BAG_L2      -3.454572  -3.465467  mean_absolute_error        3.762032       3.171802  746.907913                 0.717057                0.230367         148.507633            2       True         10\n",
      "2        LightGBMXT_BAG_L2      -3.494421  -3.544698  mean_absolute_error        3.179550       3.013127  649.547877                 0.134575                0.071692          51.147597            2       True          5\n",
      "3           XGBoost_BAG_L2      -3.501080  -3.511874  mean_absolute_error        3.100934       2.962387  602.662196                 0.055959                0.020952           4.261916            2       True         11\n",
      "4          CatBoost_BAG_L2      -3.505284  -3.506139  mean_absolute_error        3.070021       2.950244  607.583937                 0.025046                0.008810           9.183657            2       True          8\n",
      "5          LightGBM_BAG_L2      -3.510453  -3.508908  mean_absolute_error        3.091293       2.961702  620.837279                 0.046318                0.020267          22.436999            2       True          6\n",
      "6    NeuralNetTorch_BAG_L2      -3.529101  -3.543971  mean_absolute_error        3.324713       3.156440  646.275084                 0.279738                0.215005          47.874804            2       True         12\n",
      "7     ExtraTreesMSE_BAG_L2      -3.530577  -3.539674  mean_absolute_error        3.373507       3.584406  599.923070                 0.328532                0.642972           1.522790            2       True          9\n",
      "8   RandomForestMSE_BAG_L2      -3.546674  -3.549986  mean_absolute_error        3.333494       3.608692  604.765746                 0.288519                0.667257           6.365466            2       True          7\n",
      "9   RandomForestMSE_BAG_L1      -3.636705  -3.588007  mean_absolute_error        0.273288       0.638263    3.137244                 0.273288                0.638263           3.137244            1       True          3\n",
      "10     WeightedEnsemble_L2      -3.636705  -3.588007  mean_absolute_error        0.275278       0.638622    3.158273                 0.001990                0.000359           0.021029            2       True          4\n",
      "11         LightGBM_BAG_L1      -5.048021  -4.956025  mean_absolute_error        0.097989       0.042331   23.432536                 0.097989                0.042331          23.432536            1       True          2\n",
      "12       LightGBMXT_BAG_L1      -5.143542  -5.274134  mean_absolute_error        2.673698       2.260840  571.830500                 2.673698                2.260840         571.830500            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t903s\t = DyStack   runtime |\t2697s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2697s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241109_111950\"\n",
      "Train Data Rows:    24880\n",
      "Train Data Columns: 6\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14941.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.14 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.1s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.90 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1797.78s of the 2697.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.92245\n",
      "[2000]\tvalid_set's l1: 5.62969\n",
      "[3000]\tvalid_set's l1: 5.46043\n",
      "[4000]\tvalid_set's l1: 5.34185\n",
      "[5000]\tvalid_set's l1: 5.25124\n",
      "[6000]\tvalid_set's l1: 5.18188\n",
      "[7000]\tvalid_set's l1: 5.13049\n",
      "[8000]\tvalid_set's l1: 5.0891\n",
      "[9000]\tvalid_set's l1: 5.05294\n",
      "[10000]\tvalid_set's l1: 5.02345\n",
      "[1000]\tvalid_set's l1: 6.0292\n",
      "[2000]\tvalid_set's l1: 5.75662\n",
      "[3000]\tvalid_set's l1: 5.58846\n",
      "[4000]\tvalid_set's l1: 5.47182\n",
      "[5000]\tvalid_set's l1: 5.38238\n",
      "[6000]\tvalid_set's l1: 5.31468\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Trainer has no fit models that can infer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m performance \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# best model: ag-20241022_161331\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Performance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:2271\u001b[0m, in \u001b[0;36mTabularPredictor.evaluate\u001b[0;34m(self, data, model, decision_threshold, display, auxiliary_metrics, detailed_report, **kwargs)\u001b[0m\n\u001b[1;32m   2269\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(data\u001b[38;5;241m=\u001b[39mdata, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2271\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_weight \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m   2273\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_weight]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:2117\u001b[0m, in \u001b[0;36mTabularPredictor.predict\u001b[0;34m(self, data, model, as_pandas, transform_features, decision_threshold)\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decision_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2116\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_threshold\n\u001b[0;32m-> 2117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_pandas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecision_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:208\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict\u001b[0;34m(self, X, model, as_pandas, inverse_transform, transform_features, decision_threshold)\u001b[0m\n\u001b[1;32m    206\u001b[0m     decision_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    207\u001b[0m X_index \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(X\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_multiclass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_features\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m problem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_cleaner\u001b[38;5;241m.\u001b[39mproblem_type_transform \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type\n\u001b[1;32m    212\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m get_pred_from_proba(y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, problem_type\u001b[38;5;241m=\u001b[39mproblem_type, decision_threshold\u001b[38;5;241m=\u001b[39mdecision_threshold)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/learner/abstract_learner.py:189\u001b[0m, in \u001b[0;36mAbstractTabularLearner.predict_proba\u001b[0;34m(self, X, model, as_pandas, as_multiclass, inverse_transform, transform_features)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transform_features:\n\u001b[1;32m    188\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_features(X)\n\u001b[0;32m--> 189\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_predict_proba(\n\u001b[1;32m    191\u001b[0m     y_pred_proba\u001b[38;5;241m=\u001b[39my_pred_proba, as_pandas\u001b[38;5;241m=\u001b[39mas_pandas, index\u001b[38;5;241m=\u001b[39mX_index, as_multiclass\u001b[38;5;241m=\u001b[39mas_multiclass, inverse_transform\u001b[38;5;241m=\u001b[39minverse_transform\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred_proba\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:835\u001b[0m, in \u001b[0;36mAbstractTrainer.predict_proba\u001b[0;34m(self, X, model)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 835\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m     cascade \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_proba_model(X, model, cascade\u001b[38;5;241m=\u001b[39mcascade)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:843\u001b[0m, in \u001b[0;36mAbstractTrainer._get_best\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py:1517\u001b[0m, in \u001b[0;36mAbstractTrainer.get_model_best\u001b[0;34m(self, can_infer, allow_full, infer_limit, infer_limit_as_child)\u001b[0m\n\u001b[1;32m   1515\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names(can_infer\u001b[38;5;241m=\u001b[39mcan_infer)\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m models:\n\u001b[0;32m-> 1517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer has no fit models that can infer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1518\u001b[0m models_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_models_attribute_dict(models\u001b[38;5;241m=\u001b[39mmodels, attribute\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefit_full_parent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_full:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Trainer has no fit models that can infer."
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "performance\n",
    "# reset index \n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "matrix = []\n",
    "size = 1000\n",
    "for i in range(size):\n",
    "    matrix.append([])\n",
    "    for j in range(size):\n",
    "        K = 1\n",
    "        if i == j:\n",
    "            K = 1\n",
    "        else:\n",
    "            K = 0\n",
    "        matrix[i].append(K)\n",
    "# print as np matrix\n",
    "print(np.array(matrix))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "0     2023-07-01 00:00:00  1199         13.6         1.6             0.5   \n",
       "1     2023-07-01 01:00:00  1199         13.2         2.0             0.4   \n",
       "2     2023-07-01 02:00:00  1199         12.3         1.6             0.3   \n",
       "3     2023-07-01 03:00:00  1199         11.9         0.6             0.9   \n",
       "4     2023-07-01 04:00:00  1199         11.9         0.2             0.8   \n",
       "...                   ...   ...          ...         ...             ...   \n",
       "10338 2024-09-03 18:00:00  1199         17.8         2.5             1.0   \n",
       "10339 2024-09-03 19:00:00  1199         17.8         1.8             1.0   \n",
       "10340 2024-09-03 20:00:00  1199         17.7         1.1             1.0   \n",
       "10341 2024-09-03 21:00:00  1199         18.0         3.4             1.0   \n",
       "10342 2024-09-03 22:00:00  1199         17.9         3.0             1.0   \n",
       "\n",
       "       precipitation  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "...              ...  \n",
       "10338            1.4  \n",
       "10339            2.5  \n",
       "10340            2.6  \n",
       "10341            2.9  \n",
       "10342            2.7  \n",
       "\n",
       "[10343 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "data_predict = main_building[features]\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / date_time / 'prediction_drita.csv'\n",
    "# create folder\n",
    "my_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
