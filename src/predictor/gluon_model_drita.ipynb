{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/Desktop/SolarEnergyImpact\n",
      "Index(['timestamp', 'value_import', 'property_id', 'building', 'area',\n",
      "       'value_export', 'solar_consumption', 'predicted_consumption',\n",
      "       'temperature', 'wind_speed', 'wind_direction', 'cloud_fraction',\n",
      "       'precipitation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().absolute().parent.parent\n",
    "print(root)\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from src.pipeline import Pipeline, BuilingIdsEnum\n",
    "pipe = Pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_a = pipe.get_data(BuilingIdsEnum.A)\n",
    "building_b = pipe.get_data(BuilingIdsEnum.B)\n",
    "building_c = pipe.get_data(BuilingIdsEnum.C)\n",
    "\n",
    "# remove from 3. to 7. july 2024 from dataset A\n",
    "mask = (building_a['timestamp'] >= '2024-07-03') & (building_a['timestamp'] <= '2024-07-07')\n",
    "building_a = building_a[~mask]\n",
    "\n",
    "combined_df = pd.concat([building_a, building_b, building_c])\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# Perform the train-test split with stratification based on 'building_id'\n",
    "train_data, test_data = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['building']\n",
    ")\n",
    "\n",
    "# select features\n",
    "features = [\"timestamp\", \"area\", \"temperature\", \"wind_speed\", \"cloud_fraction\", \"precipitation\"]\n",
    "target = \"value_import\"\n",
    "\n",
    "train_data = train_data[features + [target]]\n",
    "test_data = test_data[features + [target]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>value_import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21775</th>\n",
       "      <td>2023-08-17 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>2024-05-24 05:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>28.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17826</th>\n",
       "      <td>2024-05-10 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9375</th>\n",
       "      <td>2024-07-29 16:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>22.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>2023-07-26 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>56.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15353</th>\n",
       "      <td>2024-01-28 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12866</th>\n",
       "      <td>2023-10-17 04:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>2023-08-04 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>2024-01-14 19:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>2023-12-31 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>34.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10276</th>\n",
       "      <td>2023-07-01 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30248</th>\n",
       "      <td>2024-08-04 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>22.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7796</th>\n",
       "      <td>2024-05-20 20:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15787</th>\n",
       "      <td>2024-02-15 21:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28751</th>\n",
       "      <td>2024-06-03 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9403</th>\n",
       "      <td>2024-07-30 20:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>18.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20380</th>\n",
       "      <td>2024-08-25 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10290</th>\n",
       "      <td>2023-07-01 20:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15144</th>\n",
       "      <td>2024-01-20 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-19.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14068</th>\n",
       "      <td>2023-12-06 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>2024-04-24 20:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10392</th>\n",
       "      <td>2023-07-06 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17535</th>\n",
       "      <td>2024-04-28 17:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18865</th>\n",
       "      <td>2024-06-23 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22925</th>\n",
       "      <td>2023-10-04 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>11.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26481</th>\n",
       "      <td>2024-02-29 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.9</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>45.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16811</th>\n",
       "      <td>2024-03-29 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9213</th>\n",
       "      <td>2024-07-22 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17138</th>\n",
       "      <td>2024-04-12 04:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25991</th>\n",
       "      <td>2024-02-09 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8876</th>\n",
       "      <td>2024-07-08 21:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>13.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>2024-05-30 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7755</th>\n",
       "      <td>2024-05-19 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22909</th>\n",
       "      <td>2023-10-03 16:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9053</th>\n",
       "      <td>2024-07-16 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29519</th>\n",
       "      <td>2024-07-05 02:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>9.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23042</th>\n",
       "      <td>2023-10-09 05:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>2023-10-25 21:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2023-07-27 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>2024-03-23 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29486</th>\n",
       "      <td>2024-07-03 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12587</th>\n",
       "      <td>2023-10-05 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>12.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>2023-09-15 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22124</th>\n",
       "      <td>2023-08-31 23:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29465</th>\n",
       "      <td>2024-07-02 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>17.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17603</th>\n",
       "      <td>2024-05-01 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14279</th>\n",
       "      <td>2023-12-15 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-12.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2023-08-26 01:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>2024-02-14 07:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22001</th>\n",
       "      <td>2023-08-26 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>14.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15120</th>\n",
       "      <td>2024-01-19 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-20.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27628</th>\n",
       "      <td>2024-04-17 07:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>2024-03-22 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20792</th>\n",
       "      <td>2023-07-07 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>22.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28957</th>\n",
       "      <td>2024-06-11 16:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17717</th>\n",
       "      <td>2024-05-06 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8535</th>\n",
       "      <td>2024-06-20 15:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>22.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>2024-01-26 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25803</th>\n",
       "      <td>2024-02-01 06:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>2023-07-19 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>19.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>2024-07-22 10:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28528</th>\n",
       "      <td>2024-05-24 19:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>2023-10-28 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>2024-04-25 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>2023-12-07 15:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>2024-05-01 20:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8473</th>\n",
       "      <td>2024-06-18 01:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>2023-07-21 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-07-01 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>22.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21758</th>\n",
       "      <td>2023-08-16 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>20.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>2023-12-27 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-14.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>2024-04-06 13:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28612</th>\n",
       "      <td>2024-05-28 07:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12608</th>\n",
       "      <td>2023-10-06 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13042</th>\n",
       "      <td>2023-10-24 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16748</th>\n",
       "      <td>2024-03-26 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>33.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13885</th>\n",
       "      <td>2023-11-28 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-01 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30864</th>\n",
       "      <td>2024-08-30 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21096</th>\n",
       "      <td>2023-07-20 03:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>11.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>2023-10-01 18:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29538</th>\n",
       "      <td>2024-07-05 21:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>12.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21234</th>\n",
       "      <td>2023-07-25 21:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>16.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>2024-02-24 07:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30855</th>\n",
       "      <td>2024-08-29 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21793</th>\n",
       "      <td>2023-08-18 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6056</th>\n",
       "      <td>2024-03-09 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20064</th>\n",
       "      <td>2024-08-12 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>2023-12-21 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-9.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>31.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>2024-04-21 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>2023-12-11 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>2023-08-08 05:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2023-09-28 15:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>2023-08-12 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21438</th>\n",
       "      <td>2023-08-03 09:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>19.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18108</th>\n",
       "      <td>2024-05-22 14:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>23.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10640</th>\n",
       "      <td>2023-07-16 10:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>2024-03-18 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16411</th>\n",
       "      <td>2024-03-12 21:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17423</th>\n",
       "      <td>2024-04-24 01:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "21775 2023-08-17 10:00:00  1384         19.0         3.7             0.7   \n",
       "7877  2024-05-24 05:00:00  1167         15.0         0.6             0.9   \n",
       "17826 2024-05-10 20:00:00  1095         10.9         0.6             0.8   \n",
       "9375  2024-07-29 16:00:00  1167         22.4         2.8             0.8   \n",
       "612   2023-07-26 12:00:00  1167         13.2         2.2             0.9   \n",
       "15353 2024-01-28 19:00:00  1095          0.8         2.4             0.9   \n",
       "12866 2023-10-17 04:00:00  1095          4.2         1.1             1.0   \n",
       "11094 2023-08-04 08:00:00  1095         16.8         1.8             0.8   \n",
       "4747  2024-01-14 19:00:00  1167         -2.2         2.5             0.9   \n",
       "14674 2023-12-31 12:00:00  1095         -2.0         2.0             1.0   \n",
       "10276 2023-07-01 06:00:00  1095         14.2         1.0             1.0   \n",
       "30248 2024-08-04 11:00:00  1384         22.5         2.6             0.5   \n",
       "7796  2024-05-20 20:00:00  1167         16.7         0.5             0.0   \n",
       "15787 2024-02-15 21:00:00  1095         -4.9         1.0             1.0   \n",
       "28751 2024-06-03 02:00:00  1384         16.8         2.6             0.9   \n",
       "9403  2024-07-30 20:00:00  1167         18.8         1.0             0.1   \n",
       "20380 2024-08-25 06:00:00  1095         10.5         0.7             0.0   \n",
       "10290 2023-07-01 20:00:00  1095         18.0         2.2             0.3   \n",
       "15144 2024-01-20 02:00:00  1095        -19.2         0.6             0.0   \n",
       "14068 2023-12-06 06:00:00  1095        -18.2         0.6             0.9   \n",
       "7172  2024-04-24 20:00:00  1167          4.3         0.8             0.9   \n",
       "10392 2023-07-06 02:00:00  1095         12.8         2.0             1.0   \n",
       "17535 2024-04-28 17:00:00  1095          4.6         1.7             1.0   \n",
       "18865 2024-06-23 03:00:00  1095         12.0         0.5             0.9   \n",
       "22925 2023-10-04 08:00:00  1384         11.5         5.4             0.3   \n",
       "26481 2024-02-29 12:00:00  1384          4.9         7.5             1.0   \n",
       "16811 2024-03-29 13:00:00  1095          7.7         2.6             1.0   \n",
       "9213  2024-07-22 22:00:00  1167         16.1         0.4             0.7   \n",
       "17138 2024-04-12 04:00:00  1095          3.0         0.7             1.0   \n",
       "25991 2024-02-09 02:00:00  1384         -7.3         5.1             0.8   \n",
       "8876  2024-07-08 21:00:00  1167         13.4         1.2             0.0   \n",
       "8039  2024-05-30 23:00:00  1167         14.1         1.0             0.9   \n",
       "7755  2024-05-19 03:00:00  1167          6.7         1.2             0.0   \n",
       "22909 2023-10-03 16:00:00  1384         14.4         3.6             0.8   \n",
       "9053  2024-07-16 06:00:00  1167         15.3         1.1             0.9   \n",
       "29519 2024-07-05 02:00:00  1384          9.1         2.9             0.5   \n",
       "23042 2023-10-09 05:00:00  1384          3.3         3.0             0.3   \n",
       "2805  2023-10-25 21:00:00  1167          2.7         2.2             1.0   \n",
       "647   2023-07-27 23:00:00  1167         12.4         0.7             0.2   \n",
       "16667 2024-03-23 13:00:00  1095          8.3         1.5             0.9   \n",
       "29486 2024-07-03 17:00:00  1384         18.0         4.9             1.0   \n",
       "12587 2023-10-05 13:00:00  1095         12.6         1.3             0.1   \n",
       "1847  2023-09-15 23:00:00  1167         11.5         1.6             0.9   \n",
       "22124 2023-08-31 23:00:00  1384         12.3         2.5             0.3   \n",
       "29465 2024-07-02 20:00:00  1384         17.3         2.7             0.5   \n",
       "17603 2024-05-01 13:00:00  1095         20.0         1.4             0.2   \n",
       "14279 2023-12-15 01:00:00  1095        -12.2         0.7             0.8   \n",
       "1345  2023-08-26 01:00:00  1167         13.1         0.9             1.0   \n",
       "5479  2024-02-14 07:00:00  1167         -4.5         0.5             0.4   \n",
       "22001 2023-08-26 20:00:00  1384         14.8         2.2             1.0   \n",
       "15120 2024-01-19 02:00:00  1095        -20.2         1.1             0.9   \n",
       "27628 2024-04-17 07:00:00  1384          3.6         1.1             0.2   \n",
       "16643 2024-03-22 13:00:00  1095          9.4         3.7             0.8   \n",
       "20792 2023-07-07 11:00:00  1384         22.3         4.4             0.0   \n",
       "28957 2024-06-11 16:00:00  1384         18.9         2.8             0.9   \n",
       "17717 2024-05-06 07:00:00  1095          9.5         1.9             1.0   \n",
       "8535  2024-06-20 15:00:00  1167         22.2         3.5             0.0   \n",
       "5038  2024-01-26 22:00:00  1167         -0.7         2.2             0.0   \n",
       "25803 2024-02-01 06:00:00  1384          1.1         6.1             0.3   \n",
       "21081 2023-07-19 12:00:00  1384         19.8         4.9             0.6   \n",
       "9201  2024-07-22 10:00:00  1167         17.8         1.3             1.0   \n",
       "28528 2024-05-24 19:00:00  1384         18.1         1.2             0.7   \n",
       "2864  2023-10-28 08:00:00  1167          0.9         1.8             0.9   \n",
       "7194  2024-04-25 18:00:00  1167          6.0         0.8             0.9   \n",
       "3831  2023-12-07 15:00:00  1167         -2.9         1.6             0.9   \n",
       "27977 2024-05-01 20:00:00  1384         15.8         4.9             0.9   \n",
       "8473  2024-06-18 01:00:00  1167         12.1         1.3             0.9   \n",
       "10765 2023-07-21 15:00:00  1095         15.2         2.5             0.8   \n",
       "12    2023-07-01 12:00:00  1167         22.3         3.0             0.6   \n",
       "21758 2023-08-16 17:00:00  1384         20.6         2.4             0.0   \n",
       "14579 2023-12-27 13:00:00  1095        -14.1         0.6             0.3   \n",
       "6733  2024-04-06 13:00:00  1167          4.8         1.2             1.0   \n",
       "28612 2024-05-28 07:00:00  1384         16.4         1.6             0.5   \n",
       "12608 2023-10-06 10:00:00  1095          6.6         0.4             1.0   \n",
       "13042 2023-10-24 12:00:00  1095          5.3         1.3             1.0   \n",
       "16748 2024-03-26 22:00:00  1095          0.7         0.7             1.0   \n",
       "13885 2023-11-28 15:00:00  1095         -6.4         2.6             0.9   \n",
       "6     2023-07-01 06:00:00  1167         14.3         0.7             0.2   \n",
       "30864 2024-08-30 03:00:00  1384         12.3         3.5             0.0   \n",
       "21096 2023-07-20 03:00:00  1384         11.9         3.6             0.6   \n",
       "12496 2023-10-01 18:00:00  1095         10.4         0.8             0.1   \n",
       "29538 2024-07-05 21:00:00  1384         12.8         5.5             0.1   \n",
       "21234 2023-07-25 21:00:00  1384         16.3         4.7             0.1   \n",
       "5719  2024-02-24 07:00:00  1167          1.0         0.7             1.0   \n",
       "30855 2024-08-29 18:00:00  1384         18.4         2.4             0.9   \n",
       "21793 2023-08-18 04:00:00  1384         15.7         3.6             0.9   \n",
       "6056  2024-03-09 08:00:00  1167         -3.1         0.7             0.9   \n",
       "20064 2024-08-12 02:00:00  1095          7.9         0.2             0.0   \n",
       "14428 2023-12-21 06:00:00  1095         -9.3         1.0             1.0   \n",
       "7084  2024-04-21 04:00:00  1167         -0.8         1.4             0.0   \n",
       "14188 2023-12-11 06:00:00  1095         -2.0         1.6             1.0   \n",
       "21554 2023-08-08 05:00:00  1384         15.3         4.4             0.9   \n",
       "2151  2023-09-28 15:00:00  1167         18.0         2.8             0.9   \n",
       "11284 2023-08-12 06:00:00  1095         13.8         0.9             0.7   \n",
       "21438 2023-08-03 09:00:00  1384         19.1         3.4             0.9   \n",
       "18108 2024-05-22 14:00:00  1095         23.5         2.2             0.1   \n",
       "10640 2023-07-16 10:00:00  1095         19.6         2.8             0.8   \n",
       "16536 2024-03-18 02:00:00  1095         -2.9         0.7             0.9   \n",
       "16411 2024-03-12 21:00:00  1095         -3.5         0.5             0.9   \n",
       "17423 2024-04-24 01:00:00  1095          3.3         0.8             0.9   \n",
       "\n",
       "       precipitation  value_import  \n",
       "21775            0.0         65.30  \n",
       "7877             0.3         28.80  \n",
       "17826            0.0         32.32  \n",
       "9375             0.0         47.40  \n",
       "612              3.1         56.40  \n",
       "15353            0.0         35.68  \n",
       "12866            0.0         23.12  \n",
       "11094            0.0         31.20  \n",
       "4747             0.0         24.00  \n",
       "14674            0.1         34.24  \n",
       "10276            0.0         22.16  \n",
       "30248            0.0         27.90  \n",
       "7796             0.0         26.20  \n",
       "15787            1.9         38.00  \n",
       "28751            0.0         43.80  \n",
       "9403             0.0         41.00  \n",
       "20380            0.0         18.96  \n",
       "10290            0.0         32.40  \n",
       "15144            0.0         29.52  \n",
       "14068            0.0         46.48  \n",
       "7172             0.0         36.20  \n",
       "10392            0.0         25.20  \n",
       "17535            0.0         31.04  \n",
       "18865            0.0         23.52  \n",
       "22925            0.0         50.20  \n",
       "26481            0.4         45.80  \n",
       "16811            0.0         26.40  \n",
       "9213             0.0         30.40  \n",
       "17138            0.0         21.76  \n",
       "25991            0.0         32.70  \n",
       "8876             0.0         37.00  \n",
       "8039             0.0         26.00  \n",
       "7755             0.0         42.20  \n",
       "22909            0.0         52.50  \n",
       "9053             0.0         36.20  \n",
       "29519            0.0         27.20  \n",
       "23042            0.0         31.70  \n",
       "2805             0.0         34.40  \n",
       "647              0.0         19.20  \n",
       "16667            0.0         25.60  \n",
       "29486            0.0         53.00  \n",
       "12587            0.0         25.04  \n",
       "1847             0.0         20.00  \n",
       "22124            0.0         34.60  \n",
       "29465            0.0         52.90  \n",
       "17603            0.0          8.08  \n",
       "14279            0.0         29.60  \n",
       "1345             0.0         19.20  \n",
       "5479             0.0         38.00  \n",
       "22001            2.0         62.80  \n",
       "15120            0.0         33.52  \n",
       "27628            0.0         45.50  \n",
       "16643            0.0         30.16  \n",
       "20792            0.0         69.80  \n",
       "28957            0.0         54.00  \n",
       "17717            0.0         32.64  \n",
       "8535             0.0         43.40  \n",
       "5038             0.0         29.00  \n",
       "25803            0.0         44.60  \n",
       "21081            0.0         50.20  \n",
       "9201             4.0         41.60  \n",
       "28528            0.0         60.10  \n",
       "2864             0.0         35.80  \n",
       "7194             0.0         35.40  \n",
       "3831             0.0         38.60  \n",
       "27977            0.0         30.50  \n",
       "8473             0.0         24.80  \n",
       "10765            0.0         32.00  \n",
       "12               0.0         43.60  \n",
       "21758            0.0         71.50  \n",
       "14579            0.0         40.08  \n",
       "6733             0.0         36.20  \n",
       "28612            0.0         45.40  \n",
       "12608            0.0         31.68  \n",
       "13042            0.0         32.08  \n",
       "16748            0.2         33.92  \n",
       "13885            0.0         35.28  \n",
       "6                0.0         34.60  \n",
       "30864            0.0         28.40  \n",
       "21096            0.0         33.40  \n",
       "12496            0.0         34.40  \n",
       "29538            0.0         46.30  \n",
       "21234            0.0         61.40  \n",
       "5719             0.0         38.20  \n",
       "30855            0.0         52.50  \n",
       "21793            0.0         45.70  \n",
       "6056             0.0         37.40  \n",
       "20064            0.0         22.64  \n",
       "14428            0.3         31.36  \n",
       "7084             0.0         24.20  \n",
       "14188            0.1         32.64  \n",
       "21554            0.0         41.70  \n",
       "2151             0.0         39.80  \n",
       "11284            0.0         21.84  \n",
       "21438            0.0         62.70  \n",
       "18108            0.0          8.24  \n",
       "10640            0.0         22.88  \n",
       "16536            0.0         24.88  \n",
       "16411            0.0         34.32  \n",
       "17423            0.0         22.56  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>value_import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18989</th>\n",
       "      <td>2024-06-28 07:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>19.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17819</th>\n",
       "      <td>2024-05-10 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22044</th>\n",
       "      <td>2023-08-28 15:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>19.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22170</th>\n",
       "      <td>2023-09-02 21:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28695</th>\n",
       "      <td>2024-05-31 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>24.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24847</th>\n",
       "      <td>2023-12-23 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26503</th>\n",
       "      <td>2024-03-01 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7509</th>\n",
       "      <td>2024-05-08 21:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30235</th>\n",
       "      <td>2024-08-03 22:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>17.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9697</th>\n",
       "      <td>2024-08-12 02:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12134</th>\n",
       "      <td>2023-09-16 16:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15949</th>\n",
       "      <td>2024-02-22 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25541</th>\n",
       "      <td>2024-01-21 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>1.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>2024-05-17 17:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>21.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21463</th>\n",
       "      <td>2023-08-04 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>22.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>2024-06-08 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>2023-08-19 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>2023-11-30 17:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26575</th>\n",
       "      <td>2024-03-04 10:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>4.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>2023-08-22 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>19.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16371</th>\n",
       "      <td>2024-03-11 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9637</th>\n",
       "      <td>2024-08-09 14:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>20.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23854</th>\n",
       "      <td>2023-11-12 01:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>2023-10-06 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>33.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>2024-08-23 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29680</th>\n",
       "      <td>2024-07-11 19:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>17.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14233</th>\n",
       "      <td>2023-12-13 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>2024-05-30 01:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>12.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>2023-07-18 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>17.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10525</th>\n",
       "      <td>2023-07-11 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17578</th>\n",
       "      <td>2024-04-30 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>2023-09-13 17:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26662</th>\n",
       "      <td>2024-03-08 01:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10534</th>\n",
       "      <td>2023-07-12 00:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15017</th>\n",
       "      <td>2024-01-14 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>45.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>2024-07-22 04:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>26.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11431</th>\n",
       "      <td>2023-08-18 09:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>18.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25473</th>\n",
       "      <td>2024-01-18 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26024</th>\n",
       "      <td>2024-02-10 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11325</th>\n",
       "      <td>2023-08-13 23:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12270</th>\n",
       "      <td>2023-09-22 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11328</th>\n",
       "      <td>2023-08-14 02:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15196</th>\n",
       "      <td>2024-01-22 06:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20678</th>\n",
       "      <td>2023-07-02 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>2023-11-07 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>2023-09-19 10:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>2023-08-16 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28527</th>\n",
       "      <td>2024-05-24 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>18.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6960</th>\n",
       "      <td>2024-04-16 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>2024-06-13 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7227</th>\n",
       "      <td>2024-04-27 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8340</th>\n",
       "      <td>2024-06-12 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18627</th>\n",
       "      <td>2024-06-13 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5134</th>\n",
       "      <td>2024-01-30 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27398</th>\n",
       "      <td>2024-04-07 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15099</th>\n",
       "      <td>2024-01-18 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-18.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28369</th>\n",
       "      <td>2024-05-18 04:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>2023-10-30 16:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14449</th>\n",
       "      <td>2023-12-22 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14819</th>\n",
       "      <td>2024-01-06 13:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-23.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22163</th>\n",
       "      <td>2023-09-02 14:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16909</th>\n",
       "      <td>2024-04-02 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28566</th>\n",
       "      <td>2024-05-26 09:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>21.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>2024-09-02 02:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15715</th>\n",
       "      <td>2024-02-12 21:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>37.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>2023-10-23 04:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>2023-12-23 06:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-13.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15517</th>\n",
       "      <td>2024-02-04 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>2023-07-17 22:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>2023-10-20 22:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27441</th>\n",
       "      <td>2024-04-09 12:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>48.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18397</th>\n",
       "      <td>2024-06-03 15:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26108</th>\n",
       "      <td>2024-02-13 23:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>30.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>2024-07-26 03:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>38.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25597</th>\n",
       "      <td>2024-01-23 16:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26582</th>\n",
       "      <td>2024-03-04 17:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18259</th>\n",
       "      <td>2024-05-28 21:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16582</th>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27423</th>\n",
       "      <td>2024-04-08 18:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>2023-10-05 23:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13225</th>\n",
       "      <td>2023-11-01 03:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>2024-04-04 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14454</th>\n",
       "      <td>2023-12-22 08:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>2023-10-07 16:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23960</th>\n",
       "      <td>2023-11-16 11:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12963</th>\n",
       "      <td>2023-10-21 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22852</th>\n",
       "      <td>2023-10-01 07:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16738</th>\n",
       "      <td>2024-03-26 12:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>2024-01-25 18:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24317</th>\n",
       "      <td>2023-12-01 08:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>2024-08-14 00:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>14.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>2023-12-05 01:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>2023-12-21 15:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19193</th>\n",
       "      <td>2024-07-06 19:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>2023-12-18 00:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12027</th>\n",
       "      <td>2023-09-12 05:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2023-07-23 08:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18765</th>\n",
       "      <td>2024-06-18 23:00:00</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27683</th>\n",
       "      <td>2024-04-19 14:00:00</td>\n",
       "      <td>1384</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>2024-01-06 12:00:00</td>\n",
       "      <td>1167</td>\n",
       "      <td>-24.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "18989 2024-06-28 07:00:00  1095         19.8         3.4             0.9   \n",
       "17819 2024-05-10 13:00:00  1095         15.5         1.6             1.0   \n",
       "22044 2023-08-28 15:00:00  1384         19.5         4.1             0.1   \n",
       "22170 2023-09-02 21:00:00  1384         15.0         1.2             0.9   \n",
       "28695 2024-05-31 18:00:00  1384         24.7         2.7             0.9   \n",
       "24847 2023-12-23 10:00:00  1384         -4.0         3.2             0.1   \n",
       "26503 2024-03-01 10:00:00  1384          4.3         6.1             0.9   \n",
       "7509  2024-05-08 21:00:00  1167         12.2         0.5             0.9   \n",
       "30235 2024-08-03 22:00:00  1384         17.9         3.9             1.0   \n",
       "9697  2024-08-12 02:00:00  1167          8.4         1.2             0.0   \n",
       "12134 2023-09-16 16:00:00  1095         10.4         0.4             1.0   \n",
       "15949 2024-02-22 15:00:00  1095          2.0         0.7             1.0   \n",
       "25541 2024-01-21 08:00:00  1384          1.4         9.6             1.0   \n",
       "7721  2024-05-17 17:00:00  1167         21.4         1.7             0.0   \n",
       "21463 2023-08-04 10:00:00  1384         22.6         3.1             0.2   \n",
       "8232  2024-06-08 00:00:00  1167          5.4         0.4             0.2   \n",
       "1179  2023-08-19 03:00:00  1167         15.3         0.9             1.0   \n",
       "3665  2023-11-30 17:00:00  1167         -1.9         1.1             0.9   \n",
       "26575 2024-03-04 10:00:00  1384          4.2         5.6             0.4   \n",
       "1266  2023-08-22 18:00:00  1167         19.8         1.2             0.1   \n",
       "16371 2024-03-11 05:00:00  1095         -2.1         0.5             0.2   \n",
       "9637  2024-08-09 14:00:00  1167         20.6         2.4             0.5   \n",
       "23854 2023-11-12 01:00:00  1384          2.6         5.6             1.0   \n",
       "2336  2023-10-06 08:00:00  1167          7.5         1.0             1.0   \n",
       "20338 2024-08-23 12:00:00  1095         14.2         2.3             1.0   \n",
       "29680 2024-07-11 19:00:00  1384         17.5         4.0             0.9   \n",
       "14233 2023-12-13 03:00:00  1095         -4.0         0.4             0.6   \n",
       "8017  2024-05-30 01:00:00  1167         12.4         3.1             1.0   \n",
       "10686 2023-07-18 08:00:00  1095         17.3         2.5             0.2   \n",
       "10525 2023-07-11 15:00:00  1095         17.7         4.5             1.0   \n",
       "17578 2024-04-30 12:00:00  1095         13.0         1.1             0.7   \n",
       "12063 2023-09-13 17:00:00  1095         13.1         1.7             0.0   \n",
       "26662 2024-03-08 01:00:00  1384         -1.0         2.7             0.0   \n",
       "10534 2023-07-12 00:00:00  1095         14.5         2.2             1.0   \n",
       "15017 2024-01-14 19:00:00  1095         -0.7         5.5             1.0   \n",
       "19562 2024-07-22 04:00:00  1095         16.3         1.1             1.0   \n",
       "11431 2023-08-18 09:00:00  1095         18.6         2.0             0.7   \n",
       "25473 2024-01-18 12:00:00  1384         -4.2         2.6             0.6   \n",
       "26024 2024-02-10 11:00:00  1384         -6.6         9.2             0.9   \n",
       "11325 2023-08-13 23:00:00  1095         15.0         1.0             0.5   \n",
       "12270 2023-09-22 08:00:00  1095         10.7         0.5             1.0   \n",
       "11328 2023-08-14 02:00:00  1095         13.3         0.4             0.6   \n",
       "15196 2024-01-22 06:00:00  1095          0.4         2.1             1.0   \n",
       "20678 2023-07-02 17:00:00  1384         17.2         3.8             0.2   \n",
       "3104  2023-11-07 08:00:00  1167         -1.7         0.7             0.6   \n",
       "1930  2023-09-19 10:00:00  1167         14.3         3.1             0.9   \n",
       "11396 2023-08-16 22:00:00  1095         14.8         0.4             0.9   \n",
       "28527 2024-05-24 18:00:00  1384         18.7         0.6             0.8   \n",
       "6960  2024-04-16 00:00:00  1167         -1.7         1.2             0.0   \n",
       "8352  2024-06-13 00:00:00  1167         10.2         1.0             0.9   \n",
       "7227  2024-04-27 03:00:00  1167         -1.8         0.8             0.0   \n",
       "8340  2024-06-12 12:00:00  1167         19.5         2.8             0.7   \n",
       "18627 2024-06-13 05:00:00  1095          9.9         0.5             1.0   \n",
       "5134  2024-01-30 22:00:00  1167         -1.3         0.6             0.0   \n",
       "27398 2024-04-07 17:00:00  1384          7.8         8.8             0.8   \n",
       "15099 2024-01-18 05:00:00  1095        -18.5         1.2             0.0   \n",
       "28369 2024-05-18 04:00:00  1384         11.5         1.8             0.0   \n",
       "2920  2023-10-30 16:00:00  1167         -0.5         4.3             0.9   \n",
       "14449 2023-12-22 03:00:00  1095         -4.6         0.9             0.9   \n",
       "14819 2024-01-06 13:00:00  1095        -23.5         0.7             0.0   \n",
       "22163 2023-09-02 14:00:00  1384         21.2         2.8             0.6   \n",
       "16909 2024-04-02 15:00:00  1095          7.4         3.0             0.9   \n",
       "28566 2024-05-26 09:00:00  1384         21.5         2.4             0.0   \n",
       "10201 2024-09-02 02:00:00  1167          8.8         0.9             0.9   \n",
       "15715 2024-02-12 21:00:00  1095         -4.2         0.8             1.0   \n",
       "2740  2023-10-23 04:00:00  1167          4.0         1.0             0.9   \n",
       "4206  2023-12-23 06:00:00  1167        -13.4         0.9             0.0   \n",
       "15517 2024-02-04 15:00:00  1095         -0.3         1.3             0.5   \n",
       "406   2023-07-17 22:00:00  1167         13.8         2.2             0.0   \n",
       "12956 2023-10-20 22:00:00  1095          0.4         3.8             0.2   \n",
       "27441 2024-04-09 12:00:00  1384         10.7         3.4             1.0   \n",
       "18397 2024-06-03 15:00:00  1095         21.1         4.7             0.3   \n",
       "26108 2024-02-13 23:00:00  1384         -3.0         1.7             1.0   \n",
       "9290  2024-07-26 03:00:00  1167         15.0         2.2             1.0   \n",
       "25597 2024-01-23 16:00:00  1384          2.7         3.2             0.0   \n",
       "26582 2024-03-04 17:00:00  1384          3.7         5.9             0.8   \n",
       "18259 2024-05-28 21:00:00  1095         14.4         0.5             0.9   \n",
       "16582 2024-03-20 00:00:00  1095          0.3         0.6             1.0   \n",
       "27423 2024-04-08 18:00:00  1384          7.0         2.7             0.9   \n",
       "2327  2023-10-05 23:00:00  1167          5.9         0.7             1.0   \n",
       "13225 2023-11-01 03:00:00  1095         -0.9         0.4             0.7   \n",
       "16950 2024-04-04 08:00:00  1095         -2.6         1.9             1.0   \n",
       "14454 2023-12-22 08:00:00  1095         -6.2         3.2             0.9   \n",
       "2368  2023-10-07 16:00:00  1167         10.2         2.2             0.0   \n",
       "23960 2023-11-16 11:00:00  1384         -1.6         5.2             0.9   \n",
       "12963 2023-10-21 05:00:00  1095          0.4         3.9             0.9   \n",
       "22852 2023-10-01 07:00:00  1384          8.0         1.8             0.7   \n",
       "16738 2024-03-26 12:00:00  1095          2.3         0.6             1.0   \n",
       "5010  2024-01-25 18:00:00  1167         -3.8         0.5             0.9   \n",
       "24317 2023-12-01 08:00:00  1384         -5.0         4.2             0.1   \n",
       "9743  2024-08-14 00:00:00  1167         14.6         1.4             0.5   \n",
       "3769  2023-12-05 01:00:00  1167         -5.7         0.7             0.6   \n",
       "4167  2023-12-21 15:00:00  1167         -1.7         2.4             0.9   \n",
       "19193 2024-07-06 19:00:00  1095         15.5         1.1             0.9   \n",
       "14350 2023-12-18 00:00:00  1095         -3.4         1.0             0.3   \n",
       "12027 2023-09-12 05:00:00  1095         14.6         0.5             1.0   \n",
       "536   2023-07-23 08:00:00  1167         17.2         0.9             0.7   \n",
       "18765 2024-06-18 23:00:00  1095          9.4         0.7             0.3   \n",
       "27683 2024-04-19 14:00:00  1384          7.1         4.0             0.4   \n",
       "4548  2024-01-06 12:00:00  1167        -24.8         1.1             0.5   \n",
       "\n",
       "       precipitation  value_import  \n",
       "18989            0.0         35.04  \n",
       "17819            0.0         13.04  \n",
       "22044            0.0         64.70  \n",
       "22170            0.0         62.80  \n",
       "28695            0.0         70.90  \n",
       "24847            0.0         48.20  \n",
       "26503            0.0         48.90  \n",
       "7509             0.0         38.20  \n",
       "30235            0.0         52.40  \n",
       "9697             0.0         21.40  \n",
       "12134            0.5         32.16  \n",
       "15949            0.1         32.88  \n",
       "25541            0.0         28.40  \n",
       "7721             0.0         26.80  \n",
       "21463            0.0         72.70  \n",
       "8232             0.0         24.20  \n",
       "1179             0.0         20.20  \n",
       "3665             0.0         41.00  \n",
       "26575            0.0         47.30  \n",
       "1266             0.0         40.20  \n",
       "16371            0.0         21.28  \n",
       "9637             0.0         52.00  \n",
       "23854            0.0         28.20  \n",
       "2336             0.5         33.60  \n",
       "20338            0.5         30.16  \n",
       "29680            0.0         57.40  \n",
       "14233            0.0         27.60  \n",
       "8017             1.9         24.20  \n",
       "10686            0.0         31.36  \n",
       "10525            0.0         35.20  \n",
       "17578            0.0         21.20  \n",
       "12063            0.0         26.64  \n",
       "26662            0.0         33.30  \n",
       "10534            0.0         28.32  \n",
       "15017            0.2         45.92  \n",
       "19562            3.1         26.24  \n",
       "11431            0.0         29.84  \n",
       "25473            0.0         45.40  \n",
       "26024            0.0         48.20  \n",
       "11325            0.0         28.80  \n",
       "12270            0.1         32.88  \n",
       "11328            0.0         26.56  \n",
       "15196            0.3         36.80  \n",
       "20678            0.0         30.80  \n",
       "3104             0.0         35.60  \n",
       "1930             0.0         32.80  \n",
       "11396            0.0         39.20  \n",
       "28527            0.0         58.50  \n",
       "6960             0.0         23.20  \n",
       "8352             0.0         23.40  \n",
       "7227             0.0         24.40  \n",
       "8340             0.0         38.60  \n",
       "18627            0.0         19.04  \n",
       "5134             0.0         26.60  \n",
       "27398            0.0         29.70  \n",
       "15099            0.0         31.76  \n",
       "28369            0.0         26.40  \n",
       "2920             0.3         36.20  \n",
       "14449            0.0         26.08  \n",
       "14819            0.0         44.00  \n",
       "22163            0.0         58.50  \n",
       "16909            0.0          8.96  \n",
       "28566            0.0         28.50  \n",
       "10201            0.0         22.40  \n",
       "15715            0.8         37.76  \n",
       "2740             0.0         20.80  \n",
       "4206             0.0         42.00  \n",
       "15517            0.0         32.08  \n",
       "406              0.0         27.80  \n",
       "12956            0.0         36.80  \n",
       "27441            0.6         48.00  \n",
       "18397            0.0          7.20  \n",
       "26108            1.7         30.60  \n",
       "9290             3.2         38.60  \n",
       "25597            0.0         49.30  \n",
       "26582            0.0         47.30  \n",
       "18259            0.0         36.88  \n",
       "16582            0.0         20.24  \n",
       "27423            0.0         46.90  \n",
       "2327             0.0         17.60  \n",
       "13225            0.0         25.36  \n",
       "16950            0.3         30.08  \n",
       "14454            0.0         39.84  \n",
       "2368             0.0         35.40  \n",
       "23960            0.0         47.70  \n",
       "12963            0.0         21.92  \n",
       "22852            0.0         30.80  \n",
       "16738            0.0         27.92  \n",
       "5010             0.0         39.40  \n",
       "24317            0.0         51.30  \n",
       "9743             0.0         25.80  \n",
       "3769             0.0         25.60  \n",
       "4167             0.0         39.40  \n",
       "19193            0.0         32.72  \n",
       "14350            0.0         26.48  \n",
       "12027            0.0         23.60  \n",
       "536              0.0         18.20  \n",
       "18765            0.0         25.92  \n",
       "27683            0.0         46.50  \n",
       "4548             0.0         44.20  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241109_185606\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       13.88 GB / 32.00 GB (43.4%)\n",
      "Disk Space Avail:   618.51 GB / 926.35 GB (66.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241109_185606/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241109_185606/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    22047\n",
      "Train Data Columns: 6\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14214.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.1s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.68 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.78s of the 899.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.92006\n",
      "[2000]\tvalid_set's l1: 5.66781\n",
      "[3000]\tvalid_set's l1: 5.51892\n",
      "[4000]\tvalid_set's l1: 5.41641\n",
      "[5000]\tvalid_set's l1: 5.34046\n",
      "[6000]\tvalid_set's l1: 5.27797\n",
      "[7000]\tvalid_set's l1: 5.22588\n",
      "[8000]\tvalid_set's l1: 5.18609\n",
      "[9000]\tvalid_set's l1: 5.15965\n",
      "[10000]\tvalid_set's l1: 5.13844\n",
      "[1000]\tvalid_set's l1: 5.89552\n",
      "[2000]\tvalid_set's l1: 5.64629\n",
      "[3000]\tvalid_set's l1: 5.50757\n",
      "[4000]\tvalid_set's l1: 5.412\n",
      "[5000]\tvalid_set's l1: 5.34405\n",
      "[6000]\tvalid_set's l1: 5.28655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6791. Best iteration is:\n",
      "\t[6790]\tvalid_set's l1: 5.2483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.89883\n",
      "[2000]\tvalid_set's l1: 5.62877\n",
      "[3000]\tvalid_set's l1: 5.47533\n",
      "[4000]\tvalid_set's l1: 5.36764\n",
      "[5000]\tvalid_set's l1: 5.29681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5428. Best iteration is:\n",
      "\t[5428]\tvalid_set's l1: 5.2738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.88329\n",
      "[2000]\tvalid_set's l1: 5.59633\n",
      "[3000]\tvalid_set's l1: 5.43578\n",
      "[4000]\tvalid_set's l1: 5.34148\n",
      "[5000]\tvalid_set's l1: 5.27177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5509. Best iteration is:\n",
      "\t[5509]\tvalid_set's l1: 5.2461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.10396\n",
      "[2000]\tvalid_set's l1: 5.84863\n",
      "[3000]\tvalid_set's l1: 5.69724\n",
      "[4000]\tvalid_set's l1: 5.59013\n",
      "[5000]\tvalid_set's l1: 5.5203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5705. Best iteration is:\n",
      "\t[5700]\tvalid_set's l1: 5.48101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.95494\n",
      "[2000]\tvalid_set's l1: 5.6501\n",
      "[3000]\tvalid_set's l1: 5.48205\n",
      "[4000]\tvalid_set's l1: 5.36771\n",
      "[5000]\tvalid_set's l1: 5.29249\n",
      "[6000]\tvalid_set's l1: 5.23835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6300. Best iteration is:\n",
      "\t[6294]\tvalid_set's l1: 5.22655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.98599\n",
      "[2000]\tvalid_set's l1: 5.72767\n",
      "[3000]\tvalid_set's l1: 5.58269\n",
      "[4000]\tvalid_set's l1: 5.47704\n",
      "[5000]\tvalid_set's l1: 5.4014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5903. Best iteration is:\n",
      "\t[5893]\tvalid_set's l1: 5.34986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.92343\n",
      "[2000]\tvalid_set's l1: 5.6299\n",
      "[3000]\tvalid_set's l1: 5.47571\n",
      "[4000]\tvalid_set's l1: 5.36757\n",
      "[5000]\tvalid_set's l1: 5.29234\n",
      "[6000]\tvalid_set's l1: 5.22839\n",
      "[7000]\tvalid_set's l1: 5.1895\n",
      "[8000]\tvalid_set's l1: 5.15044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8300. Best iteration is:\n",
      "\t[8299]\tvalid_set's l1: 5.1403\n",
      "\t-5.263\t = Validation score   (-mean_absolute_error)\n",
      "\t571.91s\t = Training   runtime\n",
      "\t2.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 24.65s of the 324.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 196. Best iteration is:\n",
      "\t[196]\tvalid_set's l1: 4.98857\n",
      "\tRan out of time, early stopping on iteration 166. Best iteration is:\n",
      "\t[166]\tvalid_set's l1: 5.07803\n",
      "\tRan out of time, early stopping on iteration 219. Best iteration is:\n",
      "\t[219]\tvalid_set's l1: 4.97151\n",
      "\tRan out of time, early stopping on iteration 226. Best iteration is:\n",
      "\t[226]\tvalid_set's l1: 5.06528\n",
      "\tRan out of time, early stopping on iteration 187. Best iteration is:\n",
      "\t[187]\tvalid_set's l1: 5.23631\n",
      "\tRan out of time, early stopping on iteration 249. Best iteration is:\n",
      "\t[249]\tvalid_set's l1: 4.89836\n",
      "\tRan out of time, early stopping on iteration 272. Best iteration is:\n",
      "\t[272]\tvalid_set's l1: 4.88503\n",
      "\tRan out of time, early stopping on iteration 326. Best iteration is:\n",
      "\t[326]\tvalid_set's l1: 4.87255\n",
      "\t-4.9995\t = Validation score   (-mean_absolute_error)\n",
      "\t23.6s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.95s of the 301.05s of remaining time.\n",
      "\t-3.6137\t = Validation score   (-mean_absolute_error)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 297.23s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.6137\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 297.21s of the 297.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5623\t = Validation score   (-mean_absolute_error)\n",
      "\t59.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 237.99s of the 237.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5165\t = Validation score   (-mean_absolute_error)\n",
      "\t26.76s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 211.16s of the 211.14s of remaining time.\n",
      "\t-3.5615\t = Validation score   (-mean_absolute_error)\n",
      "\t6.17s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 203.96s of the 203.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.512\t = Validation score   (-mean_absolute_error)\n",
      "\t11.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 192.88s of the 192.86s of remaining time.\n",
      "\t-3.5591\t = Validation score   (-mean_absolute_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 190.4s of the 190.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.4924\t = Validation score   (-mean_absolute_error)\n",
      "\t152.32s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 37.67s of the 37.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5184\t = Validation score   (-mean_absolute_error)\n",
      "\t5.73s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 31.87s of the 31.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.5833\t = Validation score   (-mean_absolute_error)\n",
      "\t29.9s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1.66s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.458, 'RandomForestMSE_BAG_L2': 0.208, 'NeuralNetTorch_BAG_L2': 0.167, 'XGBoost_BAG_L2': 0.083, 'LightGBM_BAG_L2': 0.042, 'CatBoost_BAG_L2': 0.042}\n",
      "\t-3.471\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 898.41s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 994.0 rows/s (2756 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241109_185606/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -3.418357  -3.471022  mean_absolute_error        3.906950       3.923611  830.292105                 0.001340                0.000339           0.048408            3       True         13\n",
      "1   NeuralNetFastAI_BAG_L2      -3.433279  -3.492371  mean_absolute_error        3.302078       3.007943  750.643906                 0.435572                0.296341         152.320990            2       True         10\n",
      "2          CatBoost_BAG_L2      -3.449299  -3.512031  mean_absolute_error        2.887771       2.720553  609.360386                 0.021265                0.008950          11.037470            2       True          8\n",
      "3          LightGBM_BAG_L2      -3.449682  -3.516536  mean_absolute_error        2.900224       2.737696  625.087051                 0.033718                0.026094          26.764134            2       True          6\n",
      "4           XGBoost_BAG_L2      -3.455154  -3.518389  mean_absolute_error        2.921397       2.736973  604.050695                 0.054891                0.025371           5.727778            2       True         11\n",
      "5        LightGBMXT_BAG_L2      -3.471840  -3.562280  mean_absolute_error        2.988330       2.802731  657.332426                 0.121824                0.091129          59.009509            2       True          5\n",
      "6     ExtraTreesMSE_BAG_L2      -3.488718  -3.559147  mean_absolute_error        3.153529       3.377491  599.759876                 0.287023                0.665889           1.436959            2       True          9\n",
      "7   RandomForestMSE_BAG_L2      -3.489943  -3.561457  mean_absolute_error        3.155429       3.387681  604.491185                 0.288923                0.676079           6.168269            2       True          7\n",
      "8    NeuralNetTorch_BAG_L2      -3.502563  -3.583269  mean_absolute_error        3.071241       2.890436  628.225056                 0.204735                0.178834          29.902140            2       True         12\n",
      "9   RandomForestMSE_BAG_L1      -3.569846  -3.613743  mean_absolute_error        0.271667       0.638950    2.811010                 0.271667                0.638950           2.811010            1       True          3\n",
      "10     WeightedEnsemble_L2      -3.569846  -3.613743  mean_absolute_error        0.273913       0.639262    2.828067                 0.002246                0.000312           0.017057            2       True          4\n",
      "11         LightGBM_BAG_L1      -4.899620  -4.999461  mean_absolute_error        0.097175       0.044063   23.604806                 0.097175                0.044063          23.604806            1       True          2\n",
      "12       LightGBMXT_BAG_L1      -5.215210  -5.263048  mean_absolute_error        2.497664       2.028589  571.907101                 2.497664                2.028589         571.907101            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t903s\t = DyStack   runtime |\t2697s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2697s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241109_185606\"\n",
      "Train Data Rows:    24803\n",
      "Train Data Columns: 6\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14832.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.14 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 4 | ['temperature', 'wind_speed', 'cloud_fraction', 'precipitation']\n",
      "\t\t('int', [])                  : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.1s = Fit runtime\n",
      "\t6 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1797.61s of the 2697.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 6.03549\n",
      "[2000]\tvalid_set's l1: 5.76632\n",
      "[3000]\tvalid_set's l1: 5.59784\n",
      "[4000]\tvalid_set's l1: 5.49569\n",
      "[5000]\tvalid_set's l1: 5.42093\n",
      "[6000]\tvalid_set's l1: 5.36513\n",
      "[7000]\tvalid_set's l1: 5.32061\n",
      "[8000]\tvalid_set's l1: 5.28339\n",
      "[9000]\tvalid_set's l1: 5.25948\n",
      "[10000]\tvalid_set's l1: 5.23127\n",
      "[1000]\tvalid_set's l1: 5.92203\n",
      "[2000]\tvalid_set's l1: 5.62141\n",
      "[3000]\tvalid_set's l1: 5.44073\n",
      "[4000]\tvalid_set's l1: 5.32763\n",
      "[5000]\tvalid_set's l1: 5.24937\n",
      "[6000]\tvalid_set's l1: 5.18427\n",
      "[7000]\tvalid_set's l1: 5.13078\n",
      "[8000]\tvalid_set's l1: 5.09112\n",
      "[9000]\tvalid_set's l1: 5.05224\n",
      "[10000]\tvalid_set's l1: 5.02769\n",
      "[1000]\tvalid_set's l1: 6.03997\n",
      "[2000]\tvalid_set's l1: 5.78466\n",
      "[3000]\tvalid_set's l1: 5.63077\n",
      "[4000]\tvalid_set's l1: 5.51999\n",
      "[5000]\tvalid_set's l1: 5.44119\n",
      "[6000]\tvalid_set's l1: 5.37778\n",
      "[7000]\tvalid_set's l1: 5.33243\n",
      "[8000]\tvalid_set's l1: 5.29337\n",
      "[9000]\tvalid_set's l1: 5.2602\n",
      "[10000]\tvalid_set's l1: 5.23773\n",
      "[1000]\tvalid_set's l1: 5.90469\n",
      "[2000]\tvalid_set's l1: 5.60981\n",
      "[3000]\tvalid_set's l1: 5.43333\n",
      "[4000]\tvalid_set's l1: 5.32112\n",
      "[5000]\tvalid_set's l1: 5.24477\n",
      "[6000]\tvalid_set's l1: 5.18767\n",
      "[7000]\tvalid_set's l1: 5.1454\n",
      "[8000]\tvalid_set's l1: 5.10974\n",
      "[9000]\tvalid_set's l1: 5.08432\n",
      "[10000]\tvalid_set's l1: 5.05838\n",
      "[1000]\tvalid_set's l1: 5.87912\n",
      "[2000]\tvalid_set's l1: 5.60941\n",
      "[3000]\tvalid_set's l1: 5.46996\n",
      "[4000]\tvalid_set's l1: 5.36417\n",
      "[5000]\tvalid_set's l1: 5.29571\n",
      "[6000]\tvalid_set's l1: 5.23877\n",
      "[7000]\tvalid_set's l1: 5.19889\n",
      "[8000]\tvalid_set's l1: 5.16638\n",
      "[9000]\tvalid_set's l1: 5.13282\n",
      "[10000]\tvalid_set's l1: 5.10286\n",
      "[1000]\tvalid_set's l1: 6.01029\n",
      "[2000]\tvalid_set's l1: 5.71364\n",
      "[3000]\tvalid_set's l1: 5.55122\n",
      "[4000]\tvalid_set's l1: 5.44727\n",
      "[5000]\tvalid_set's l1: 5.37038\n",
      "[6000]\tvalid_set's l1: 5.31693\n",
      "[7000]\tvalid_set's l1: 5.27071\n",
      "[8000]\tvalid_set's l1: 5.23907\n",
      "[9000]\tvalid_set's l1: 5.20974\n",
      "[10000]\tvalid_set's l1: 5.18413\n",
      "[1000]\tvalid_set's l1: 5.83223\n",
      "[2000]\tvalid_set's l1: 5.58193\n",
      "[3000]\tvalid_set's l1: 5.43712\n",
      "[4000]\tvalid_set's l1: 5.33114\n",
      "[5000]\tvalid_set's l1: 5.25525\n",
      "[6000]\tvalid_set's l1: 5.19901\n",
      "[7000]\tvalid_set's l1: 5.14801\n",
      "[8000]\tvalid_set's l1: 5.10329\n",
      "[9000]\tvalid_set's l1: 5.07217\n",
      "[10000]\tvalid_set's l1: 5.04136\n",
      "[1000]\tvalid_set's l1: 5.84922\n",
      "[2000]\tvalid_set's l1: 5.57699\n",
      "[3000]\tvalid_set's l1: 5.44205\n",
      "[4000]\tvalid_set's l1: 5.34093\n",
      "[5000]\tvalid_set's l1: 5.27225\n",
      "[6000]\tvalid_set's l1: 5.22839\n",
      "[7000]\tvalid_set's l1: 5.19153\n",
      "[8000]\tvalid_set's l1: 5.15867\n",
      "[9000]\tvalid_set's l1: 5.1338\n",
      "[10000]\tvalid_set's l1: 5.11606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-5.1246\t = Validation score   (-mean_absolute_error)\n",
      "\t880.36s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 911.98s of the 1811.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.61443\n",
      "[2000]\tvalid_set's l1: 4.44388\n",
      "[3000]\tvalid_set's l1: 4.36272\n",
      "[4000]\tvalid_set's l1: 4.3291\n",
      "[5000]\tvalid_set's l1: 4.31051\n",
      "[6000]\tvalid_set's l1: 4.29542\n",
      "[7000]\tvalid_set's l1: 4.28493\n",
      "[8000]\tvalid_set's l1: 4.28281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8143. Best iteration is:\n",
      "\t[7929]\tvalid_set's l1: 4.28196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.45353\n",
      "[2000]\tvalid_set's l1: 4.24334\n",
      "[3000]\tvalid_set's l1: 4.17267\n",
      "[4000]\tvalid_set's l1: 4.13844\n",
      "[5000]\tvalid_set's l1: 4.11029\n",
      "[6000]\tvalid_set's l1: 4.09064\n",
      "[7000]\tvalid_set's l1: 4.07954\n",
      "[8000]\tvalid_set's l1: 4.08101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 8911. Best iteration is:\n",
      "\t[8900]\tvalid_set's l1: 4.07494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.60128\n",
      "[2000]\tvalid_set's l1: 4.41894\n",
      "[3000]\tvalid_set's l1: 4.31968\n",
      "[4000]\tvalid_set's l1: 4.2763\n",
      "[5000]\tvalid_set's l1: 4.25339\n",
      "[6000]\tvalid_set's l1: 4.24271\n",
      "[7000]\tvalid_set's l1: 4.23167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 7645. Best iteration is:\n",
      "\t[7331]\tvalid_set's l1: 4.22771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.54051\n",
      "[2000]\tvalid_set's l1: 4.36481\n",
      "[3000]\tvalid_set's l1: 4.28381\n",
      "[4000]\tvalid_set's l1: 4.25482\n",
      "[5000]\tvalid_set's l1: 4.23095\n",
      "[6000]\tvalid_set's l1: 4.22501\n",
      "[7000]\tvalid_set's l1: 4.22529\n",
      "[1000]\tvalid_set's l1: 4.5333\n",
      "[2000]\tvalid_set's l1: 4.32893\n",
      "[3000]\tvalid_set's l1: 4.23856\n",
      "[4000]\tvalid_set's l1: 4.19375\n",
      "[5000]\tvalid_set's l1: 4.16914\n",
      "[6000]\tvalid_set's l1: 4.15592\n",
      "[7000]\tvalid_set's l1: 4.14293\n",
      "[8000]\tvalid_set's l1: 4.13753\n",
      "[9000]\tvalid_set's l1: 4.13851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 9862. Best iteration is:\n",
      "\t[9825]\tvalid_set's l1: 4.1341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.52472\n",
      "[2000]\tvalid_set's l1: 4.34443\n",
      "[3000]\tvalid_set's l1: 4.26338\n",
      "[4000]\tvalid_set's l1: 4.2182\n",
      "[5000]\tvalid_set's l1: 4.19852\n",
      "[6000]\tvalid_set's l1: 4.18526\n",
      "[7000]\tvalid_set's l1: 4.1845\n",
      "[8000]\tvalid_set's l1: 4.18805\n",
      "[1000]\tvalid_set's l1: 4.43207\n",
      "[2000]\tvalid_set's l1: 4.2146\n",
      "[3000]\tvalid_set's l1: 4.12805\n",
      "[4000]\tvalid_set's l1: 4.07309\n",
      "[5000]\tvalid_set's l1: 4.03687\n",
      "[6000]\tvalid_set's l1: 4.01975\n",
      "[7000]\tvalid_set's l1: 4.00471\n",
      "[8000]\tvalid_set's l1: 3.99838\n",
      "[9000]\tvalid_set's l1: 4.00485\n",
      "[10000]\tvalid_set's l1: 4.01093\n",
      "[1000]\tvalid_set's l1: 4.45086\n",
      "[2000]\tvalid_set's l1: 4.26881\n",
      "[3000]\tvalid_set's l1: 4.19756\n",
      "[4000]\tvalid_set's l1: 4.16446\n",
      "[5000]\tvalid_set's l1: 4.14628\n",
      "[6000]\tvalid_set's l1: 4.14811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.1577\t = Validation score   (-mean_absolute_error)\n",
      "\t753.37s\t = Training   runtime\n",
      "\t2.3s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 155.0s of the 1054.47s of remaining time.\n",
      "\t-3.4913\t = Validation score   (-mean_absolute_error)\n",
      "\t3.72s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 150.19s of the 1049.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 3826.\n",
      "\tRan out of time, early stopping on iteration 3947.\n",
      "\tRan out of time, early stopping on iteration 3957.\n",
      "\tRan out of time, early stopping on iteration 4136.\n",
      "\tRan out of time, early stopping on iteration 4257.\n",
      "\tRan out of time, early stopping on iteration 4417.\n",
      "\tRan out of time, early stopping on iteration 4630.\n",
      "\tRan out of time, early stopping on iteration 5149.\n",
      "\t-4.4949\t = Validation score   (-mean_absolute_error)\n",
      "\t144.04s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 6.07s of the 905.55s of remaining time.\n",
      "\t-3.7325\t = Validation score   (-mean_absolute_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 3.78s of the 903.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3.21s of the 902.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-4.8575\t = Validation score   (-mean_absolute_error)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.09s of the 899.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 899.36s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.4913\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 899.32s of the 899.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.4022\t = Validation score   (-mean_absolute_error)\n",
      "\t17.93s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 881.19s of the 881.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3854\t = Validation score   (-mean_absolute_error)\n",
      "\t8.82s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 872.28s of the 872.26s of remaining time.\n",
      "\t-3.3991\t = Validation score   (-mean_absolute_error)\n",
      "\t10.78s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 860.32s of the 860.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3793\t = Validation score   (-mean_absolute_error)\n",
      "\t17.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 842.89s of the 842.86s of remaining time.\n",
      "\t-3.3762\t = Validation score   (-mean_absolute_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 839.5s of the 839.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.339\t = Validation score   (-mean_absolute_error)\n",
      "\t188.81s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 650.22s of the 650.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3879\t = Validation score   (-mean_absolute_error)\n",
      "\t4.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 645.76s of the 645.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 99)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.2903\t = Validation score   (-mean_absolute_error)\n",
      "\t508.42s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 137.02s of the 136.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.384\t = Validation score   (-mean_absolute_error)\n",
      "\t35.61s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 101.17s of the 101.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3842\t = Validation score   (-mean_absolute_error)\n",
      "\t12.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 88.95s of the 88.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 9)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.3302\t = Validation score   (-mean_absolute_error)\n",
      "\t84.73s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3.71s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L2': 0.467, 'NeuralNetTorch_r79_BAG_L2': 0.267, 'NeuralNetFastAI_BAG_L2': 0.133, 'RandomForestMSE_BAG_L2': 0.067, 'ExtraTreesMSE_BAG_L2': 0.067}\n",
      "\t-3.2527\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2693.57s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 431.7 rows/s (3101 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241109_185606\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 6 features using 5000 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t470.48s\t= Expected runtime (94.1s per shuffle set)\n",
      "\t379.88s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "                importance    stddev       p_value  n  p99_high   p99_low\n",
      "area              5.598706  0.073723  3.607004e-09  5  5.750503  5.446909\n",
      "timestamp         5.322324  0.073009  4.247822e-09  5  5.472650  5.171998\n",
      "temperature       4.483050  0.063063  4.697688e-09  5  4.612898  4.353201\n",
      "wind_speed        0.673139  0.029942  4.685364e-07  5  0.734791  0.611488\n",
      "cloud_fraction    0.503317  0.029003  1.317308e-06  5  0.563036  0.443599\n",
      "precipitation     0.172484  0.010950  1.939006e-06  5  0.195031  0.149937\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "performance\n",
    "# reset index \n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed  cloud_fraction  \\\n",
       "0     2023-07-01 00:00:00  1199         13.6         1.6             0.5   \n",
       "1     2023-07-01 01:00:00  1199         13.2         2.0             0.4   \n",
       "2     2023-07-01 02:00:00  1199         12.3         1.6             0.3   \n",
       "3     2023-07-01 03:00:00  1199         11.9         0.6             0.9   \n",
       "4     2023-07-01 04:00:00  1199         11.9         0.2             0.8   \n",
       "...                   ...   ...          ...         ...             ...   \n",
       "10338 2024-09-03 18:00:00  1199         17.8         2.5             1.0   \n",
       "10339 2024-09-03 19:00:00  1199         17.8         1.8             1.0   \n",
       "10340 2024-09-03 20:00:00  1199         17.7         1.1             1.0   \n",
       "10341 2024-09-03 21:00:00  1199         18.0         3.4             1.0   \n",
       "10342 2024-09-03 22:00:00  1199         17.9         3.0             1.0   \n",
       "\n",
       "       precipitation  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "...              ...  \n",
       "10338            1.4  \n",
       "10339            2.5  \n",
       "10340            2.6  \n",
       "10341            2.9  \n",
       "10342            2.7  \n",
       "\n",
       "[10343 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "data_predict = main_building[features]\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / 'prediction_drita2.csv'\n",
    "# create folder\n",
    "my_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
