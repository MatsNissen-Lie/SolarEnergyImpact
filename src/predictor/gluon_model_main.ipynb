{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/Desktop/SolarEnergyImpact\n",
      "Index(['HourUTC', 'HourDK', 'PriceArea', 'SpotPriceDKK', 'SpotPriceEUR'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().absolute().parent.parent\n",
    "print(root)\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from src.pipeline import Pipeline, BuilingIdsEnum\n",
    "pipe = Pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_b = pipe.get_data(BuilingIdsEnum.B)\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "# remove from 3. to 7. july 2024 from dataset A\n",
    "# mask = (building_a['timestamp'] >= '2024-07-03') & (building_a['timestamp'] <= '2024-07-07')\n",
    "# building_a = building_a[~mask]\n",
    "# let value_import be equal to net_consumption for main\n",
    "main_building['value_import'] = main_building['net_consumption']\n",
    "\n",
    "# normalize the value_import by area and call it value for main and building_b\n",
    "main_building['value'] = main_building['value_import'] / main_building['area']\n",
    "building_b['value'] = building_b['value_import'] / building_b['area']\n",
    "\n",
    "combined_df = pd.concat([main_building, building_b])\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# Perform the train-test split with stratification based on 'building_id'\n",
    "train_data, test_data = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['building']\n",
    ")\n",
    "\n",
    "# select features\n",
    "target = \"value\"\n",
    "features = [\"timestamp\", \"area\", \"temperature\", \"wind_speed\"] #, \"cloud_fraction\", \"precipitation\"\n",
    "\n",
    "train_data = train_data[features + [target]]\n",
    "test_data = test_data[features + [target]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241111_184001\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       13.34 GB / 32.00 GB (41.7%)\n",
      "Disk Space Avail:   615.03 GB / 926.35 GB (66.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241111_184001/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241111_184001/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    14727\n",
      "Train Data Columns: 4\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13650.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.45 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 2 | ['temperature', 'wind_speed']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 2 | ['temperature', 'wind_speed']\n",
      "\t\t('int', ['bool'])            : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.80 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.58s of the 899.58s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` \n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00472896\n",
      "[2000]\tvalid_set's l1: 0.00448904\n",
      "[3000]\tvalid_set's l1: 0.00435077\n",
      "[4000]\tvalid_set's l1: 0.00425569\n",
      "[5000]\tvalid_set's l1: 0.00418858\n",
      "[6000]\tvalid_set's l1: 0.00413702\n",
      "[7000]\tvalid_set's l1: 0.00410055\n",
      "[8000]\tvalid_set's l1: 0.00407165\n",
      "[9000]\tvalid_set's l1: 0.00404406\n",
      "[10000]\tvalid_set's l1: 0.00401983\n",
      "[1000]\tvalid_set's l1: 0.0047699\n",
      "[2000]\tvalid_set's l1: 0.00458817\n",
      "[3000]\tvalid_set's l1: 0.00447966\n",
      "[4000]\tvalid_set's l1: 0.00442139\n",
      "[5000]\tvalid_set's l1: 0.00437038\n",
      "[6000]\tvalid_set's l1: 0.00432504\n",
      "[7000]\tvalid_set's l1: 0.00429198\n",
      "[8000]\tvalid_set's l1: 0.00426502\n",
      "[9000]\tvalid_set's l1: 0.00424478\n",
      "[10000]\tvalid_set's l1: 0.00423059\n",
      "[1000]\tvalid_set's l1: 0.00485891\n",
      "[2000]\tvalid_set's l1: 0.00466093\n",
      "[3000]\tvalid_set's l1: 0.0045501\n",
      "[4000]\tvalid_set's l1: 0.00447003\n",
      "[5000]\tvalid_set's l1: 0.00440727\n",
      "[6000]\tvalid_set's l1: 0.00435942\n",
      "[7000]\tvalid_set's l1: 0.00432405\n",
      "[8000]\tvalid_set's l1: 0.00429692\n",
      "[9000]\tvalid_set's l1: 0.00427844\n",
      "[10000]\tvalid_set's l1: 0.00426359\n",
      "[1000]\tvalid_set's l1: 0.00471778\n",
      "[2000]\tvalid_set's l1: 0.00451778\n",
      "[3000]\tvalid_set's l1: 0.00440358\n",
      "[4000]\tvalid_set's l1: 0.0043239\n",
      "[5000]\tvalid_set's l1: 0.00426236\n",
      "[6000]\tvalid_set's l1: 0.00421927\n",
      "[7000]\tvalid_set's l1: 0.00418178\n",
      "[8000]\tvalid_set's l1: 0.00415322\n",
      "[9000]\tvalid_set's l1: 0.00412801\n",
      "[10000]\tvalid_set's l1: 0.0041103\n",
      "[1000]\tvalid_set's l1: 0.00481163\n",
      "[2000]\tvalid_set's l1: 0.00463134\n",
      "[3000]\tvalid_set's l1: 0.00453193\n",
      "[4000]\tvalid_set's l1: 0.00446478\n",
      "[5000]\tvalid_set's l1: 0.00441239\n",
      "[6000]\tvalid_set's l1: 0.00436908\n",
      "[7000]\tvalid_set's l1: 0.00433584\n",
      "[8000]\tvalid_set's l1: 0.00431271\n",
      "[9000]\tvalid_set's l1: 0.0042855\n",
      "[10000]\tvalid_set's l1: 0.00426175\n",
      "[1000]\tvalid_set's l1: 0.00487674\n",
      "[2000]\tvalid_set's l1: 0.00468655\n",
      "[3000]\tvalid_set's l1: 0.0045745\n",
      "[4000]\tvalid_set's l1: 0.00449706\n",
      "[5000]\tvalid_set's l1: 0.00443089\n",
      "[6000]\tvalid_set's l1: 0.00438289\n",
      "[7000]\tvalid_set's l1: 0.00434343\n",
      "[8000]\tvalid_set's l1: 0.00430917\n",
      "[9000]\tvalid_set's l1: 0.00428179\n",
      "[10000]\tvalid_set's l1: 0.00426095\n",
      "[1000]\tvalid_set's l1: 0.00488461\n",
      "[2000]\tvalid_set's l1: 0.00469491\n",
      "[3000]\tvalid_set's l1: 0.004593\n",
      "[4000]\tvalid_set's l1: 0.0045175\n",
      "[5000]\tvalid_set's l1: 0.00445532\n",
      "[6000]\tvalid_set's l1: 0.00441173\n",
      "[7000]\tvalid_set's l1: 0.00438163\n",
      "[8000]\tvalid_set's l1: 0.00436074\n",
      "[9000]\tvalid_set's l1: 0.00433782\n",
      "[10000]\tvalid_set's l1: 0.00432636\n",
      "[1000]\tvalid_set's l1: 0.00483429\n",
      "[2000]\tvalid_set's l1: 0.00459645\n",
      "[3000]\tvalid_set's l1: 0.0044592\n",
      "[4000]\tvalid_set's l1: 0.00437855\n",
      "[5000]\tvalid_set's l1: 0.00430827\n",
      "[6000]\tvalid_set's l1: 0.00425051\n",
      "[7000]\tvalid_set's l1: 0.00421576\n",
      "[8000]\tvalid_set's l1: 0.00419069\n",
      "[9000]\tvalid_set's l1: 0.00416605\n",
      "[10000]\tvalid_set's l1: 0.00415032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t363.99s\t = Training   runtime\n",
      "\t2.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 230.61s of the 530.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00412601\n",
      "[2000]\tvalid_set's l1: 0.00404992\n",
      "[3000]\tvalid_set's l1: 0.0040444\n",
      "[1000]\tvalid_set's l1: 0.00419994\n",
      "[2000]\tvalid_set's l1: 0.00415827\n",
      "[3000]\tvalid_set's l1: 0.00414751\n",
      "[1000]\tvalid_set's l1: 0.00436727\n",
      "[2000]\tvalid_set's l1: 0.00426687\n",
      "[3000]\tvalid_set's l1: 0.00425386\n",
      "[4000]\tvalid_set's l1: 0.00424554\n",
      "[1000]\tvalid_set's l1: 0.00414138\n",
      "[2000]\tvalid_set's l1: 0.00404368\n",
      "[3000]\tvalid_set's l1: 0.00399897\n",
      "[4000]\tvalid_set's l1: 0.00399908\n",
      "[1000]\tvalid_set's l1: 0.00427519\n",
      "[2000]\tvalid_set's l1: 0.00422783\n",
      "[3000]\tvalid_set's l1: 0.00421783\n",
      "[4000]\tvalid_set's l1: 0.0042188\n",
      "[1000]\tvalid_set's l1: 0.00427314\n",
      "[2000]\tvalid_set's l1: 0.00418951\n",
      "[3000]\tvalid_set's l1: 0.00417945\n",
      "[1000]\tvalid_set's l1: 0.00439342\n",
      "[2000]\tvalid_set's l1: 0.00430359\n",
      "[3000]\tvalid_set's l1: 0.00428648\n",
      "[1000]\tvalid_set's l1: 0.00426344\n",
      "[2000]\tvalid_set's l1: 0.00418495\n",
      "[3000]\tvalid_set's l1: 0.00416015\n",
      "[4000]\tvalid_set's l1: 0.00415603\n",
      "[5000]\tvalid_set's l1: 0.00415085\n",
      "[6000]\tvalid_set's l1: 0.00415147\n",
      "[7000]\tvalid_set's l1: 0.00416308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t139.69s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 89.83s of the 389.84s of remaining time.\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 86.93s of the 386.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 2585.\n",
      "\tRan out of time, early stopping on iteration 2666.\n",
      "\tRan out of time, early stopping on iteration 2788.\n",
      "\tRan out of time, early stopping on iteration 2865.\n",
      "\tRan out of time, early stopping on iteration 2933.\n",
      "\tRan out of time, early stopping on iteration 3087.\n",
      "\tRan out of time, early stopping on iteration 3269.\n",
      "\tRan out of time, early stopping on iteration 3659.\n",
      "\t-0.0043\t = Validation score   (-mean_absolute_error)\n",
      "\t83.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3.5s of the 303.5s of remaining time.\n",
      "\t-0.0035\t = Validation score   (-mean_absolute_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2.01s of the 302.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1.39s of the 301.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0047\t = Validation score   (-mean_absolute_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.03s of the 300.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 299.39s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.857, 'ExtraTreesMSE_BAG_L1': 0.143}\n",
      "\t-0.0034\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 299.35s of the 299.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t18.78s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 280.46s of the 280.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t14.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 265.69s of the 265.66s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t5.96s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 258.94s of the 258.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t12.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 246.79s of the 246.76s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 244.8s of the 244.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t115.26s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 129.23s of the 129.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 122.54s of the 122.5s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 22)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 27)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-0.0032\t = Validation score   (-mean_absolute_error)\n",
      "\t117.11s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 5.06s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L2': 0.667, 'RandomForestMSE_BAG_L2': 0.167, 'NeuralNetFastAI_BAG_L2': 0.167}\n",
      "\t-0.0032\t = Validation score   (-mean_absolute_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 894.7s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 580.7 rows/s (1841 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241111_184001/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -0.003400  -0.003195  mean_absolute_error        5.233987       4.418585  829.684507                 0.001409                0.000383           0.110739            3       True         16\n",
      "1    NeuralNetTorch_BAG_L2      -0.003401  -0.003210  mean_absolute_error        4.664420       3.714951  708.353792                 0.493945                0.170814         117.108412            2       True         15\n",
      "2   NeuralNetFastAI_BAG_L2      -0.003439  -0.003270  mean_absolute_error        4.505722       3.742586  706.502748                 0.335247                0.198449         115.257368            2       True         13\n",
      "3        LightGBMXT_BAG_L2      -0.003447  -0.003308  mean_absolute_error        4.241526       3.583116  610.022477                 0.071051                0.038979          18.777096            2       True          8\n",
      "4          CatBoost_BAG_L2      -0.003467  -0.003295  mean_absolute_error        4.194334       3.553423  603.328870                 0.023859                0.009286          12.083489            2       True         11\n",
      "5           XGBoost_BAG_L2      -0.003479  -0.003315  mean_absolute_error        4.235769       3.563761  597.867673                 0.065294                0.019624           6.622292            2       True         14\n",
      "6          LightGBM_BAG_L2      -0.003493  -0.003309  mean_absolute_error        4.210707       3.562731  605.943523                 0.040232                0.018595          14.698143            2       True          9\n",
      "7     ExtraTreesMSE_BAG_L2      -0.003506  -0.003309  mean_absolute_error        4.386545       4.021751  592.484029                 0.216070                0.477614           1.238648            2       True         12\n",
      "8   RandomForestMSE_BAG_L2      -0.003528  -0.003322  mean_absolute_error        4.403386       4.048939  597.207989                 0.232911                0.504802           5.962608            2       True         10\n",
      "9   RandomForestMSE_BAG_L1      -0.003555  -0.003389  mean_absolute_error        0.186108       0.469262    2.144770                 0.186108                0.469262           2.144770            1       True          3\n",
      "10     WeightedEnsemble_L2      -0.003559  -0.003387  mean_absolute_error        0.434446       0.921702    2.922815                 0.001243                0.000375           0.034314            2       True          7\n",
      "11    ExtraTreesMSE_BAG_L1      -0.003663  -0.003461  mean_absolute_error        0.247095       0.452065    0.743731                 0.247095                0.452065           0.743731            1       True          5\n",
      "12         LightGBM_BAG_L1      -0.004060  -0.004155  mean_absolute_error        0.724753       0.486403  139.686430                 0.724753                0.486403         139.686430            1       True          2\n",
      "13       LightGBMXT_BAG_L1      -0.004224  -0.004203  mean_absolute_error        2.908972       2.100039  363.988972                 2.908972                2.100039         363.988972            1       True          1\n",
      "14         CatBoost_BAG_L1      -0.004244  -0.004280  mean_absolute_error        0.056927       0.019682   83.362241                 0.056927                0.019682          83.362241            1       True          4\n",
      "15          XGBoost_BAG_L1      -0.004644  -0.004676  mean_absolute_error        0.046619       0.016685    1.319237                 0.046619                0.016685           1.319237            1       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t901s\t = DyStack   runtime |\t2699s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2699s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241111_184001\"\n",
      "Train Data Rows:    16568\n",
      "Train Data Columns: 4\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14529.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 2 | ['temperature', 'wind_speed']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 2 | ['temperature', 'wind_speed']\n",
      "\t\t('int', ['bool'])            : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.90 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1798.91s of the 2699.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00482935\n",
      "[2000]\tvalid_set's l1: 0.00461036\n",
      "[3000]\tvalid_set's l1: 0.0044795\n",
      "[4000]\tvalid_set's l1: 0.00439591\n",
      "[5000]\tvalid_set's l1: 0.00433442\n",
      "[6000]\tvalid_set's l1: 0.00427695\n",
      "[7000]\tvalid_set's l1: 0.0042304\n",
      "[8000]\tvalid_set's l1: 0.00419964\n",
      "[9000]\tvalid_set's l1: 0.00417641\n",
      "[10000]\tvalid_set's l1: 0.00416157\n",
      "[1000]\tvalid_set's l1: 0.00492026\n",
      "[2000]\tvalid_set's l1: 0.00469514\n",
      "[3000]\tvalid_set's l1: 0.00457025\n",
      "[4000]\tvalid_set's l1: 0.00448159\n",
      "[5000]\tvalid_set's l1: 0.00442187\n",
      "[6000]\tvalid_set's l1: 0.00438088\n",
      "[7000]\tvalid_set's l1: 0.00434411\n",
      "[8000]\tvalid_set's l1: 0.00431559\n",
      "[9000]\tvalid_set's l1: 0.00429465\n",
      "[10000]\tvalid_set's l1: 0.00427878\n",
      "[1000]\tvalid_set's l1: 0.00481017\n",
      "[2000]\tvalid_set's l1: 0.0045881\n",
      "[3000]\tvalid_set's l1: 0.00445241\n",
      "[4000]\tvalid_set's l1: 0.00435743\n",
      "[5000]\tvalid_set's l1: 0.00429681\n",
      "[6000]\tvalid_set's l1: 0.0042522\n",
      "[7000]\tvalid_set's l1: 0.00422262\n",
      "[8000]\tvalid_set's l1: 0.00418963\n",
      "[9000]\tvalid_set's l1: 0.00416986\n",
      "[10000]\tvalid_set's l1: 0.00415255\n",
      "[1000]\tvalid_set's l1: 0.0047753\n",
      "[2000]\tvalid_set's l1: 0.00456979\n",
      "[3000]\tvalid_set's l1: 0.00442682\n",
      "[4000]\tvalid_set's l1: 0.00432849\n",
      "[5000]\tvalid_set's l1: 0.00425449\n",
      "[6000]\tvalid_set's l1: 0.00419242\n",
      "[7000]\tvalid_set's l1: 0.00414959\n",
      "[8000]\tvalid_set's l1: 0.00411466\n",
      "[9000]\tvalid_set's l1: 0.00408366\n",
      "[10000]\tvalid_set's l1: 0.00406319\n",
      "[1000]\tvalid_set's l1: 0.00481457\n",
      "[2000]\tvalid_set's l1: 0.00461216\n",
      "[3000]\tvalid_set's l1: 0.00449752\n",
      "[4000]\tvalid_set's l1: 0.00441626\n",
      "[5000]\tvalid_set's l1: 0.00436968\n",
      "[6000]\tvalid_set's l1: 0.00433559\n",
      "[7000]\tvalid_set's l1: 0.00430368\n",
      "[8000]\tvalid_set's l1: 0.0042789\n",
      "[9000]\tvalid_set's l1: 0.00425661\n",
      "[10000]\tvalid_set's l1: 0.00423445\n",
      "[1000]\tvalid_set's l1: 0.00472313\n",
      "[2000]\tvalid_set's l1: 0.00452333\n",
      "[3000]\tvalid_set's l1: 0.00441884\n",
      "[4000]\tvalid_set's l1: 0.0043493\n",
      "[5000]\tvalid_set's l1: 0.00429697\n",
      "[6000]\tvalid_set's l1: 0.00425032\n",
      "[7000]\tvalid_set's l1: 0.00421506\n",
      "[8000]\tvalid_set's l1: 0.00418948\n",
      "[9000]\tvalid_set's l1: 0.00416049\n",
      "[10000]\tvalid_set's l1: 0.00414504\n",
      "[1000]\tvalid_set's l1: 0.0048182\n",
      "[2000]\tvalid_set's l1: 0.00460003\n",
      "[3000]\tvalid_set's l1: 0.00447248\n",
      "[4000]\tvalid_set's l1: 0.00437966\n",
      "[5000]\tvalid_set's l1: 0.00431703\n",
      "[6000]\tvalid_set's l1: 0.00427295\n",
      "[7000]\tvalid_set's l1: 0.00423668\n",
      "[8000]\tvalid_set's l1: 0.0042147\n",
      "[9000]\tvalid_set's l1: 0.00419042\n",
      "[10000]\tvalid_set's l1: 0.00417519\n",
      "[1000]\tvalid_set's l1: 0.00465409\n",
      "[2000]\tvalid_set's l1: 0.00446622\n",
      "[3000]\tvalid_set's l1: 0.00434757\n",
      "[4000]\tvalid_set's l1: 0.00427019\n",
      "[5000]\tvalid_set's l1: 0.00420997\n",
      "[6000]\tvalid_set's l1: 0.00416627\n",
      "[7000]\tvalid_set's l1: 0.0041339\n",
      "[8000]\tvalid_set's l1: 0.00410988\n",
      "[9000]\tvalid_set's l1: 0.00408483\n",
      "[10000]\tvalid_set's l1: 0.00407341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0042\t = Validation score   (-mean_absolute_error)\n",
      "\t551.78s\t = Training   runtime\n",
      "\t2.72s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1242.41s of the 2142.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.00425082\n",
      "[2000]\tvalid_set's l1: 0.00415555\n",
      "[3000]\tvalid_set's l1: 0.00412293\n",
      "[4000]\tvalid_set's l1: 0.0041114\n",
      "[5000]\tvalid_set's l1: 0.00411928\n",
      "[1000]\tvalid_set's l1: 0.00425988\n",
      "[2000]\tvalid_set's l1: 0.00414088\n",
      "[3000]\tvalid_set's l1: 0.00409826\n",
      "[4000]\tvalid_set's l1: 0.00408468\n",
      "[5000]\tvalid_set's l1: 0.00408067\n",
      "[1000]\tvalid_set's l1: 0.00420067\n",
      "[2000]\tvalid_set's l1: 0.00411525\n",
      "[3000]\tvalid_set's l1: 0.00408929\n",
      "[1000]\tvalid_set's l1: 0.00415359\n",
      "[2000]\tvalid_set's l1: 0.00405715\n",
      "[3000]\tvalid_set's l1: 0.00404306\n",
      "[1000]\tvalid_set's l1: 0.00427508\n",
      "[2000]\tvalid_set's l1: 0.00423254\n",
      "[3000]\tvalid_set's l1: 0.00422948\n",
      "[1000]\tvalid_set's l1: 0.00421013\n",
      "[2000]\tvalid_set's l1: 0.00416374\n",
      "[3000]\tvalid_set's l1: 0.00414688\n",
      "[4000]\tvalid_set's l1: 0.00414682\n",
      "[1000]\tvalid_set's l1: 0.0041934\n",
      "[2000]\tvalid_set's l1: 0.00408812\n",
      "[3000]\tvalid_set's l1: 0.00406765\n",
      "[1000]\tvalid_set's l1: 0.00409936\n",
      "[2000]\tvalid_set's l1: 0.0040537\n",
      "[3000]\tvalid_set's l1: 0.00404045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0041\t = Validation score   (-mean_absolute_error)\n",
      "\t258.01s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 983.2s of the 1883.32s of remaining time.\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 980.11s of the 1880.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 5185.\n",
      "\tRan out of time, early stopping on iteration 3130.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 340.87s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 340.83s of the 340.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 336. Best iteration is:\n",
      "\t[226]\tvalid_set's l1: 0.00333657\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -1557.92s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-0.0033\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4257.17s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 29089.4 rows/s (16568 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241111_184001\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 4 features using 4142 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t6.79s\t= Expected runtime (1.36s per shuffle set)\n",
      "\t2.09s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             importance    stddev       p_value  n  p99_high   p99_low\n",
      "timestamp      0.003790  0.000075  1.841552e-08  5  0.003945  0.003636\n",
      "temperature    0.003285  0.000047  4.952707e-09  5  0.003381  0.003188\n",
      "area           0.002735  0.000068  4.589780e-08  5  0.002876  0.002595\n",
      "wind_speed     0.000635  0.000032  7.981253e-07  5  0.000702  0.000569\n"
     ]
    }
   ],
   "source": [
    "# AutogluonModels/ag-20241109_185606\n",
    "# predictor = TabularPredictor.load(\"AutogluonModels/ag-20241109_185606\")\n",
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "performance\n",
    "# reset index \n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': -0.0033329320087102667,\n",
       " 'root_mean_squared_error': -0.004850775099255721,\n",
       " 'mean_squared_error': -2.353001906355935e-05,\n",
       " 'r2': 0.7085213089279626,\n",
       " 'pearsonr': 0.8420261238965251,\n",
       " 'median_absolute_error': -0.00227640745880669}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  area  temperature  wind_speed\n",
       "0     2023-07-01 00:00:00  1199         13.6         1.6\n",
       "1     2023-07-01 01:00:00  1199         13.2         2.0\n",
       "2     2023-07-01 02:00:00  1199         12.3         1.6\n",
       "3     2023-07-01 03:00:00  1199         11.9         0.6\n",
       "4     2023-07-01 04:00:00  1199         11.9         0.2\n",
       "...                   ...   ...          ...         ...\n",
       "10338 2024-09-03 18:00:00  1199         17.8         2.5\n",
       "10339 2024-09-03 19:00:00  1199         17.8         1.8\n",
       "10340 2024-09-03 20:00:00  1199         17.7         1.1\n",
       "10341 2024-09-03 21:00:00  1199         18.0         3.4\n",
       "10342 2024-09-03 22:00:00  1199         17.9         3.0\n",
       "\n",
       "[10343 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "data_predict = main_building[features]\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / 'prediction_main_b.csv'\n",
    "# create folder\n",
    "my_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
