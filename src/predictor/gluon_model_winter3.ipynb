{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matsalexander/Desktop/SolarEnergyImpact\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path().resolve().absolute().parent.parent\n",
    "print(root)\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from src.pipeline import Pipeline, BuilingIdsEnum\n",
    "pipe = Pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_b = pipe.get_data(BuilingIdsEnum.B)\n",
    "building_c = pipe.get_data(BuilingIdsEnum.C)\n",
    "building_a = pipe.get_data(BuilingIdsEnum.A)\n",
    "\n",
    "# from november 1. to 1. mars only use data from dataset b \n",
    "# else use data from dataset c and a\n",
    "\n",
    "# remove from 2024-01-15 from dataset b\n",
    "mask_b = (building_b['timestamp'] >= '2024-01-15') & (building_b['timestamp'] <= '2024-01-15')\n",
    "building_b = building_b[~mask_b]\n",
    "\n",
    "start_winter = '2023-11-01'\n",
    "end_winter = '2024-03-01'\n",
    "# mask_b = (building_b['timestamp'] >= start_winter) & (building_b['timestamp'] <= end_winter)\n",
    "# building_b = building_b[mask_b]\n",
    "\n",
    "mask_c = (building_c['timestamp'] < start_winter) | (building_c['timestamp'] > end_winter)\n",
    "building_c = building_c[mask_c]\n",
    "\n",
    "\n",
    "# building a, not in winter\n",
    "# remove anomoloes\n",
    "\n",
    "# mask_a = (building_a['timestamp'] < start_winter) | (building_a['timestamp'] > end_winter)\n",
    "# building_a = building_a[mask_a]\n",
    "# mask_a2 = (building_a['timestamp'] >= '2024-07-03') & (building_a['timestamp'] <= '2024-07-07')\n",
    "# building_a = building_a[~mask_a2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df = pd.concat([building_b, building_c], ignore_index=True)\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# Perform the train-test split with stratification based on 'building_id'\n",
    "train_data, test_data = train_test_split(\n",
    "    combined_df,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=combined_df['building']\n",
    ")\n",
    "\n",
    "# select features\n",
    "target = \"value_import\"\n",
    "features = [\"timestamp\", \"temperature\", \"area\", \"wind_speed\"] #, \"cloud_fraction\", \"precipitation\", \"area\", \"wind_speed\"\n",
    "\n",
    "train_data = train_data[features + [target]]\n",
    "test_data = test_data[features + [target]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241114_133540\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       13.42 GB / 32.00 GB (41.9%)\n",
      "Disk Space Avail:   601.03 GB / 926.35 GB (64.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.31.0 detected. 2.10.0 <= ray < 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"AutogluonModels/ag-20241114_133540/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241114_133540/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    12677\n",
      "Train Data Columns: 3\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    13746.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.29 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', ['bool'])            : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.59 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.82s of the 899.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.38456\n",
      "[2000]\tvalid_set's l1: 5.0647\n",
      "[3000]\tvalid_set's l1: 4.90083\n",
      "[4000]\tvalid_set's l1: 4.80739\n",
      "[5000]\tvalid_set's l1: 4.7437\n",
      "[6000]\tvalid_set's l1: 4.69949\n",
      "[7000]\tvalid_set's l1: 4.65688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 7467. Best iteration is:\n",
      "\t[7467]\tvalid_set's l1: 4.64607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.4868\n",
      "[2000]\tvalid_set's l1: 5.19035\n",
      "[3000]\tvalid_set's l1: 5.02765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3696. Best iteration is:\n",
      "\t[3691]\tvalid_set's l1: 4.95082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.17339\n",
      "[2000]\tvalid_set's l1: 4.8711\n",
      "[3000]\tvalid_set's l1: 4.71456\n",
      "[4000]\tvalid_set's l1: 4.60654\n",
      "[5000]\tvalid_set's l1: 4.53536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5665. Best iteration is:\n",
      "\t[5665]\tvalid_set's l1: 4.49885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.363\n",
      "[2000]\tvalid_set's l1: 5.09132\n",
      "[3000]\tvalid_set's l1: 4.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3933. Best iteration is:\n",
      "\t[3903]\tvalid_set's l1: 4.84749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.29751\n",
      "[2000]\tvalid_set's l1: 4.98568\n",
      "[3000]\tvalid_set's l1: 4.82844\n",
      "[4000]\tvalid_set's l1: 4.72533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4317. Best iteration is:\n",
      "\t[4313]\tvalid_set's l1: 4.70314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.43035\n",
      "[2000]\tvalid_set's l1: 5.16848\n",
      "[3000]\tvalid_set's l1: 5.01584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3735. Best iteration is:\n",
      "\t[3709]\tvalid_set's l1: 4.93793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.34135\n",
      "[2000]\tvalid_set's l1: 5.03168\n",
      "[3000]\tvalid_set's l1: 4.85528\n",
      "[4000]\tvalid_set's l1: 4.74613\n",
      "[5000]\tvalid_set's l1: 4.661\n",
      "[6000]\tvalid_set's l1: 4.58617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 6577. Best iteration is:\n",
      "\t[6547]\tvalid_set's l1: 4.56101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.33793\n",
      "[2000]\tvalid_set's l1: 5.0584\n",
      "[3000]\tvalid_set's l1: 4.90484\n",
      "[4000]\tvalid_set's l1: 4.81947\n",
      "[5000]\tvalid_set's l1: 4.75287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 5796. Best iteration is:\n",
      "\t[5796]\tvalid_set's l1: 4.70822\n",
      "\t-4.7317\t = Validation score   (-mean_absolute_error)\n",
      "\t573.58s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 23.46s of the 323.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 133. Best iteration is:\n",
      "\t[133]\tvalid_set's l1: 4.99318\n",
      "\tRan out of time, early stopping on iteration 141. Best iteration is:\n",
      "\t[141]\tvalid_set's l1: 5.12245\n",
      "\tRan out of time, early stopping on iteration 140. Best iteration is:\n",
      "\t[140]\tvalid_set's l1: 4.76073\n",
      "\tRan out of time, early stopping on iteration 155. Best iteration is:\n",
      "\t[155]\tvalid_set's l1: 5.02356\n",
      "\tRan out of time, early stopping on iteration 161. Best iteration is:\n",
      "\t[161]\tvalid_set's l1: 4.829\n",
      "\tRan out of time, early stopping on iteration 174. Best iteration is:\n",
      "\t[174]\tvalid_set's l1: 5.01546\n",
      "\tRan out of time, early stopping on iteration 161. Best iteration is:\n",
      "\t[161]\tvalid_set's l1: 4.92853\n",
      "\tRan out of time, early stopping on iteration 238. Best iteration is:\n",
      "\t[238]\tvalid_set's l1: 4.91048\n",
      "\t-4.9479\t = Validation score   (-mean_absolute_error)\n",
      "\t22.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.88s of the 301.01s of remaining time.\n",
      "\t-3.5015\t = Validation score   (-mean_absolute_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 297.94s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.5015\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 297.91s of the 297.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5661\t = Validation score   (-mean_absolute_error)\n",
      "\t69.47s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 228.27s of the 228.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5179\t = Validation score   (-mean_absolute_error)\n",
      "\t39.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 189.01s of the 188.99s of remaining time.\n",
      "\t-3.5675\t = Validation score   (-mean_absolute_error)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 182.59s of the 182.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5093\t = Validation score   (-mean_absolute_error)\n",
      "\t11.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 170.76s of the 170.74s of remaining time.\n",
      "\t-3.5618\t = Validation score   (-mean_absolute_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 168.56s of the 168.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 10)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.5344\t = Validation score   (-mean_absolute_error)\n",
      "\t155.81s\t = Training   runtime\n",
      "\t0.86s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 11.79s of the 11.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.5291\t = Validation score   (-mean_absolute_error)\n",
      "\t10.74s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 0.81s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 0.4, 'RandomForestMSE_BAG_L2': 0.2, 'CatBoost_BAG_L2': 0.2, 'NeuralNetFastAI_BAG_L2': 0.12, 'LightGBM_BAG_L2': 0.04, 'ExtraTreesMSE_BAG_L2': 0.04}\n",
      "\t-3.4825\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 899.32s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 662.4 rows/s (1585 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241114_133540/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -3.451634  -3.482474  mean_absolute_error        3.232146       3.594736  811.970298                 0.001625                0.000576           0.117078            3       True         12\n",
      "1          LightGBM_BAG_L2      -3.467437  -3.517851  mean_absolute_error        2.086077       1.792513  637.405023                 0.038998                0.023474          39.182151            2       True          6\n",
      "2           XGBoost_BAG_L2      -3.469793  -3.529147  mean_absolute_error        2.129477       1.802482  608.962579                 0.082398                0.033442          10.739707            2       True         11\n",
      "3          CatBoost_BAG_L2      -3.470899  -3.509335  mean_absolute_error        2.077125       1.777651  610.006773                 0.030046                0.008612          11.783901            2       True          8\n",
      "4   RandomForestMSE_BAG_L1      -3.476216  -3.501540  mean_absolute_error        0.210616       0.443014    2.174470                 0.210616                0.443014           2.174470            1       True          3\n",
      "5      WeightedEnsemble_L2      -3.476216  -3.501540  mean_absolute_error        0.213067       0.443436    2.196827                 0.002451                0.000422           0.022357            2       True          4\n",
      "6   NeuralNetFastAI_BAG_L2      -3.497149  -3.534388  mean_absolute_error        2.664969       2.631933  754.028980                 0.617890                0.862893         155.806108            2       True         10\n",
      "7        LightGBMXT_BAG_L2      -3.506212  -3.566081  mean_absolute_error        2.184855       1.827860  667.695117                 0.137776                0.058821          69.472245            2       True          5\n",
      "8   RandomForestMSE_BAG_L2      -3.520289  -3.567512  mean_absolute_error        2.305003       2.252507  603.745862                 0.257924                0.483467           5.522990            2       True          7\n",
      "9     ExtraTreesMSE_BAG_L2      -3.550593  -3.561808  mean_absolute_error        2.285662       2.215714  599.558069                 0.238583                0.446675           1.335197            2       True          9\n",
      "10       LightGBMXT_BAG_L1      -4.831251  -4.731689  mean_absolute_error        1.742234       1.284487  573.580657                 1.742234                1.284487         573.580657            1       True          1\n",
      "11         LightGBM_BAG_L1      -4.972496  -4.947924  mean_absolute_error        0.094229       0.041539   22.467745                 0.094229                0.041539          22.467745            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t903s\t = DyStack   runtime |\t2697s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2697s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20241114_133540\"\n",
      "Train Data Rows:    14262\n",
      "Train Data Columns: 3\n",
      "Label Column:       value_import\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    14435.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.33 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('datetime', []) : 1 | ['timestamp']\n",
      "\t\t('float', [])    : 1 | ['temperature']\n",
      "\t\t('int', [])      : 1 | ['area']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 1 | ['temperature']\n",
      "\t\t('int', ['bool'])            : 1 | ['area']\n",
      "\t\t('int', ['datetime_as_int']) : 5 | ['timestamp', 'timestamp.year', 'timestamp.month', 'timestamp.day', 'timestamp.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t3 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1797.61s of the 2697.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 5.40836\n",
      "[2000]\tvalid_set's l1: 5.08523\n",
      "[3000]\tvalid_set's l1: 4.89805\n",
      "[4000]\tvalid_set's l1: 4.78217\n",
      "[5000]\tvalid_set's l1: 4.69109\n",
      "[6000]\tvalid_set's l1: 4.63084\n",
      "[7000]\tvalid_set's l1: 4.5825\n",
      "[8000]\tvalid_set's l1: 4.54605\n",
      "[9000]\tvalid_set's l1: 4.51343\n",
      "[10000]\tvalid_set's l1: 4.48813\n",
      "[1000]\tvalid_set's l1: 5.41999\n",
      "[2000]\tvalid_set's l1: 5.15527\n",
      "[3000]\tvalid_set's l1: 5.00131\n",
      "[4000]\tvalid_set's l1: 4.89132\n",
      "[5000]\tvalid_set's l1: 4.82107\n",
      "[6000]\tvalid_set's l1: 4.76444\n",
      "[7000]\tvalid_set's l1: 4.71779\n",
      "[8000]\tvalid_set's l1: 4.68678\n",
      "[9000]\tvalid_set's l1: 4.65724\n",
      "[10000]\tvalid_set's l1: 4.63145\n",
      "[1000]\tvalid_set's l1: 5.49166\n",
      "[2000]\tvalid_set's l1: 5.17814\n",
      "[3000]\tvalid_set's l1: 4.99278\n",
      "[4000]\tvalid_set's l1: 4.87591\n",
      "[5000]\tvalid_set's l1: 4.79311\n",
      "[6000]\tvalid_set's l1: 4.72692\n",
      "[7000]\tvalid_set's l1: 4.6699\n",
      "[8000]\tvalid_set's l1: 4.62315\n",
      "[9000]\tvalid_set's l1: 4.58264\n",
      "[10000]\tvalid_set's l1: 4.55041\n",
      "[1000]\tvalid_set's l1: 5.38948\n",
      "[2000]\tvalid_set's l1: 5.08126\n",
      "[3000]\tvalid_set's l1: 4.90339\n",
      "[4000]\tvalid_set's l1: 4.80284\n",
      "[5000]\tvalid_set's l1: 4.72946\n",
      "[6000]\tvalid_set's l1: 4.66742\n",
      "[7000]\tvalid_set's l1: 4.62201\n",
      "[8000]\tvalid_set's l1: 4.58625\n",
      "[9000]\tvalid_set's l1: 4.55768\n",
      "[10000]\tvalid_set's l1: 4.53344\n",
      "[1000]\tvalid_set's l1: 5.48275\n",
      "[2000]\tvalid_set's l1: 5.18176\n",
      "[3000]\tvalid_set's l1: 5.02408\n",
      "[4000]\tvalid_set's l1: 4.9063\n",
      "[5000]\tvalid_set's l1: 4.83286\n",
      "[6000]\tvalid_set's l1: 4.77434\n",
      "[7000]\tvalid_set's l1: 4.73012\n",
      "[8000]\tvalid_set's l1: 4.69303\n",
      "[9000]\tvalid_set's l1: 4.66347\n",
      "[10000]\tvalid_set's l1: 4.63423\n",
      "[1000]\tvalid_set's l1: 5.1538\n",
      "[2000]\tvalid_set's l1: 4.86649\n",
      "[3000]\tvalid_set's l1: 4.72661\n",
      "[4000]\tvalid_set's l1: 4.63279\n",
      "[5000]\tvalid_set's l1: 4.56075\n",
      "[6000]\tvalid_set's l1: 4.50396\n",
      "[7000]\tvalid_set's l1: 4.4574\n",
      "[8000]\tvalid_set's l1: 4.42343\n",
      "[9000]\tvalid_set's l1: 4.39283\n",
      "[10000]\tvalid_set's l1: 4.36236\n",
      "[1000]\tvalid_set's l1: 5.14132\n",
      "[2000]\tvalid_set's l1: 4.8506\n",
      "[3000]\tvalid_set's l1: 4.68326\n",
      "[4000]\tvalid_set's l1: 4.57307\n",
      "[5000]\tvalid_set's l1: 4.49382\n",
      "[6000]\tvalid_set's l1: 4.43378\n",
      "[7000]\tvalid_set's l1: 4.39125\n",
      "[8000]\tvalid_set's l1: 4.35361\n",
      "[9000]\tvalid_set's l1: 4.32372\n",
      "[10000]\tvalid_set's l1: 4.30161\n",
      "[1000]\tvalid_set's l1: 5.36643\n",
      "[2000]\tvalid_set's l1: 5.05405\n",
      "[3000]\tvalid_set's l1: 4.89572\n",
      "[4000]\tvalid_set's l1: 4.80307\n",
      "[5000]\tvalid_set's l1: 4.73916\n",
      "[6000]\tvalid_set's l1: 4.6911\n",
      "[7000]\tvalid_set's l1: 4.64846\n",
      "[8000]\tvalid_set's l1: 4.61923\n",
      "[9000]\tvalid_set's l1: 4.59319\n",
      "[10000]\tvalid_set's l1: 4.57095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.5089\t = Validation score   (-mean_absolute_error)\n",
      "\t987.87s\t = Training   runtime\n",
      "\t3.43s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 803.36s of the 1702.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.55837\n",
      "[2000]\tvalid_set's l1: 4.49056\n",
      "[3000]\tvalid_set's l1: 4.47374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3268. Best iteration is:\n",
      "\t[3266]\tvalid_set's l1: 4.46944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.57555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1254. Best iteration is:\n",
      "\t[1248]\tvalid_set's l1: 4.52928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.53827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2014. Best iteration is:\n",
      "\t[2012]\tvalid_set's l1: 4.43932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l1: 4.44002\n",
      "[1000]\tvalid_set's l1: 4.49358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1347. Best iteration is:\n",
      "\t[1313]\tvalid_set's l1: 4.46281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.53619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1665. Best iteration is:\n",
      "\t[1665]\tvalid_set's l1: 4.41673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.22182\n",
      "[2000]\tvalid_set's l1: 4.14622\n",
      "[3000]\tvalid_set's l1: 4.13491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3014. Best iteration is:\n",
      "\t[2876]\tvalid_set's l1: 4.13082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.25889\n",
      "[2000]\tvalid_set's l1: 4.19812\n",
      "[3000]\tvalid_set's l1: 4.1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3371. Best iteration is:\n",
      "\t[2608]\tvalid_set's l1: 4.18882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 4.53271\n",
      "[2000]\tvalid_set's l1: 4.44832\n",
      "[3000]\tvalid_set's l1: 4.43915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-4.3841\t = Validation score   (-mean_absolute_error)\n",
      "\t697.79s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 104.03s of the 1003.51s of remaining time.\n",
      "\t-3.3765\t = Validation score   (-mean_absolute_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 101.78s of the 1001.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tRan out of time, early stopping on iteration 2964.\n",
      "\tRan out of time, early stopping on iteration 2975.\n",
      "\tRan out of time, early stopping on iteration 3097.\n",
      "\tRan out of time, early stopping on iteration 3181.\n",
      "\tRan out of time, early stopping on iteration 3270.\n",
      "\tRan out of time, early stopping on iteration 3397.\n",
      "\tRan out of time, early stopping on iteration 3557.\n",
      "\tRan out of time, early stopping on iteration 3957.\n",
      "\t-4.4912\t = Validation score   (-mean_absolute_error)\n",
      "\t97.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 4.09s of the 903.57s of remaining time.\n",
      "\t-3.5639\t = Validation score   (-mean_absolute_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2.55s of the 902.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1.83s of the 901.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-4.9152\t = Validation score   (-mean_absolute_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.04s of the 899.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 899.32s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L1': 1.0}\n",
      "\t-3.3765\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 899.28s of the 899.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3848\t = Validation score   (-mean_absolute_error)\n",
      "\t27.51s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 871.56s of the 871.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3465\t = Validation score   (-mean_absolute_error)\n",
      "\t16.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 854.67s of the 854.63s of remaining time.\n",
      "\t-3.3539\t = Validation score   (-mean_absolute_error)\n",
      "\t5.07s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 848.75s of the 848.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3528\t = Validation score   (-mean_absolute_error)\n",
      "\t12.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 836.27s of the 836.22s of remaining time.\n",
      "\t-3.3372\t = Validation score   (-mean_absolute_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 834.17s of the 834.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\t-3.3198\t = Validation score   (-mean_absolute_error)\n",
      "\t143.86s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 689.95s of the 689.9s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3609\t = Validation score   (-mean_absolute_error)\n",
      "\t6.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 683.41s of the 683.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 67)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 70)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 82)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.2616\t = Validation score   (-mean_absolute_error)\n",
      "\t437.3s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 245.68s of the 245.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3702\t = Validation score   (-mean_absolute_error)\n",
      "\t57.58s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 187.87s of the 187.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-3.3456\t = Validation score   (-mean_absolute_error)\n",
      "\t11.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 176.2s of the 176.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 21)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 33)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1623: FutureWarning: \n",
      "The format of the columns of the 'remainder' transformer in ColumnTransformer.transformers_ will change in version 1.7 to match the format of the other transformers.\n",
      "At the moment the remainder columns are stored as indices (of type int). With the same ColumnTransformer configuration, in the future they will be stored as column names (of type str).\n",
      "To use the new behavior now and suppress this warning, use ColumnTransformer(force_int_remainder_cols=False).\n",
      "\n",
      "  warnings.warn(\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 26)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ag/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(net_filename)\n",
      "\t-3.3033\t = Validation score   (-mean_absolute_error)\n",
      "\t168.09s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 7.64s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L2': 0.571, 'NeuralNetTorch_r79_BAG_L2': 0.19, 'RandomForestMSE_BAG_L2': 0.143, 'ExtraTreesMSE_BAG_L2': 0.095}\n",
      "\t-3.2387\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2689.59s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 347.7 rows/s (1783 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241114_133540\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model using AutoGluon\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(\n",
    "    train_data, \n",
    "    presets='best_quality',\n",
    "    excluded_model_types=['KNN']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 3 features using 3566 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t150.27s\t= Expected runtime (30.05s per shuffle set)\n",
      "\t99.19s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             importance    stddev       p_value  n  p99_high   p99_low\n",
      "area           5.904166  0.200952  1.607860e-07  5  6.317929  5.490402\n",
      "timestamp      5.426094  0.050479  8.987353e-10  5  5.530031  5.322156\n",
      "temperature    5.267191  0.104772  1.877635e-08  5  5.482917  5.051464\n"
     ]
    }
   ],
   "source": [
    "# winter3 first predictor = TabularPredictor.load(\"AutogluonModels/ag-20241113_235901\")\n",
    "# Evaluate on test data\n",
    "performance = predictor.evaluate(test_data)\n",
    "# best model: ag-20241022_161331\n",
    "\n",
    "print(\"Evaluation Performance:\")\n",
    "performance\n",
    "# reset index \n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# To see feature importance\n",
    "global_importance = predictor.feature_importance(test_data)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)  # Shows which features had the most impact on model predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': -3.2692989379906345,\n",
       " 'root_mean_squared_error': -4.7716187976610325,\n",
       " 'mean_squared_error': -22.768345950192113,\n",
       " 'r2': 0.8600666500763289,\n",
       " 'pearsonr': 0.9274823410190682,\n",
       " 'median_absolute_error': -2.0179219055175786}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>temperature</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>13.6</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>11.9</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>17.8</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>17.9</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10343 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  temperature  area\n",
       "0     2023-07-01 00:00:00         13.6  1199\n",
       "1     2023-07-01 01:00:00         13.2  1199\n",
       "2     2023-07-01 02:00:00         12.3  1199\n",
       "3     2023-07-01 03:00:00         11.9  1199\n",
       "4     2023-07-01 04:00:00         11.9  1199\n",
       "...                   ...          ...   ...\n",
       "10338 2024-09-03 18:00:00         17.8  1199\n",
       "10339 2024-09-03 19:00:00         17.8  1199\n",
       "10340 2024-09-03 20:00:00         17.7  1199\n",
       "10341 2024-09-03 21:00:00         18.0  1199\n",
       "10342 2024-09-03 22:00:00         17.9  1199\n",
       "\n",
       "[10343 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model location => AutogluonModels/ag-20241016_095906\n",
    "main_building = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "data_predict = main_building[features]\n",
    "data_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton1 = predictor.predict(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predicitons as a csv in data folder from root.\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prediciton1_df = pd.DataFrame(prediciton1)\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / 'prediction_winter3.csv'\n",
    "if my_path.exists():\n",
    "    my_path = Path().resolve().parent.parent / 'data'/ \"pred\" / f'prediction_winter3_{date_time}.csv'\n",
    "# create folder\n",
    "my_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "prediciton1_df.to_csv(my_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMAPE: 11.02%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Reset index if needed\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Compute SMAPE witch means Symmetric Mean Absolute Percentage Error\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    smape_values = np.where(denominator != 0, diff / denominator, 0)\n",
    "    return np.mean(smape_values) * 100\n",
    "\n",
    "# Replace 'target_column' with your actual target column name\n",
    "y_true = test_data[target]\n",
    "y_pred = predictor.predict(test_data)\n",
    "smape_value = smape(y_true, y_pred)\n",
    "print(f\"\\nSMAPE: {smape_value:.2f}%\")\n",
    "\n",
    "# Analyze percent-wise errors\n",
    "percentage_errors = np.abs((y_true - y_pred) / y_true) * 100\n",
    "percentage_errors = np.where(y_true != 0, percentage_errors, 0)\n",
    "\n",
    "mean_percentage_error = np.mean(percentage_errors)\n",
    "median_percentage_error = np.median(percentage_errors)\n",
    "max_percentage_error = np.max(percentage_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percentage Error: 13.59%\n",
      "Median Percentage Error: 5.92%\n",
      "Max Percentage Error: 5874.37%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(f\"Mean Percentage Error: {mean_percentage_error:.2f}%\")\n",
    "print(f\"Median Percentage Error: {median_percentage_error:.2f}%\")\n",
    "print(f\"Max Percentage Error: {max_percentage_error:.2f}%\")\n",
    "percentage_errors_trimmed = percentage_errors[percentage_errors < 100]\n",
    "# Plot percentage error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(percentage_errors_trimmed, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Percentage Error (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Percentage Errors')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "# global_importance = predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "             importance    stddev       p_value  n  p99_high   p99_low\n",
      "area           5.904166  0.200952  1.607860e-07  5  6.317929  5.490402\n",
      "timestamp      5.426094  0.050479  8.987353e-10  5  5.530031  5.322156\n",
      "temperature    5.267191  0.104772  1.877635e-08  5  5.482917  5.051464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nFeature Importance:\")\n",
    "print(global_importance)\n",
    "\n",
    "# Plot top 20 feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "# remove the n from the golbal_importance plot\n",
    "global_importance.columns\n",
    "# Index(['importance'\t'stddev'\t'p_value'\t'n'\t'p99_high'\t'p99_low']\tdtype='object')\n",
    "\n",
    "if 'n' in global_importance.columns:\n",
    "    global_importance = global_importance.drop(columns=['n'])\n",
    "global_importance.head(20).plot(kind='barh')\n",
    "\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 3 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Optional: SHAP Values for advanced insights\n",
    "# explainer = predictor.explain(test_data, model='best')\n",
    "# explainer.plot_feature_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>predicted_consumption</th>\n",
       "      <th>value_export</th>\n",
       "      <th>solar_consumption</th>\n",
       "      <th>net_consumption</th>\n",
       "      <th>net_consumption_per_sqm</th>\n",
       "      <th>predicted_consumption</th>\n",
       "      <th>building</th>\n",
       "      <th>property_id</th>\n",
       "      <th>area</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>cloud_fraction</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>spot_price_nok</th>\n",
       "      <th>value_import_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-02</td>\n",
       "      <td>903.50</td>\n",
       "      <td>178.00</td>\n",
       "      <td>955.427660</td>\n",
       "      <td>1680.927660</td>\n",
       "      <td>1.401941</td>\n",
       "      <td>1757.470821</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.239583</td>\n",
       "      <td>2.479167</td>\n",
       "      <td>251.500000</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.189583</td>\n",
       "      <td>1.137242</td>\n",
       "      <td>25.746155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-09</td>\n",
       "      <td>4324.30</td>\n",
       "      <td>220.10</td>\n",
       "      <td>2663.311274</td>\n",
       "      <td>6767.511274</td>\n",
       "      <td>5.644296</td>\n",
       "      <td>7397.542175</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.372024</td>\n",
       "      <td>2.097619</td>\n",
       "      <td>96.300000</td>\n",
       "      <td>0.583929</td>\n",
       "      <td>0.150595</td>\n",
       "      <td>0.734471</td>\n",
       "      <td>25.609608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-16</td>\n",
       "      <td>5008.30</td>\n",
       "      <td>66.70</td>\n",
       "      <td>1734.309332</td>\n",
       "      <td>6675.909332</td>\n",
       "      <td>5.567898</td>\n",
       "      <td>7472.545071</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.825595</td>\n",
       "      <td>2.894643</td>\n",
       "      <td>63.200000</td>\n",
       "      <td>0.738690</td>\n",
       "      <td>0.180952</td>\n",
       "      <td>1.041394</td>\n",
       "      <td>27.073210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-23</td>\n",
       "      <td>4155.10</td>\n",
       "      <td>169.10</td>\n",
       "      <td>2656.076572</td>\n",
       "      <td>6642.076572</td>\n",
       "      <td>5.539680</td>\n",
       "      <td>6819.156256</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.225000</td>\n",
       "      <td>2.585119</td>\n",
       "      <td>201.500000</td>\n",
       "      <td>0.573810</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.496056</td>\n",
       "      <td>27.110697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>4563.50</td>\n",
       "      <td>219.40</td>\n",
       "      <td>2299.114591</td>\n",
       "      <td>6643.214591</td>\n",
       "      <td>5.540629</td>\n",
       "      <td>6869.377289</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.952381</td>\n",
       "      <td>2.278571</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.583929</td>\n",
       "      <td>0.111905</td>\n",
       "      <td>0.674216</td>\n",
       "      <td>27.063702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-08-06</td>\n",
       "      <td>4547.50</td>\n",
       "      <td>70.40</td>\n",
       "      <td>2227.371748</td>\n",
       "      <td>6704.471748</td>\n",
       "      <td>5.591720</td>\n",
       "      <td>7471.431577</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.923810</td>\n",
       "      <td>1.627976</td>\n",
       "      <td>160.600000</td>\n",
       "      <td>0.605952</td>\n",
       "      <td>0.085119</td>\n",
       "      <td>0.664450</td>\n",
       "      <td>26.109138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-08-13</td>\n",
       "      <td>5090.10</td>\n",
       "      <td>52.20</td>\n",
       "      <td>1571.916839</td>\n",
       "      <td>6609.816839</td>\n",
       "      <td>5.512775</td>\n",
       "      <td>7141.931791</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.998810</td>\n",
       "      <td>2.424405</td>\n",
       "      <td>313.400000</td>\n",
       "      <td>0.749405</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.588623</td>\n",
       "      <td>28.459934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-20</td>\n",
       "      <td>5411.10</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1423.202317</td>\n",
       "      <td>6797.802317</td>\n",
       "      <td>5.669560</td>\n",
       "      <td>8146.789063</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.184524</td>\n",
       "      <td>2.311310</td>\n",
       "      <td>183.200000</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>0.867745</td>\n",
       "      <td>29.813784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-08-27</td>\n",
       "      <td>4992.00</td>\n",
       "      <td>57.40</td>\n",
       "      <td>1747.366035</td>\n",
       "      <td>6681.966035</td>\n",
       "      <td>5.572949</td>\n",
       "      <td>7195.936084</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>1.788095</td>\n",
       "      <td>153.100000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.067262</td>\n",
       "      <td>0.847740</td>\n",
       "      <td>27.845287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-09-03</td>\n",
       "      <td>4719.90</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1952.757229</td>\n",
       "      <td>6667.157229</td>\n",
       "      <td>5.560598</td>\n",
       "      <td>6610.651114</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.094643</td>\n",
       "      <td>1.406548</td>\n",
       "      <td>122.100000</td>\n",
       "      <td>0.472619</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.909783</td>\n",
       "      <td>23.036877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-09-10</td>\n",
       "      <td>5282.20</td>\n",
       "      <td>32.00</td>\n",
       "      <td>1456.647447</td>\n",
       "      <td>6706.847447</td>\n",
       "      <td>5.593701</td>\n",
       "      <td>7533.559487</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.698214</td>\n",
       "      <td>1.695238</td>\n",
       "      <td>138.400000</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862716</td>\n",
       "      <td>20.361374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-09-17</td>\n",
       "      <td>5314.30</td>\n",
       "      <td>34.10</td>\n",
       "      <td>1275.001363</td>\n",
       "      <td>6555.201363</td>\n",
       "      <td>5.467224</td>\n",
       "      <td>6693.579625</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>14.032738</td>\n",
       "      <td>2.158333</td>\n",
       "      <td>75.300000</td>\n",
       "      <td>0.663095</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.858647</td>\n",
       "      <td>20.103357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-09-24</td>\n",
       "      <td>5549.69</td>\n",
       "      <td>1.40</td>\n",
       "      <td>902.325838</td>\n",
       "      <td>6450.615838</td>\n",
       "      <td>5.379997</td>\n",
       "      <td>6458.413331</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.429762</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>60.200000</td>\n",
       "      <td>0.729762</td>\n",
       "      <td>0.389881</td>\n",
       "      <td>0.114518</td>\n",
       "      <td>19.496626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>5554.90</td>\n",
       "      <td>16.90</td>\n",
       "      <td>868.472727</td>\n",
       "      <td>6406.472727</td>\n",
       "      <td>5.343180</td>\n",
       "      <td>6644.253128</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.899405</td>\n",
       "      <td>2.689881</td>\n",
       "      <td>187.100000</td>\n",
       "      <td>0.668452</td>\n",
       "      <td>0.048810</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>20.276810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-10-08</td>\n",
       "      <td>5267.30</td>\n",
       "      <td>11.40</td>\n",
       "      <td>1024.092877</td>\n",
       "      <td>6279.992877</td>\n",
       "      <td>5.237692</td>\n",
       "      <td>6139.315823</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>9.736905</td>\n",
       "      <td>1.891667</td>\n",
       "      <td>167.300000</td>\n",
       "      <td>0.607738</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.522057</td>\n",
       "      <td>21.433884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-10-15</td>\n",
       "      <td>5528.30</td>\n",
       "      <td>2.70</td>\n",
       "      <td>747.527215</td>\n",
       "      <td>6273.127215</td>\n",
       "      <td>5.231966</td>\n",
       "      <td>6077.512462</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>7.922024</td>\n",
       "      <td>3.042857</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.098810</td>\n",
       "      <td>0.570979</td>\n",
       "      <td>19.626102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-10-22</td>\n",
       "      <td>5689.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>536.634522</td>\n",
       "      <td>6226.334522</td>\n",
       "      <td>5.192940</td>\n",
       "      <td>5971.146013</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>4.011310</td>\n",
       "      <td>314.700000</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.468074</td>\n",
       "      <td>24.340889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>5766.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>266.524990</td>\n",
       "      <td>6033.254990</td>\n",
       "      <td>5.031906</td>\n",
       "      <td>5889.763742</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.448214</td>\n",
       "      <td>3.420833</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>28.238678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>6217.71</td>\n",
       "      <td>0.90</td>\n",
       "      <td>52.804613</td>\n",
       "      <td>6269.614613</td>\n",
       "      <td>5.229036</td>\n",
       "      <td>5893.136672</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>2.598810</td>\n",
       "      <td>4.973214</td>\n",
       "      <td>30.700000</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.298214</td>\n",
       "      <td>0.219281</td>\n",
       "      <td>29.976297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>6116.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>193.679231</td>\n",
       "      <td>6310.179231</td>\n",
       "      <td>5.262868</td>\n",
       "      <td>5879.980232</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>2.495238</td>\n",
       "      <td>1.679762</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>0.851786</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>0.577373</td>\n",
       "      <td>30.085102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>6350.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>205.839076</td>\n",
       "      <td>6556.739076</td>\n",
       "      <td>5.468506</td>\n",
       "      <td>5792.290841</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-1.642262</td>\n",
       "      <td>2.404167</td>\n",
       "      <td>324.200000</td>\n",
       "      <td>0.597619</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>1.020711</td>\n",
       "      <td>29.392317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>6268.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>156.081057</td>\n",
       "      <td>6424.081057</td>\n",
       "      <td>5.357866</td>\n",
       "      <td>6014.009028</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>0.518452</td>\n",
       "      <td>2.061310</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>0.468452</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.830202</td>\n",
       "      <td>28.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-12-03</td>\n",
       "      <td>7009.80</td>\n",
       "      <td>98.70</td>\n",
       "      <td>48.819741</td>\n",
       "      <td>6959.919741</td>\n",
       "      <td>5.804770</td>\n",
       "      <td>6147.240681</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-5.323214</td>\n",
       "      <td>2.129762</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.486905</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>1.126537</td>\n",
       "      <td>28.458567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-12-10</td>\n",
       "      <td>7034.50</td>\n",
       "      <td>56.80</td>\n",
       "      <td>22.665463</td>\n",
       "      <td>7000.365463</td>\n",
       "      <td>5.838503</td>\n",
       "      <td>6145.668178</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-2.753571</td>\n",
       "      <td>2.814881</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>1.115639</td>\n",
       "      <td>27.383474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-12-17</td>\n",
       "      <td>6897.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.562840</td>\n",
       "      <td>6907.062840</td>\n",
       "      <td>5.760686</td>\n",
       "      <td>6273.903673</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-1.336310</td>\n",
       "      <td>2.041071</td>\n",
       "      <td>47.900000</td>\n",
       "      <td>0.702976</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.752205</td>\n",
       "      <td>30.662228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-12-24</td>\n",
       "      <td>6796.90</td>\n",
       "      <td>6.40</td>\n",
       "      <td>56.849272</td>\n",
       "      <td>6847.349272</td>\n",
       "      <td>5.710883</td>\n",
       "      <td>6260.398166</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-3.482143</td>\n",
       "      <td>2.109524</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>0.516071</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.603219</td>\n",
       "      <td>30.632494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>6545.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.434764</td>\n",
       "      <td>6546.234764</td>\n",
       "      <td>5.459745</td>\n",
       "      <td>6321.281952</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-4.657738</td>\n",
       "      <td>2.222024</td>\n",
       "      <td>75.900000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.435798</td>\n",
       "      <td>30.648392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>7329.30</td>\n",
       "      <td>40.60</td>\n",
       "      <td>0.316244</td>\n",
       "      <td>7289.016244</td>\n",
       "      <td>6.079246</td>\n",
       "      <td>6750.662263</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-12.305952</td>\n",
       "      <td>3.579762</td>\n",
       "      <td>52.300000</td>\n",
       "      <td>0.523214</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.603034</td>\n",
       "      <td>30.818260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>7570.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.314518</td>\n",
       "      <td>7570.514518</td>\n",
       "      <td>6.314024</td>\n",
       "      <td>7222.069233</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-10.205952</td>\n",
       "      <td>1.060119</td>\n",
       "      <td>113.900000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.980968</td>\n",
       "      <td>31.200258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>7508.40</td>\n",
       "      <td>51.00</td>\n",
       "      <td>1.242446</td>\n",
       "      <td>7458.642446</td>\n",
       "      <td>6.220719</td>\n",
       "      <td>7186.228496</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-10.229167</td>\n",
       "      <td>2.070238</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.698810</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>0.785025</td>\n",
       "      <td>32.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>6239.00</td>\n",
       "      <td>13.30</td>\n",
       "      <td>80.673104</td>\n",
       "      <td>6306.373104</td>\n",
       "      <td>5.259694</td>\n",
       "      <td>5688.609781</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>1.981548</td>\n",
       "      <td>3.143452</td>\n",
       "      <td>164.500000</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.113095</td>\n",
       "      <td>0.575208</td>\n",
       "      <td>31.404234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2024-02-04</td>\n",
       "      <td>5799.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>215.771846</td>\n",
       "      <td>6014.671846</td>\n",
       "      <td>5.016407</td>\n",
       "      <td>5898.151877</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>2.278571</td>\n",
       "      <td>2.520238</td>\n",
       "      <td>171.500000</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.064881</td>\n",
       "      <td>0.566558</td>\n",
       "      <td>30.083069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>6444.16</td>\n",
       "      <td>12.03</td>\n",
       "      <td>115.065231</td>\n",
       "      <td>6547.195231</td>\n",
       "      <td>5.460546</td>\n",
       "      <td>6197.042381</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-5.105357</td>\n",
       "      <td>3.322024</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.134895</td>\n",
       "      <td>27.539988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>6376.30</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.157412</td>\n",
       "      <td>6378.257412</td>\n",
       "      <td>5.319648</td>\n",
       "      <td>5823.972629</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>-1.160119</td>\n",
       "      <td>1.841667</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>0.874405</td>\n",
       "      <td>0.254762</td>\n",
       "      <td>0.709094</td>\n",
       "      <td>27.255322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2024-02-25</td>\n",
       "      <td>5739.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>193.681547</td>\n",
       "      <td>5929.381547</td>\n",
       "      <td>4.945272</td>\n",
       "      <td>5923.357733</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>2.179167</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.582311</td>\n",
       "      <td>26.351835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>5424.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>413.033084</td>\n",
       "      <td>5837.633084</td>\n",
       "      <td>4.868752</td>\n",
       "      <td>5893.694028</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.879762</td>\n",
       "      <td>3.207738</td>\n",
       "      <td>344.900000</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.638323</td>\n",
       "      <td>24.419682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>5240.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>783.800591</td>\n",
       "      <td>6024.100591</td>\n",
       "      <td>5.024271</td>\n",
       "      <td>6012.997689</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>2.102976</td>\n",
       "      <td>2.789286</td>\n",
       "      <td>43.800000</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667096</td>\n",
       "      <td>24.745453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>5665.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>239.867289</td>\n",
       "      <td>5905.467289</td>\n",
       "      <td>4.925327</td>\n",
       "      <td>6109.062110</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.575595</td>\n",
       "      <td>2.754167</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>0.751786</td>\n",
       "      <td>0.092262</td>\n",
       "      <td>0.592692</td>\n",
       "      <td>24.443321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-03-24</td>\n",
       "      <td>5042.70</td>\n",
       "      <td>29.90</td>\n",
       "      <td>979.398875</td>\n",
       "      <td>5992.198875</td>\n",
       "      <td>4.997664</td>\n",
       "      <td>5920.496889</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.980952</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>0.630696</td>\n",
       "      <td>23.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>4762.20</td>\n",
       "      <td>9.30</td>\n",
       "      <td>546.038805</td>\n",
       "      <td>5298.938805</td>\n",
       "      <td>4.419465</td>\n",
       "      <td>5465.842253</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>5.283929</td>\n",
       "      <td>2.644643</td>\n",
       "      <td>42.900000</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>0.705644</td>\n",
       "      <td>23.906605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024-04-07</td>\n",
       "      <td>4951.60</td>\n",
       "      <td>154.30</td>\n",
       "      <td>1038.792384</td>\n",
       "      <td>5836.092384</td>\n",
       "      <td>4.867467</td>\n",
       "      <td>5472.947351</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.769643</td>\n",
       "      <td>3.051190</td>\n",
       "      <td>341.400000</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.621483</td>\n",
       "      <td>27.185741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>4416.10</td>\n",
       "      <td>178.00</td>\n",
       "      <td>1832.892262</td>\n",
       "      <td>6070.992262</td>\n",
       "      <td>5.063380</td>\n",
       "      <td>6078.012030</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>9.717857</td>\n",
       "      <td>3.160119</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.648214</td>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.494829</td>\n",
       "      <td>25.478622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-04-21</td>\n",
       "      <td>3976.70</td>\n",
       "      <td>233.00</td>\n",
       "      <td>2087.598541</td>\n",
       "      <td>5831.298541</td>\n",
       "      <td>4.863468</td>\n",
       "      <td>5764.143249</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>5.142262</td>\n",
       "      <td>1.969048</td>\n",
       "      <td>337.200000</td>\n",
       "      <td>0.344643</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.546244</td>\n",
       "      <td>27.676895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>4300.10</td>\n",
       "      <td>64.50</td>\n",
       "      <td>1606.913575</td>\n",
       "      <td>5842.513575</td>\n",
       "      <td>4.872822</td>\n",
       "      <td>5796.793380</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>5.546429</td>\n",
       "      <td>2.126786</td>\n",
       "      <td>58.400000</td>\n",
       "      <td>0.632738</td>\n",
       "      <td>0.042262</td>\n",
       "      <td>0.760249</td>\n",
       "      <td>27.743940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>3863.70</td>\n",
       "      <td>170.00</td>\n",
       "      <td>2137.402899</td>\n",
       "      <td>5831.102899</td>\n",
       "      <td>4.863305</td>\n",
       "      <td>6085.158467</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>14.201190</td>\n",
       "      <td>3.076190</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>0.601762</td>\n",
       "      <td>28.270388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>3573.50</td>\n",
       "      <td>244.80</td>\n",
       "      <td>2233.828437</td>\n",
       "      <td>5562.528437</td>\n",
       "      <td>4.639306</td>\n",
       "      <td>5669.727257</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>12.848214</td>\n",
       "      <td>2.155952</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>33.500935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2024-05-19</td>\n",
       "      <td>2948.20</td>\n",
       "      <td>237.30</td>\n",
       "      <td>2944.331318</td>\n",
       "      <td>5655.231318</td>\n",
       "      <td>4.716623</td>\n",
       "      <td>6003.304352</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.552976</td>\n",
       "      <td>1.969643</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313550</td>\n",
       "      <td>26.607361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024-05-26</td>\n",
       "      <td>3964.40</td>\n",
       "      <td>141.20</td>\n",
       "      <td>2415.884225</td>\n",
       "      <td>6239.084225</td>\n",
       "      <td>5.203573</td>\n",
       "      <td>6874.557659</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>19.263690</td>\n",
       "      <td>2.772619</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.394048</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.565720</td>\n",
       "      <td>27.023720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2024-06-02</td>\n",
       "      <td>4594.90</td>\n",
       "      <td>147.10</td>\n",
       "      <td>2274.957488</td>\n",
       "      <td>6722.757488</td>\n",
       "      <td>5.606970</td>\n",
       "      <td>6977.073659</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.851786</td>\n",
       "      <td>2.200595</td>\n",
       "      <td>68.700000</td>\n",
       "      <td>0.617262</td>\n",
       "      <td>0.186310</td>\n",
       "      <td>0.595813</td>\n",
       "      <td>26.114525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2024-06-09</td>\n",
       "      <td>3910.00</td>\n",
       "      <td>178.60</td>\n",
       "      <td>2400.007267</td>\n",
       "      <td>6131.407267</td>\n",
       "      <td>5.113768</td>\n",
       "      <td>6014.337008</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>13.137500</td>\n",
       "      <td>3.098810</td>\n",
       "      <td>317.400000</td>\n",
       "      <td>0.575595</td>\n",
       "      <td>0.169048</td>\n",
       "      <td>0.570734</td>\n",
       "      <td>26.367502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2024-06-16</td>\n",
       "      <td>4200.30</td>\n",
       "      <td>197.30</td>\n",
       "      <td>2216.065564</td>\n",
       "      <td>6219.065564</td>\n",
       "      <td>5.186877</td>\n",
       "      <td>5832.235423</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>14.276786</td>\n",
       "      <td>2.569643</td>\n",
       "      <td>296.500000</td>\n",
       "      <td>0.744643</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.429978</td>\n",
       "      <td>26.400955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2024-06-23</td>\n",
       "      <td>3890.50</td>\n",
       "      <td>350.33</td>\n",
       "      <td>2904.002368</td>\n",
       "      <td>6444.172368</td>\n",
       "      <td>5.374622</td>\n",
       "      <td>5983.326577</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.941071</td>\n",
       "      <td>2.348214</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>0.502381</td>\n",
       "      <td>0.141071</td>\n",
       "      <td>0.498993</td>\n",
       "      <td>25.367430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>4117.90</td>\n",
       "      <td>192.30</td>\n",
       "      <td>2990.536569</td>\n",
       "      <td>6916.136569</td>\n",
       "      <td>5.768254</td>\n",
       "      <td>6713.485236</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.168452</td>\n",
       "      <td>2.845238</td>\n",
       "      <td>348.800000</td>\n",
       "      <td>0.395238</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.564359</td>\n",
       "      <td>26.514252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>4536.50</td>\n",
       "      <td>96.40</td>\n",
       "      <td>1935.143308</td>\n",
       "      <td>6375.243308</td>\n",
       "      <td>5.317134</td>\n",
       "      <td>6050.438947</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>14.807143</td>\n",
       "      <td>2.530357</td>\n",
       "      <td>49.075475</td>\n",
       "      <td>0.549405</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.489324</td>\n",
       "      <td>26.565987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2024-07-14</td>\n",
       "      <td>4654.50</td>\n",
       "      <td>92.20</td>\n",
       "      <td>2189.595332</td>\n",
       "      <td>6751.895332</td>\n",
       "      <td>5.631272</td>\n",
       "      <td>6232.700556</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.692262</td>\n",
       "      <td>2.454167</td>\n",
       "      <td>208.600000</td>\n",
       "      <td>0.578571</td>\n",
       "      <td>0.176786</td>\n",
       "      <td>0.496169</td>\n",
       "      <td>29.472328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2024-07-21</td>\n",
       "      <td>4720.90</td>\n",
       "      <td>124.27</td>\n",
       "      <td>2370.713163</td>\n",
       "      <td>6967.343163</td>\n",
       "      <td>5.810962</td>\n",
       "      <td>6594.504015</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.769048</td>\n",
       "      <td>1.792262</td>\n",
       "      <td>121.900000</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.098810</td>\n",
       "      <td>0.445001</td>\n",
       "      <td>29.759705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2024-07-28</td>\n",
       "      <td>5041.10</td>\n",
       "      <td>173.20</td>\n",
       "      <td>2118.126799</td>\n",
       "      <td>6986.026799</td>\n",
       "      <td>5.826544</td>\n",
       "      <td>6458.019554</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.430952</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>118.900000</td>\n",
       "      <td>0.626190</td>\n",
       "      <td>0.344643</td>\n",
       "      <td>0.494799</td>\n",
       "      <td>25.120201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>4442.90</td>\n",
       "      <td>129.70</td>\n",
       "      <td>2578.447053</td>\n",
       "      <td>6891.647053</td>\n",
       "      <td>5.747829</td>\n",
       "      <td>6298.332160</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>17.960119</td>\n",
       "      <td>1.868452</td>\n",
       "      <td>338.300000</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.538057</td>\n",
       "      <td>22.860989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2024-08-11</td>\n",
       "      <td>5231.10</td>\n",
       "      <td>146.50</td>\n",
       "      <td>1991.980714</td>\n",
       "      <td>7076.580714</td>\n",
       "      <td>5.902069</td>\n",
       "      <td>6250.729696</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>18.122619</td>\n",
       "      <td>3.030952</td>\n",
       "      <td>51.100000</td>\n",
       "      <td>0.545238</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.596519</td>\n",
       "      <td>22.568504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2024-08-18</td>\n",
       "      <td>5186.40</td>\n",
       "      <td>33.90</td>\n",
       "      <td>1752.937261</td>\n",
       "      <td>6905.437261</td>\n",
       "      <td>5.759330</td>\n",
       "      <td>6089.945542</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>16.693452</td>\n",
       "      <td>2.279762</td>\n",
       "      <td>358.300000</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.057738</td>\n",
       "      <td>0.608740</td>\n",
       "      <td>20.641499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2024-08-25</td>\n",
       "      <td>5717.30</td>\n",
       "      <td>71.60</td>\n",
       "      <td>1168.109777</td>\n",
       "      <td>6813.809777</td>\n",
       "      <td>5.682911</td>\n",
       "      <td>5915.811599</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.157143</td>\n",
       "      <td>3.139286</td>\n",
       "      <td>309.900000</td>\n",
       "      <td>0.694643</td>\n",
       "      <td>0.410417</td>\n",
       "      <td>0.637214</td>\n",
       "      <td>21.449982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>5253.60</td>\n",
       "      <td>90.10</td>\n",
       "      <td>1596.257747</td>\n",
       "      <td>6759.757747</td>\n",
       "      <td>5.637830</td>\n",
       "      <td>6012.214702</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.338690</td>\n",
       "      <td>2.212500</td>\n",
       "      <td>176.200000</td>\n",
       "      <td>0.536905</td>\n",
       "      <td>0.058929</td>\n",
       "      <td>0.287649</td>\n",
       "      <td>21.673393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2024-09-08</td>\n",
       "      <td>1781.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>158.440097</td>\n",
       "      <td>1939.840097</td>\n",
       "      <td>1.617882</td>\n",
       "      <td>1779.562769</td>\n",
       "      <td>main building</td>\n",
       "      <td>10724</td>\n",
       "      <td>1199</td>\n",
       "      <td>15.234043</td>\n",
       "      <td>2.363830</td>\n",
       "      <td>356.200000</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>0.588150</td>\n",
       "      <td>25.930389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  predicted_consumption  value_export  solar_consumption  \\\n",
       "0  2023-07-02                 903.50        178.00         955.427660   \n",
       "1  2023-07-09                4324.30        220.10        2663.311274   \n",
       "2  2023-07-16                5008.30         66.70        1734.309332   \n",
       "3  2023-07-23                4155.10        169.10        2656.076572   \n",
       "4  2023-07-30                4563.50        219.40        2299.114591   \n",
       "5  2023-08-06                4547.50         70.40        2227.371748   \n",
       "6  2023-08-13                5090.10         52.20        1571.916839   \n",
       "7  2023-08-20                5411.10         36.50        1423.202317   \n",
       "8  2023-08-27                4992.00         57.40        1747.366035   \n",
       "9  2023-09-03                4719.90          5.50        1952.757229   \n",
       "10 2023-09-10                5282.20         32.00        1456.647447   \n",
       "11 2023-09-17                5314.30         34.10        1275.001363   \n",
       "12 2023-09-24                5549.69          1.40         902.325838   \n",
       "13 2023-10-01                5554.90         16.90         868.472727   \n",
       "14 2023-10-08                5267.30         11.40        1024.092877   \n",
       "15 2023-10-15                5528.30          2.70         747.527215   \n",
       "16 2023-10-22                5689.70          0.00         536.634522   \n",
       "17 2023-10-29                5766.73          0.00         266.524990   \n",
       "18 2023-11-05                6217.71          0.90          52.804613   \n",
       "19 2023-11-12                6116.50          0.00         193.679231   \n",
       "20 2023-11-19                6350.90          0.00         205.839076   \n",
       "21 2023-11-26                6268.00          0.00         156.081057   \n",
       "22 2023-12-03                7009.80         98.70          48.819741   \n",
       "23 2023-12-10                7034.50         56.80          22.665463   \n",
       "24 2023-12-17                6897.50          0.00           9.562840   \n",
       "25 2023-12-24                6796.90          6.40          56.849272   \n",
       "26 2023-12-31                6545.80          0.00           0.434764   \n",
       "27 2024-01-07                7329.30         40.60           0.316244   \n",
       "28 2024-01-14                7570.70          0.50           0.314518   \n",
       "29 2024-01-21                7508.40         51.00           1.242446   \n",
       "30 2024-01-28                6239.00         13.30          80.673104   \n",
       "31 2024-02-04                5799.00          0.10         215.771846   \n",
       "32 2024-02-11                6444.16         12.03         115.065231   \n",
       "33 2024-02-18                6376.30          2.20           4.157412   \n",
       "34 2024-02-25                5739.30          3.60         193.681547   \n",
       "35 2024-03-03                5424.60          0.00         413.033084   \n",
       "36 2024-03-10                5240.30          0.00         783.800591   \n",
       "37 2024-03-17                5665.70          0.10         239.867289   \n",
       "38 2024-03-24                5042.70         29.90         979.398875   \n",
       "39 2024-03-31                4762.20          9.30         546.038805   \n",
       "40 2024-04-07                4951.60        154.30        1038.792384   \n",
       "41 2024-04-14                4416.10        178.00        1832.892262   \n",
       "42 2024-04-21                3976.70        233.00        2087.598541   \n",
       "43 2024-04-28                4300.10         64.50        1606.913575   \n",
       "44 2024-05-05                3863.70        170.00        2137.402899   \n",
       "45 2024-05-12                3573.50        244.80        2233.828437   \n",
       "46 2024-05-19                2948.20        237.30        2944.331318   \n",
       "47 2024-05-26                3964.40        141.20        2415.884225   \n",
       "48 2024-06-02                4594.90        147.10        2274.957488   \n",
       "49 2024-06-09                3910.00        178.60        2400.007267   \n",
       "50 2024-06-16                4200.30        197.30        2216.065564   \n",
       "51 2024-06-23                3890.50        350.33        2904.002368   \n",
       "52 2024-06-30                4117.90        192.30        2990.536569   \n",
       "53 2024-07-07                4536.50         96.40        1935.143308   \n",
       "54 2024-07-14                4654.50         92.20        2189.595332   \n",
       "55 2024-07-21                4720.90        124.27        2370.713163   \n",
       "56 2024-07-28                5041.10        173.20        2118.126799   \n",
       "57 2024-08-04                4442.90        129.70        2578.447053   \n",
       "58 2024-08-11                5231.10        146.50        1991.980714   \n",
       "59 2024-08-18                5186.40         33.90        1752.937261   \n",
       "60 2024-08-25                5717.30         71.60        1168.109777   \n",
       "61 2024-09-01                5253.60         90.10        1596.257747   \n",
       "62 2024-09-08                1781.40          0.00         158.440097   \n",
       "\n",
       "    net_consumption  net_consumption_per_sqm  predicted_consumption  \\\n",
       "0       1680.927660                 1.401941            1757.470821   \n",
       "1       6767.511274                 5.644296            7397.542175   \n",
       "2       6675.909332                 5.567898            7472.545071   \n",
       "3       6642.076572                 5.539680            6819.156256   \n",
       "4       6643.214591                 5.540629            6869.377289   \n",
       "5       6704.471748                 5.591720            7471.431577   \n",
       "6       6609.816839                 5.512775            7141.931791   \n",
       "7       6797.802317                 5.669560            8146.789063   \n",
       "8       6681.966035                 5.572949            7195.936084   \n",
       "9       6667.157229                 5.560598            6610.651114   \n",
       "10      6706.847447                 5.593701            7533.559487   \n",
       "11      6555.201363                 5.467224            6693.579625   \n",
       "12      6450.615838                 5.379997            6458.413331   \n",
       "13      6406.472727                 5.343180            6644.253128   \n",
       "14      6279.992877                 5.237692            6139.315823   \n",
       "15      6273.127215                 5.231966            6077.512462   \n",
       "16      6226.334522                 5.192940            5971.146013   \n",
       "17      6033.254990                 5.031906            5889.763742   \n",
       "18      6269.614613                 5.229036            5893.136672   \n",
       "19      6310.179231                 5.262868            5879.980232   \n",
       "20      6556.739076                 5.468506            5792.290841   \n",
       "21      6424.081057                 5.357866            6014.009028   \n",
       "22      6959.919741                 5.804770            6147.240681   \n",
       "23      7000.365463                 5.838503            6145.668178   \n",
       "24      6907.062840                 5.760686            6273.903673   \n",
       "25      6847.349272                 5.710883            6260.398166   \n",
       "26      6546.234764                 5.459745            6321.281952   \n",
       "27      7289.016244                 6.079246            6750.662263   \n",
       "28      7570.514518                 6.314024            7222.069233   \n",
       "29      7458.642446                 6.220719            7186.228496   \n",
       "30      6306.373104                 5.259694            5688.609781   \n",
       "31      6014.671846                 5.016407            5898.151877   \n",
       "32      6547.195231                 5.460546            6197.042381   \n",
       "33      6378.257412                 5.319648            5823.972629   \n",
       "34      5929.381547                 4.945272            5923.357733   \n",
       "35      5837.633084                 4.868752            5893.694028   \n",
       "36      6024.100591                 5.024271            6012.997689   \n",
       "37      5905.467289                 4.925327            6109.062110   \n",
       "38      5992.198875                 4.997664            5920.496889   \n",
       "39      5298.938805                 4.419465            5465.842253   \n",
       "40      5836.092384                 4.867467            5472.947351   \n",
       "41      6070.992262                 5.063380            6078.012030   \n",
       "42      5831.298541                 4.863468            5764.143249   \n",
       "43      5842.513575                 4.872822            5796.793380   \n",
       "44      5831.102899                 4.863305            6085.158467   \n",
       "45      5562.528437                 4.639306            5669.727257   \n",
       "46      5655.231318                 4.716623            6003.304352   \n",
       "47      6239.084225                 5.203573            6874.557659   \n",
       "48      6722.757488                 5.606970            6977.073659   \n",
       "49      6131.407267                 5.113768            6014.337008   \n",
       "50      6219.065564                 5.186877            5832.235423   \n",
       "51      6444.172368                 5.374622            5983.326577   \n",
       "52      6916.136569                 5.768254            6713.485236   \n",
       "53      6375.243308                 5.317134            6050.438947   \n",
       "54      6751.895332                 5.631272            6232.700556   \n",
       "55      6967.343163                 5.810962            6594.504015   \n",
       "56      6986.026799                 5.826544            6458.019554   \n",
       "57      6891.647053                 5.747829            6298.332160   \n",
       "58      7076.580714                 5.902069            6250.729696   \n",
       "59      6905.437261                 5.759330            6089.945542   \n",
       "60      6813.809777                 5.682911            5915.811599   \n",
       "61      6759.757747                 5.637830            6012.214702   \n",
       "62      1939.840097                 1.617882            1779.562769   \n",
       "\n",
       "         building  property_id  area  temperature  wind_speed  wind_direction  \\\n",
       "0   main building        10724  1199    17.239583    2.479167      251.500000   \n",
       "1   main building        10724  1199    17.372024    2.097619       96.300000   \n",
       "2   main building        10724  1199    16.825595    2.894643       63.200000   \n",
       "3   main building        10724  1199    16.225000    2.585119      201.500000   \n",
       "4   main building        10724  1199    15.952381    2.278571        0.300000   \n",
       "5   main building        10724  1199    16.923810    1.627976      160.600000   \n",
       "6   main building        10724  1199    15.998810    2.424405      313.400000   \n",
       "7   main building        10724  1199    18.184524    2.311310      183.200000   \n",
       "8   main building        10724  1199    16.333333    1.788095      153.100000   \n",
       "9   main building        10724  1199    15.094643    1.406548      122.100000   \n",
       "10  main building        10724  1199    17.698214    1.695238      138.400000   \n",
       "11  main building        10724  1199    14.032738    2.158333       75.300000   \n",
       "12  main building        10724  1199    13.429762    3.333333       60.200000   \n",
       "13  main building        10724  1199    13.899405    2.689881      187.100000   \n",
       "14  main building        10724  1199     9.736905    1.891667      167.300000   \n",
       "15  main building        10724  1199     7.922024    3.042857       90.400000   \n",
       "16  main building        10724  1199     4.142857    4.011310      314.700000   \n",
       "17  main building        10724  1199     3.448214    3.420833        7.800000   \n",
       "18  main building        10724  1199     2.598810    4.973214       30.700000   \n",
       "19  main building        10724  1199     2.495238    1.679762       11.200000   \n",
       "20  main building        10724  1199    -1.642262    2.404167      324.200000   \n",
       "21  main building        10724  1199     0.518452    2.061310       24.900000   \n",
       "22  main building        10724  1199    -5.323214    2.129762        0.800000   \n",
       "23  main building        10724  1199    -2.753571    2.814881       29.600000   \n",
       "24  main building        10724  1199    -1.336310    2.041071       47.900000   \n",
       "25  main building        10724  1199    -3.482143    2.109524      335.000000   \n",
       "26  main building        10724  1199    -4.657738    2.222024       75.900000   \n",
       "27  main building        10724  1199   -12.305952    3.579762       52.300000   \n",
       "28  main building        10724  1199   -10.205952    1.060119      113.900000   \n",
       "29  main building        10724  1199   -10.229167    2.070238        2.400000   \n",
       "30  main building        10724  1199     1.981548    3.143452      164.500000   \n",
       "31  main building        10724  1199     2.278571    2.520238      171.500000   \n",
       "32  main building        10724  1199    -5.105357    3.322024      235.500000   \n",
       "33  main building        10724  1199    -1.160119    1.841667       38.400000   \n",
       "34  main building        10724  1199     2.687500    2.179167       39.800000   \n",
       "35  main building        10724  1199     3.879762    3.207738      344.900000   \n",
       "36  main building        10724  1199     2.102976    2.789286       43.800000   \n",
       "37  main building        10724  1199     3.575595    2.754167       31.400000   \n",
       "38  main building        10724  1199     3.980952    2.066667       63.600000   \n",
       "39  main building        10724  1199     5.283929    2.644643       42.900000   \n",
       "40  main building        10724  1199     3.769643    3.051190      341.400000   \n",
       "41  main building        10724  1199     9.717857    3.160119      155.000000   \n",
       "42  main building        10724  1199     5.142262    1.969048      337.200000   \n",
       "43  main building        10724  1199     5.546429    2.126786       58.400000   \n",
       "44  main building        10724  1199    14.201190    3.076190       60.400000   \n",
       "45  main building        10724  1199    12.848214    2.155952       26.400000   \n",
       "46  main building        10724  1199    16.552976    1.969643       46.000000   \n",
       "47  main building        10724  1199    19.263690    2.772619       51.000000   \n",
       "48  main building        10724  1199    17.851786    2.200595       68.700000   \n",
       "49  main building        10724  1199    13.137500    3.098810      317.400000   \n",
       "50  main building        10724  1199    14.276786    2.569643      296.500000   \n",
       "51  main building        10724  1199    15.941071    2.348214       71.500000   \n",
       "52  main building        10724  1199    18.168452    2.845238      348.800000   \n",
       "53  main building        10724  1199    14.807143    2.530357       49.075475   \n",
       "54  main building        10724  1199    16.692262    2.454167      208.600000   \n",
       "55  main building        10724  1199    18.769048    1.792262      121.900000   \n",
       "56  main building        10724  1199    18.430952    2.133333      118.900000   \n",
       "57  main building        10724  1199    17.960119    1.868452      338.300000   \n",
       "58  main building        10724  1199    18.122619    3.030952       51.100000   \n",
       "59  main building        10724  1199    16.693452    2.279762      358.300000   \n",
       "60  main building        10724  1199    15.157143    3.139286      309.900000   \n",
       "61  main building        10724  1199    15.338690    2.212500      176.200000   \n",
       "62  main building        10724  1199    15.234043    2.363830      356.200000   \n",
       "\n",
       "    cloud_fraction  precipitation  spot_price_nok  value_import_y  \n",
       "0         0.570833       0.189583        1.137242       25.746155  \n",
       "1         0.583929       0.150595        0.734471       25.609608  \n",
       "2         0.738690       0.180952        1.041394       27.073210  \n",
       "3         0.573810       0.008929        0.496056       27.110697  \n",
       "4         0.583929       0.111905        0.674216       27.063702  \n",
       "5         0.605952       0.085119        0.664450       26.109138  \n",
       "6         0.749405       0.205357        0.588623       28.459934  \n",
       "7         0.752381       0.229762        0.867745       29.813784  \n",
       "8         0.628571       0.067262        0.847740       27.845287  \n",
       "9         0.472619       0.004167        0.909783       23.036877  \n",
       "10        0.558929       0.000000        0.862716       20.361374  \n",
       "11        0.663095       0.016667        0.858647       20.103357  \n",
       "12        0.729762       0.389881        0.114518       19.496626  \n",
       "13        0.668452       0.048810        0.002748       20.276810  \n",
       "14        0.607738       0.026786        0.522057       21.433884  \n",
       "15        0.491667       0.098810        0.570979       19.626102  \n",
       "16        0.678571       0.036905        0.468074       24.340889  \n",
       "17        0.741071       0.002381        0.652040       28.238678  \n",
       "18        0.901190       0.298214        0.219281       29.976297  \n",
       "19        0.851786       0.151786        0.577373       30.085102  \n",
       "20        0.597619       0.001190        1.020711       29.392317  \n",
       "21        0.468452       0.000595        0.830202       28.933100  \n",
       "22        0.486905       0.000595        1.126537       28.458567  \n",
       "23        0.827381       0.106548        1.115639       27.383474  \n",
       "24        0.702976       0.004762        0.752205       30.662228  \n",
       "25        0.516071       0.050595        0.603219       30.632494  \n",
       "26        0.716667       0.112500        0.435798       30.648392  \n",
       "27        0.523214       0.076190        0.603034       30.818260  \n",
       "28        0.437500       0.006548        0.980968       31.200258  \n",
       "29        0.698810       0.220238        0.785025       32.232300  \n",
       "30        0.652381       0.113095        0.575208       31.404234  \n",
       "31        0.540476       0.064881        0.566558       30.083069  \n",
       "32        0.733333       0.023810        0.134895       27.539988  \n",
       "33        0.874405       0.254762        0.709094       27.255322  \n",
       "34        0.879167       0.130357        0.582311       26.351835  \n",
       "35        0.817857       0.170833        0.638323       24.419682  \n",
       "36        0.590476       0.000000        0.667096       24.745453  \n",
       "37        0.751786       0.092262        0.592692       24.443321  \n",
       "38        0.818452       0.036310        0.630696       23.894737  \n",
       "39        0.954762       0.109524        0.705644       23.906605  \n",
       "40        0.819643       0.067857        0.621483       27.185741  \n",
       "41        0.648214       0.069048        0.494829       25.478622  \n",
       "42        0.344643       0.008929        0.546244       27.676895  \n",
       "43        0.632738       0.042262        0.760249       27.743940  \n",
       "44        0.695833       0.019643        0.601762       28.270388  \n",
       "45        0.612500       0.005952        0.555100       33.500935  \n",
       "46        0.262500       0.000000        0.313550       26.607361  \n",
       "47        0.394048       0.054762        0.565720       27.023720  \n",
       "48        0.617262       0.186310        0.595813       26.114525  \n",
       "49        0.575595       0.169048        0.570734       26.367502  \n",
       "50        0.744643       0.145833        0.429978       26.400955  \n",
       "51        0.502381       0.141071        0.498993       25.367430  \n",
       "52        0.395238       0.030357        0.564359       26.514252  \n",
       "53        0.549405       0.087500        0.489324       26.565987  \n",
       "54        0.578571       0.176786        0.496169       29.472328  \n",
       "55        0.641667       0.098810        0.445001       29.759705  \n",
       "56        0.626190       0.344643        0.494799       25.120201  \n",
       "57        0.462500       0.036905        0.538057       22.860989  \n",
       "58        0.545238       0.108333        0.596519       22.568504  \n",
       "59        0.589286       0.057738        0.608740       20.641499  \n",
       "60        0.694643       0.410417        0.637214       21.449982  \n",
       "61        0.536905       0.058929        0.287649       21.673393  \n",
       "62        0.865957       0.312766        0.588150       25.930389  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils import ColumnParam, plot_energy_usage\n",
    "\n",
    "plot_cols =[\n",
    "    ColumnParam(\"value_import\", \"Energy Import\"),\n",
    "    ColumnParam(\"net_consumption\", \"Net Consumption\"),\n",
    "    ColumnParam(\"predicted_consumption\", \"Predicted Consumption\"),\n",
    "]\n",
    "main = pipe.get_data(BuilingIdsEnum.MAIN)\n",
    "\n",
    "# Merge the predicted values with the actual values on index and call the new columns 'predicted_consumption2'\n",
    "# daily_main = daily_main.merge(prediciton1_df, left_index=True, right_index=True)\n",
    "# daily_main = daily_main.rename(columns={\"value_import_x\": 'predicted_consumption'})\n",
    "# # trim\n",
    "# daily_main = daily_main.iloc[1:-1]\n",
    "# plot_energy_usage(daily_main, plot_cols,\n",
    "#                   titel=\"Energy Usage actual vs predicted\",\n",
    "#                   yaxis_title=\"Energy [kWh]\",\n",
    "#                   tozeroy=True,\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
